{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794d62f7",
   "metadata": {},
   "source": [
    "# Consensus Docking Results Analysis\n",
    "\n",
    "This notebook analyzes large-scale consensus docking results to evaluate binding pose consistency and perform cluster-based selectivity analysis.\n",
    "\n",
    "## üöÄ Quick Start Guide\n",
    "\n",
    "**For most users:** Simply run cells 2-4 to load, filter, and start analysis immediately.\n",
    "\n",
    "**First-time users or data updates:** If you need to create/update the data files, run the optimized data preparation script:\n",
    "```bash\n",
    "python parse_prepare.py\n",
    "```\n",
    "This high-performance script uses multiprocessing to efficiently process millions of docking results in minutes instead of hours.\n",
    "\n",
    "## üìä Analysis Overview\n",
    "\n",
    "### Main Analysis Workflow\n",
    "1. **Data Loading** (Step 1) - Smart loading of existing data files\n",
    "2. **Cluster Integration** (Step 2) - Add cavity similarity information  \n",
    "3. **Tool Coverage Filtering** (Step 2.5) - **NEW:** Filter for complete tool coverage\n",
    "4. **Data Quality Check** (Step 3) - Dataset overview of filtered data\n",
    "5. **Tool Reliability Analysis** (Step 4) - Consensus analysis between tools\n",
    "6. **Cluster Analysis** - Binding site similarity and drug selectivity\n",
    "7. **Visualizations** - Comprehensive plots and insights\n",
    "\n",
    "### Key Outputs\n",
    "- **Fair tool comparisons** using only drug-target pairs with complete tool coverage\n",
    "- Pose consistency metrics across docking tools\n",
    "- Drug-target binding success rates\n",
    "- Cluster-based selectivity patterns\n",
    "- Tool agreement analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754698f7",
   "metadata": {},
   "source": [
    "## üì• Step 1: Smart Data Loading\n",
    "\n",
    "This cell automatically detects and loads the best available data source. Run this first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üì• SMART DATA LOADING - START HERE\n",
    "# =============================================================================\n",
    "\n",
    "import os, re\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PARQUET_FILE = \"combined_consensus_docking_results.parquet\"\n",
    "CSV_FILE = \"combined_consensus_docking_results.csv\"\n",
    "BASE_FOLDER = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/consensus_docking_results\"\n",
    "\n",
    "print(\"üîç Checking for existing data files...\")\n",
    "\n",
    "# Smart data loading: try parquet first, then CSV, then create from scratch\n",
    "combined_results = None\n",
    "\n",
    "if os.path.exists(PARQUET_FILE):\n",
    "    print(f\"‚úÖ Found Parquet file: {PARQUET_FILE}\")\n",
    "    print(\"üìñ Loading data (this is the fastest option)...\")\n",
    "    combined_results = pl.read_parquet(PARQUET_FILE)\n",
    "    print(f\"   Shape: {combined_results.shape}\")\n",
    "    print(f\"   Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "\n",
    "elif os.path.exists(CSV_FILE):\n",
    "    print(f\"‚úÖ Found CSV file: {CSV_FILE}\")\n",
    "    print(\"üìñ Loading data (slower than Parquet but still good)...\")\n",
    "    combined_results = pl.read_csv(CSV_FILE)\n",
    "    print(f\"   Shape: {combined_results.shape}\")\n",
    "    print(f\"   Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No preprocessed data files found!\")\n",
    "    print(f\"   Looking for: {PARQUET_FILE} or {CSV_FILE}\")\n",
    "    print(\"\\nÔøΩ To create the data files, run the optimized preparation script:\")\n",
    "    print(\"   ```bash\")\n",
    "    print(\"   python parse_prepare.py\")\n",
    "    print(\"   ```\")\n",
    "    print(\"\\n‚ö° This high-performance script features:\")\n",
    "    print(\"   ‚Ä¢ Multiprocessing across all CPU cores\")\n",
    "    print(\"   ‚Ä¢ Progress bars for visual feedback\")\n",
    "    print(\"   ‚Ä¢ Processing rate: ~25,000 records/second\")\n",
    "    print(\"   ‚Ä¢ Creates both CSV and Parquet formats\")\n",
    "    print(\"   ‚Ä¢ Typical runtime: 5-10 minutes for millions of records\")\n",
    "    combined_results = pl.DataFrame()  # Empty dataframe\n",
    "\n",
    "# Quick validation\n",
    "if not combined_results.is_empty():\n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   Total rows: {combined_results.height:,}\")\n",
    "    print(f\"   Columns: {combined_results.width}\")\n",
    "    print(f\"   Key columns: {combined_results.columns}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['drugbank_id', 'uniprot_id', 'RMSD', 'Score1', 'Score2']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_results.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è  Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required columns present\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No data available for analysis\")\n",
    "    print(\"   Please run: python parse_prepare.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc9474",
   "metadata": {},
   "source": [
    "## üß¨ Step 2: Cluster Integration\n",
    "\n",
    "Add cavity cluster information for advanced analysis (run once per session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dcac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üóÉÔ∏è STEP 2: CAVITY CLUSTER INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üóÉÔ∏è CAVITY CLUSTER INTEGRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    # Check if we already have cluster information\n",
    "    if 'cavity_cluster_id' in combined_results.columns:\n",
    "        non_null_clusters = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "        if non_null_clusters > 0:\n",
    "            print(f\"‚úÖ Cluster data already present: {non_null_clusters:,} mapped entries\")\n",
    "            print(\"   Skipping cluster integration...\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Cluster column exists but empty - proceeding with integration...\")\n",
    "    \n",
    "    # Proceed with cluster integration if needed\n",
    "    if 'cavity_cluster_id' not in combined_results.columns or combined_results['cavity_cluster_id'].drop_nulls().len() == 0:\n",
    "        try:\n",
    "            print(f\"üìñ Loading cavity cluster data...\")\n",
    "            cluster_file = \"/opt/data/cavity_space/cavity_cluster_similarity07.csv\"\n",
    "            clusters_df = pl.read_csv(cluster_file, separator='\\t')\n",
    "            print(f\"üìñ Loaded {clusters_df.height:,} clusters from CavitySpace\")\n",
    "            \n",
    "            # Extract uniprot_id and cavity_index from source_dir if not already present\n",
    "            print(f\"ÔøΩ Extracting cavity identifiers from source paths...\")\n",
    "            \n",
    "            combined_results = combined_results.with_columns([\n",
    "                # Extract drugbank_id (1st component: DB00390_ATP1A1_Q13286_cavity_3)\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'(DB\\d+)_[A-Z0-9]+_[A-Z0-9]+_cavity_\\d+', group_index=1)\n",
    "                .alias('extracted_drugbank_id'),\n",
    "                \n",
    "                # Extract gene_name (2nd component: DB00390_ATP1A1_Q13286_cavity_3)\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'DB\\d+_([A-Z0-9]+)_[A-Z0-9]+_cavity_\\d+', group_index=1)\n",
    "                .alias('extracted_gene_name'),\n",
    "                \n",
    "                # Extract uniprot_id (3rd component: DB00390_ATP1A1_Q13286_cavity_3)\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'DB\\d+_[A-Z0-9]+_([A-Z0-9]+)_cavity_\\d+', group_index=1)\n",
    "                .alias('extracted_uniprot_id'),\n",
    "                \n",
    "                # Extract cavity_index (number after 'cavity_')\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'cavity_(\\d+)', group_index=1)\n",
    "                .cast(pl.Int64, strict=False)\n",
    "                .alias('extracted_cavity_index')\n",
    "            ])\n",
    "            \n",
    "            # Check extraction results\n",
    "            non_null_uniprot = combined_results['extracted_uniprot_id'].drop_nulls().len()\n",
    "            non_null_cavity = combined_results['extracted_cavity_index'].drop_nulls().len()\n",
    "            \n",
    "            print(f\"‚úÖ Extraction results:\")\n",
    "            print(f\"   Extracted uniprot_id: {non_null_uniprot:,} non-null values\")\n",
    "            print(f\"   Extracted cavity_index: {non_null_cavity:,} non-null values\")\n",
    "            \n",
    "            # Show sample extracted data for debugging\n",
    "            sample_data = combined_results.select(['source_dir', 'extracted_uniprot_id', 'extracted_cavity_index']).head(3)\n",
    "            print(f\"   Sample extracted data:\")\n",
    "            print(sample_data)\n",
    "            \n",
    "            if non_null_uniprot > 0 and non_null_cavity > 0:\n",
    "                # Create mapping dictionary from the cluster file\n",
    "                cavity_to_cluster = {}\n",
    "                successful_parses = 0\n",
    "                failed_parses = 0\n",
    "                \n",
    "                print(f\"üîÑ Processing cluster file to create mapping...\")\n",
    "                \n",
    "                for i, row in enumerate(clusters_df.to_dicts()):\n",
    "                    cluster_id = row['id']  # The cluster ID\n",
    "                    cavity_items = row['items']  # Comma-separated cavity IDs\n",
    "                    \n",
    "                    # Split the cavity items and process each one\n",
    "                    if cavity_items and isinstance(cavity_items, str):\n",
    "                        cavity_ids = cavity_items.split(',')\n",
    "                        \n",
    "                        for cavity_id in cavity_ids:\n",
    "                            cavity_id = cavity_id.strip()\n",
    "                            \n",
    "                            # Parse cavity format: AF-{UniProtID}-F{Fragment}-model_v1_C{CavityIndex}\n",
    "                            match = re.match(r'AF-([A-Z0-9]+)-F\\d+-model_v1_C(\\d+)', cavity_id)\n",
    "                            if match:\n",
    "                                uniprot_id, cavity_index = match.groups()\n",
    "                                key = (uniprot_id, int(cavity_index))\n",
    "                                cavity_to_cluster[key] = cluster_id\n",
    "                                successful_parses += 1\n",
    "                            else:\n",
    "                                failed_parses += 1\n",
    "                                if failed_parses <= 5:  # Show first few failures\n",
    "                                    print(f\"   ‚ö†Ô∏è Failed to parse cavity ID: '{cavity_id}'\")\n",
    "                \n",
    "                print(f\"üìä Cluster parsing results:\")\n",
    "                print(f\"   Successfully parsed: {successful_parses:,} cavity IDs\")\n",
    "                print(f\"   Failed to parse: {failed_parses:,} cavity IDs\")\n",
    "                print(f\"   Created mapping for {len(cavity_to_cluster):,} unique cavities\")\n",
    "                \n",
    "                # Show sample mapping entries\n",
    "                sample_keys = list(cavity_to_cluster.keys())[:5]\n",
    "                print(f\"   Sample mappings:\")\n",
    "                for key in sample_keys:\n",
    "                    print(f\"     {key} -> cluster {cavity_to_cluster[key]}\")\n",
    "                \n",
    "                # Check what UniProt IDs we have in our data vs cluster file\n",
    "                our_uniprots = set(combined_results.filter(pl.col('extracted_uniprot_id').is_not_null())['extracted_uniprot_id'].unique().to_list())\n",
    "                cluster_uniprots = set(key[0] for key in cavity_to_cluster.keys())\n",
    "                \n",
    "                print(f\"\\nüîç UniProt ID overlap analysis:\")\n",
    "                print(f\"   UniProts in our data: {len(our_uniprots):,}\")\n",
    "                print(f\"   UniProts in cluster file: {len(cluster_uniprots):,}\")\n",
    "                print(f\"   Overlap: {len(our_uniprots & cluster_uniprots):,}\")\n",
    "                \n",
    "                # Show sample UniProts from each set\n",
    "                print(f\"   Sample from our data: {sorted(list(our_uniprots))[:5]}\")\n",
    "                print(f\"   Sample from clusters: {sorted(list(cluster_uniprots))[:5]}\")\n",
    "                \n",
    "                # Map clusters to our data\n",
    "                def map_cluster(uniprot_id, cavity_index):\n",
    "                    if cavity_index is None or uniprot_id is None:\n",
    "                        return None\n",
    "                    key = (uniprot_id, cavity_index)\n",
    "                    return cavity_to_cluster.get(key)\n",
    "                \n",
    "                print(f\"\\nüîÑ Applying cluster mapping...\")\n",
    "                \n",
    "                combined_results = combined_results.with_columns([\n",
    "                    pl.struct(['extracted_uniprot_id', 'extracted_cavity_index'])\n",
    "                    .map_elements(lambda x: map_cluster(x['extracted_uniprot_id'], x['extracted_cavity_index']), return_dtype=pl.Int64)\n",
    "                    .alias('cavity_cluster_id')\n",
    "                ])\n",
    "                \n",
    "                # Report mapping results\n",
    "                mapped_count = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "                total_count = len(combined_results)\n",
    "                unique_clusters = combined_results['cavity_cluster_id'].n_unique()\n",
    "                \n",
    "                print(f\"‚úÖ Cluster mapping complete:\")\n",
    "                print(f\"   Mapped: {mapped_count:,}/{total_count:,} ({mapped_count/total_count*100:.1f}%)\")\n",
    "                print(f\"   Unique clusters: {unique_clusters:,}\")\n",
    "                \n",
    "                if mapped_count > 0:\n",
    "                    # Show sample mapped data\n",
    "                    sample_mapped = combined_results.filter(pl.col('cavity_cluster_id').is_not_null()).select(['extracted_uniprot_id', 'extracted_cavity_index', 'cavity_cluster_id']).head(3)\n",
    "                    print(f\"   Sample mapped data:\")\n",
    "                    print(sample_mapped)\n",
    "                    \n",
    "                # Debug unmapped entries\n",
    "                if mapped_count < total_count:\n",
    "                    print(f\"\\nüîç Debugging unmapped entries:\")\n",
    "                    unmapped = combined_results.filter(pl.col('cavity_cluster_id').is_null())\n",
    "                    sample_unmapped = unmapped.select(['extracted_uniprot_id', 'extracted_cavity_index']).head(5)\n",
    "                    print(f\"   Sample unmapped entries:\")\n",
    "                    print(sample_unmapped)\n",
    "                    \n",
    "                    # Check if these should have mappings\n",
    "                    for row in sample_unmapped.to_dicts():\n",
    "                        uniprot_id = row['extracted_uniprot_id']\n",
    "                        cavity_index = row['extracted_cavity_index']\n",
    "                        key = (uniprot_id, cavity_index)\n",
    "                        if key in cavity_to_cluster:\n",
    "                            print(f\"   ‚ùó Key {key} should map to cluster {cavity_to_cluster[key]} but doesn't!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚úì Key {key} correctly not in cluster mapping\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Extraction failed - adding empty cluster column...\")\n",
    "                combined_results = combined_results.with_columns([\n",
    "                    pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "                ])\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è  Cluster file not found: {cluster_file}\")\n",
    "            print(\"   Adding empty cluster column...\")\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading clusters: {e}\")\n",
    "            print(f\"   Error details: {type(e).__name__}: {str(e)}\")\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "            ])\n",
    "    \n",
    "    print(f\"üéØ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for cluster integration\")\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96afb0",
   "metadata": {},
   "source": [
    "## üîç Step 2.5: Filter for Complete Tool Coverage\n",
    "\n",
    "**Important:** To ensure fair comparison between tools, we filter the dataset to include only drug-target pairs where **ALL THREE tools** (Gold, Smina, LeDock) made predictions. This eliminates bias from partial tool coverage and ensures all subsequent analyses compare tools on an equal footing.\n",
    "\n",
    "This filtering step is performed immediately after data loading to establish a consistent baseline for all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üîç FILTER FOR COMPLETE TOOL COVERAGE\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"üîç Starting filtering process...\")\n",
    "    \n",
    "    # STEP 1: Filter for final_results source_type only\n",
    "    print(\"üìã Step 1: Filtering for final_results source_type...\")\n",
    "    original_rows = combined_results.height\n",
    "    \n",
    "    if 'source_type' in combined_results.columns:\n",
    "        # Check what source_type values we have\n",
    "        source_types = combined_results['source_type'].unique().to_list()\n",
    "        print(f\"   Available source_types: {source_types}\")\n",
    "        \n",
    "        # Filter for final_results only\n",
    "        combined_results = combined_results.filter(pl.col('source_type') == 'final_results')\n",
    "        filtered_rows = combined_results.height\n",
    "        \n",
    "        print(f\"   Original rows: {original_rows:,}\")\n",
    "        print(f\"   After final_results filter: {filtered_rows:,} ({filtered_rows/original_rows*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No source_type column found, skipping source_type filtering\")\n",
    "    \n",
    "    # STEP 2: Analyze tool coverage\n",
    "    print(\"\\nüìã Step 2: Analyzing tool coverage...\")\n",
    "    \n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        # Filter out null values from tool lists\n",
    "        tool1_list = combined_results.filter(pl.col('Tool1').is_not_null())['Tool1'].unique().to_list()\n",
    "        tool2_list = combined_results.filter(pl.col('Tool2').is_not_null())['Tool2'].unique().to_list()\n",
    "        \n",
    "        # Combine and sort, excluding any None values\n",
    "        all_tools = tool1_list + tool2_list\n",
    "        all_detected_tools = sorted([tool for tool in set(all_tools) if tool is not None])\n",
    "        \n",
    "        print(f\"   All detected tools: {all_detected_tools}\")\n",
    "        \n",
    "        # Define the three main tools we expect\n",
    "        expected_tools = ['Gold', 'Smina', 'LeDock']\n",
    "        available_expected_tools = [tool for tool in expected_tools if tool in all_detected_tools]\n",
    "        \n",
    "        print(f\"   Expected tools found: {available_expected_tools}\")\n",
    "        \n",
    "        if len(available_expected_tools) >= 2:  # Need at least 2 tools for comparison\n",
    "            print(f\"\\nüìä Step 3: Checking tool coverage per drug-target pair...\")\n",
    "            \n",
    "            # Group by drug-target pairs and check tool coverage\n",
    "            drug_target_groups = combined_results.group_by(['drugbank_id', 'uniprot_id'])\n",
    "            \n",
    "            complete_coverage_pairs = []\n",
    "            coverage_summary = []\n",
    "            \n",
    "            for group_key, group_data in drug_target_groups:\n",
    "                drug = group_key[0]\n",
    "                target = group_key[1]\n",
    "                \n",
    "                # Get unique tools that made predictions for this drug-target pair\n",
    "                tools_t1 = group_data.filter(pl.col('Tool1').is_not_null())['Tool1'].unique().to_list()\n",
    "                tools_t2 = group_data.filter(pl.col('Tool2').is_not_null())['Tool2'].unique().to_list()\n",
    "                tools_in_group = set(tools_t1 + tools_t2)\n",
    "                tools_present = [tool for tool in available_expected_tools if tool in tools_in_group]\n",
    "                \n",
    "                coverage_summary.append({\n",
    "                    'drug': drug,\n",
    "                    'target': target,\n",
    "                    'tools_present': tools_present,\n",
    "                    'n_tools': len(tools_present),\n",
    "                    'complete_coverage': len(tools_present) == len(available_expected_tools),\n",
    "                    'original_rows': group_data.height\n",
    "                })\n",
    "                \n",
    "                # If all expected tools are present, keep this drug-target pair\n",
    "                if len(tools_present) == len(available_expected_tools):\n",
    "                    complete_coverage_pairs.append((drug, target))\n",
    "            \n",
    "            # STEP 3: Filter for complete coverage pairs\n",
    "            if complete_coverage_pairs:\n",
    "                print(f\"   Found {len(complete_coverage_pairs):,} pairs with complete tool coverage\")\n",
    "                \n",
    "                # Create filter for complete coverage pairs using Polars syntax\n",
    "                complete_filter = pl.lit(False)  # Start with False\n",
    "                \n",
    "                for drug, target in complete_coverage_pairs:\n",
    "                    pair_filter = (pl.col('drugbank_id') == drug) & (pl.col('uniprot_id') == target)\n",
    "                    complete_filter = complete_filter | pair_filter\n",
    "                \n",
    "                # Apply the filter\n",
    "                combined_results = combined_results.filter(complete_filter)\n",
    "                final_rows = combined_results.height\n",
    "                \n",
    "                # Report filtering results\n",
    "                original_pairs = len(coverage_summary)\n",
    "                complete_pairs = len(complete_coverage_pairs)\n",
    "                \n",
    "                print(f\"\\nüìà FILTERING RESULTS:\")\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"üìä Original drug-target pairs: {original_pairs:,}\")\n",
    "                print(f\"‚úÖ Complete coverage pairs: {complete_pairs:,} ({complete_pairs/original_pairs*100:.1f}%)\")\n",
    "                print(f\"üìã After source_type filter: {filtered_rows:,}\")\n",
    "                print(f\"üîÑ Final filtered data: {final_rows:,} ({final_rows/filtered_rows*100:.1f}%)\")\n",
    "                \n",
    "                # Tool coverage distribution\n",
    "                coverage_dist = {}\n",
    "                for item in coverage_summary:\n",
    "                    n_tools = item['n_tools']\n",
    "                    coverage_dist[n_tools] = coverage_dist.get(n_tools, 0) + 1\n",
    "                \n",
    "                print(f\"\\nüéØ TOOL COVERAGE DISTRIBUTION:\")\n",
    "                for n_tools in sorted(coverage_dist.keys(), reverse=True):\n",
    "                    count = coverage_dist[n_tools]\n",
    "                    pct = count / original_pairs * 100\n",
    "                    print(f\"   {n_tools} tools: {count:,} pairs ({pct:.1f}%)\")\n",
    "                \n",
    "                print(f\"\\n‚úÖ Dataset filtered for fair tool comparison\")\n",
    "                print(f\"   Only using drug-target pairs where ALL {len(available_expected_tools)} expected tools made predictions\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå No drug-target pairs have complete tool coverage!\")\n",
    "                print(\"   Cannot proceed with fair comparison analysis\")\n",
    "        else:\n",
    "            print(f\"‚ùå Not enough tools found ({len(available_expected_tools)} < 2)\")\n",
    "    else:\n",
    "        print(\"‚ùå Tool1 or Tool2 columns not found\")\n",
    "        \n",
    "    print(f\"\\nüéØ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for filtering\")\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf873fd4",
   "metadata": {},
   "source": [
    "## üìä Step 3: Data Overview & Quality Check\n",
    "\n",
    "Get familiar with the **filtered** dataset structure and check data quality. This step now analyzes the data after filtering for complete tool coverage, ensuring all statistics reflect the dataset used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc109b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä STEP 3: DATA OVERVIEW & QUALITY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"üîç DATASET OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Shape: {combined_results.shape} (rows √ó columns)\")\n",
    "    print(f\"üíæ Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(f\"üìã Columns: {combined_results.width}\")\n",
    "    \n",
    "    print(f\"\\nüìö Column Names:\")\n",
    "    for i, col in enumerate(combined_results.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüß¨ KEY DATASET STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Core identifiers\n",
    "    print(f\"üî¨ Unique Drug-Target Combinations: {combined_results.select(['drugbank_id', 'uniprot_id']).unique().height:,}\")\n",
    "    print(f\"üíä Unique Drugs (DrugBank IDs): {combined_results['drugbank_id'].n_unique():,}\")\n",
    "    print(f\"üß¨ Unique Proteins (UniProt IDs): {combined_results['uniprot_id'].n_unique():,}\")\n",
    "    \n",
    "    # Check for cavity index information\n",
    "    if 'extracted_cavity_index' in combined_results.columns:\n",
    "        print(f\"üï≥Ô∏è  Unique Cavities: {combined_results['extracted_cavity_index'].n_unique():,}\")\n",
    "    elif 'cavity_index' in combined_results.columns:\n",
    "        print(f\"üï≥Ô∏è  Unique Cavities: {combined_results['cavity_index'].n_unique():,}\")\n",
    "    else:\n",
    "        print(\"üï≥Ô∏è  Cavity information: Not available\")\n",
    "    \n",
    "    # Cluster information\n",
    "    if 'cavity_cluster_id' in combined_results.columns:\n",
    "        cluster_mapped = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "        cluster_total = combined_results.height\n",
    "        unique_clusters = combined_results['cavity_cluster_id'].n_unique()\n",
    "        print(f\"üß© Cavity Clusters: {unique_clusters:,} unique clusters\")\n",
    "        print(f\"   Mapped: {cluster_mapped:,}/{cluster_total:,} ({cluster_mapped/cluster_total*100:.1f}%)\")\n",
    "    \n",
    "    # Check for essential analysis columns\n",
    "    print(f\"\\nüîç DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for RMSD columns\n",
    "    rmsd_columns = [col for col in combined_results.columns if 'rmsd' in col.lower()]\n",
    "    if rmsd_columns:\n",
    "        rmsd_col = rmsd_columns[0]\n",
    "        print(f\"‚úÖ RMSD data available: {rmsd_col}\")\n",
    "        \n",
    "        # RMSD statistics\n",
    "        rmsd_stats = combined_results.select([\n",
    "            pl.col(rmsd_col).min().alias('min_rmsd'),\n",
    "            pl.col(rmsd_col).max().alias('max_rmsd'),\n",
    "            pl.col(rmsd_col).mean().alias('mean_rmsd'),\n",
    "            pl.col(rmsd_col).median().alias('median_rmsd'),\n",
    "            (pl.col(rmsd_col) < 2.0).mean().alias('good_poses_pct')\n",
    "        ]).to_pandas().iloc[0]\n",
    "        \n",
    "        print(f\"   Range: {rmsd_stats['min_rmsd']:.2f} - {rmsd_stats['max_rmsd']:.2f} √Ö\")\n",
    "        print(f\"   Mean: {rmsd_stats['mean_rmsd']:.2f} √Ö, Median: {rmsd_stats['median_rmsd']:.2f} √Ö\")\n",
    "        print(f\"   Good poses (RMSD < 2.0 √Ö): {rmsd_stats['good_poses_pct']*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No RMSD columns found - pose consistency analysis may be limited\")\n",
    "    \n",
    "    # Check for score columns\n",
    "    score_columns = [col for col in combined_results.columns if 'score' in col.lower()]\n",
    "    print(f\"‚úÖ Score columns available: {len(score_columns)}\")\n",
    "    for col in score_columns[:5]:  # Show first 5 score columns\n",
    "        print(f\"   - {col}\")\n",
    "    if len(score_columns) > 5:\n",
    "        print(f\"   ... and {len(score_columns) - 5} more\")\n",
    "    \n",
    "    # Tool information\n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        tool1_unique = combined_results.filter(pl.col('Tool1').is_not_null())['Tool1'].n_unique()\n",
    "        tool2_unique = combined_results.filter(pl.col('Tool2').is_not_null())['Tool2'].n_unique()\n",
    "        print(f\"üîß Tool1 variants: {tool1_unique}\")\n",
    "        print(f\"üîß Tool2 variants: {tool2_unique}\")\n",
    "    \n",
    "    # Source information\n",
    "    if 'source_type' in combined_results.columns:\n",
    "        source_types = combined_results['source_type'].value_counts().to_pandas()\n",
    "        print(f\"üìÅ Source types:\")\n",
    "        for _, row in source_types.iterrows():\n",
    "            print(f\"   - {row['source_type']}: {row['count']:,} rows\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset quality check complete\")\n",
    "    print(f\"üéØ Ready for consensus analysis!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for overview\")\n",
    "    print(\"   Please load and filter data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76502af3",
   "metadata": {},
   "source": [
    "## üî¨ Step 4: Research Question 1 - Tool Reliability & Consensus Analysis\n",
    "\n",
    "**Research Question 1:** *How reliable is consensus between different docking tools?*\n",
    "\n",
    "This analysis addresses one of the most fundamental questions in computational drug discovery: **When can we trust docking predictions?** \n",
    "\n",
    "**Note:** All analysis now uses the filtered dataset with complete tool coverage, ensuring fair comparisons between tools.\n",
    "\n",
    "### üéØ Analysis Goals:\n",
    "1. **Quantify agreement** between different docking tools (Gold, Smina, LeDock)\n",
    "2. **Identify tool pairs** that show the best/worst consensus\n",
    "3. **Understand when** docking predictions are most trustworthy\n",
    "4. **Establish quality thresholds** for reliable predictions\n",
    "\n",
    "### üìä Key Metrics:\n",
    "- **RMSD between tools** - Lower values indicate better pose agreement\n",
    "- **Tool agreement frequency** - How often tools produce similar results\n",
    "- **Score-RMSD correlation** - Relationship between structural and scoring agreement\n",
    "\n",
    "### üìà Main Visualization:\n",
    "- **Tool Agreement Distribution Plot** - Comprehensive analysis combining RMSD distributions and score patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üõ†Ô∏è SETUP: Import Libraries and Prepare Data for Tool Reliability Analysis\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "\n",
    "# Check data availability for tool reliability analysis\n",
    "if not combined_results.is_empty():\n",
    "    print(f\"\\nüîç CHECKING FILTERED DATA SUITABILITY FOR TOOL RELIABILITY ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìã NOTE: Analysis uses filtered data with complete tool coverage\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_columns = ['Tool1', 'Tool2', 'Score1', 'Score2', 'RMSD']\n",
    "    available_columns = [col for col in required_columns if col in combined_results.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_results.columns]\n",
    "    \n",
    "    print(f\"‚úÖ Available columns: {available_columns}\")\n",
    "    if missing_columns:\n",
    "        print(f\"‚ö†Ô∏è  Missing columns: {missing_columns}\")\n",
    "        print(\"   Analysis will be adapted based on available data\")\n",
    "    \n",
    "    # Check tool information\n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        tool1_types = combined_results['Tool1'].unique().to_list()\n",
    "        tool2_types = combined_results['Tool2'].unique().to_list()\n",
    "        all_tools = list(set(tool1_types + tool2_types))\n",
    "        \n",
    "        print(f\"\\nüîß Detected docking tools: {all_tools}\")\n",
    "        print(f\"   Tool1 variants: {tool1_types}\")\n",
    "        print(f\"   Tool2 variants: {tool2_types}\")\n",
    "        \n",
    "        # Check for RMSD data\n",
    "        if 'RMSD' in combined_results.columns:\n",
    "            rmsd_stats = combined_results.select([\n",
    "                pl.col('RMSD').count().alias('total_comparisons'),\n",
    "                pl.col('RMSD').mean().alias('mean_rmsd'),\n",
    "                pl.col('RMSD').std().alias('std_rmsd'),\n",
    "                pl.col('RMSD').min().alias('min_rmsd'),\n",
    "                pl.col('RMSD').max().alias('max_rmsd')\n",
    "            ]).to_pandas().iloc[0]\n",
    "            \n",
    "            print(f\"\\nüìê RMSD Statistics (Tool Agreement Metric):\")\n",
    "            print(f\"   Total pairwise comparisons: {rmsd_stats['total_comparisons']:,}\")\n",
    "            print(f\"   Mean RMSD: {rmsd_stats['mean_rmsd']:.2f} ¬± {rmsd_stats['std_rmsd']:.2f} √Ö\")\n",
    "            print(f\"   Range: {rmsd_stats['min_rmsd']:.2f} - {rmsd_stats['max_rmsd']:.2f} √Ö\")\n",
    "            \n",
    "            # Calculate agreement categories\n",
    "            excellent_agreement = combined_results.filter(pl.col('RMSD') <= 1.0).height\n",
    "            good_agreement = combined_results.filter((pl.col('RMSD') > 1.0) & (pl.col('RMSD') <= 2.0)).height\n",
    "            poor_agreement = combined_results.filter(pl.col('RMSD') > 2.0).height\n",
    "            total_comparisons = combined_results.height\n",
    "            \n",
    "            print(f\"\\nüéØ Tool Agreement Categories:\")\n",
    "            print(f\"   Excellent (RMSD ‚â§ 1.0 √Ö): {excellent_agreement:,} ({excellent_agreement/total_comparisons*100:.1f}%)\")\n",
    "            print(f\"   Good (1.0 < RMSD ‚â§ 2.0 √Ö): {good_agreement:,} ({good_agreement/total_comparisons*100:.1f}%)\")\n",
    "            print(f\"   Poor (RMSD > 2.0 √Ö): {poor_agreement:,} ({poor_agreement/total_comparisons*100:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ Filtered data is suitable for tool reliability analysis!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  No RMSD data available - analysis will be limited to score comparisons\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Missing tool information - cannot perform tool reliability analysis\")\n",
    "        print(\"   Please ensure the dataset contains Tool1 and Tool2 columns\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")\n",
    "    print(\"   Please run Steps 1-3 first to load, filter, and prepare the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fff2a9",
   "metadata": {},
   "source": [
    "### üìä Tool Agreement & Score Distribution Analysis\n",
    "\n",
    "**Purpose:** Comprehensive analysis of tool agreement and scoring behavior addressing Research Question 1.\n",
    "\n",
    "**Layout Structure:**\n",
    "1. **Top Plot:** RMSD distribution violin plots showing tool agreement quality\n",
    "2. **Three Ridge Plots:** One subplot per tool (GOLD, LeDock, Smina) showing score distributions across tool pairs and RMSD ranges\n",
    "\n",
    "**Analysis Logic:**\n",
    "- **Best Agreement Focus:** For each drug-target pair, only the cavity/pose with the lowest RMSD between tools is considered\n",
    "- **Unique Tool Pairs:** Avoids counting A-B and B-A as separate comparisons\n",
    "- **Tool-Specific Scoring:** Each tool's scores are analyzed separately (never mixed) since each has its own scoring function\n",
    "\n",
    "**Ridge Plot Features:**\n",
    "- **Color-blind friendly palette:** Orange (GOLD), Sky Blue (LeDock), Bluish Green (Smina)\n",
    "- **RMSD Stratification:** Scores grouped by Good (<2√Ö), Medium (2-5√Ö), Poor (>5√Ö) RMSD ranges\n",
    "- **Fixed Scales:** GOLD (0-100), LeDock/Smina (-12 to 5) for consistent comparison\n",
    "- **Clean Design:** No annotations for cleaner, poster-ready appearance\n",
    "\n",
    "**Scientific Rationale:**\n",
    "- **No Score Mixing:** GOLD, Smina, and LeDock each use different scoring functions, so their scores should never be directly compared or combined\n",
    "- **Tool-Specific Insights:** Shows how each tool's scoring function relates to structural agreement quality\n",
    "- **Fair Comparison:** Enables assessment of whether good/poor RMSD agreement corresponds to favorable/unfavorable scores for each individual tool\n",
    "\n",
    "**Interpretation:**\n",
    "- **Agreement Quality:** Lower, narrower RMSD distributions indicate better and more consistent tool agreement\n",
    "- **Score-Quality Relationship:** For each tool, check if good RMSD ranges show more favorable scores than poor ranges\n",
    "- **Tool Behavior:** Compare how different tools' scoring functions relate to structural agreement\n",
    "\n",
    "**Poster-Ready Features:** Color-blind friendly palette, clear borders, proper axis labels with units, and optimized layout for maximum presentation impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb67f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä PLOT 1A: TOOL AGREEMENT DISTRIBUTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty() and 'RMSD' in combined_results.columns and 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "    \n",
    "    print(\"üî• Generating Tool Agreement Distribution Analysis...\")\n",
    "    \n",
    "    # Set up poster-quality plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (16, 20),      # Vertical layout - taller figure\n",
    "        'font.size': 18,                 # Even larger base font size\n",
    "        'axes.titlesize': 22,            # Even larger title font\n",
    "        'axes.labelsize': 20,            # Even larger axis labels\n",
    "        'xtick.labelsize': 16,           # Even larger tick labels\n",
    "        'ytick.labelsize': 16,           # Even larger tick labels\n",
    "        'legend.fontsize': 16,           # Even larger legend font\n",
    "        'figure.titlesize': 26           # Even larger figure title\n",
    "    })\n",
    "    \n",
    "    # Convert to pandas for easier manipulation\n",
    "    df = combined_results.to_pandas()\n",
    "    \n",
    "    # Get all unique tools\n",
    "    all_tools = sorted(list(set(df['Tool1'].unique().tolist() + df['Tool2'].unique().tolist())))\n",
    "    print(f\"üîß Detected tools: {all_tools}\")\n",
    "    \n",
    "    # Check for score columns\n",
    "    score_columns = [col for col in df.columns if any(x in col.lower() for x in ['score', 'energy', 'affinity'])]\n",
    "    print(f\"üìä Available scoring columns: {score_columns}\")\n",
    "    \n",
    "    # Collect RMSD distributions and scores for unique tool pairs\n",
    "    print(\"üìä Computing tool agreement distributions using lowest RMSD per drug-target pair...\")\n",
    "    \n",
    "    tool_pair_data = []\n",
    "    \n",
    "    # Only consider unique pairs (avoid duplicates like A-B and B-A)\n",
    "    from itertools import combinations\n",
    "    unique_tool_pairs = list(combinations(all_tools, 2))\n",
    "    \n",
    "    for tool1, tool2 in unique_tool_pairs:\n",
    "        # Find all comparisons between these two tools (both directions)\n",
    "        mask1 = (df['Tool1'] == tool1) & (df['Tool2'] == tool2)\n",
    "        mask2 = (df['Tool1'] == tool2) & (df['Tool2'] == tool1)\n",
    "        \n",
    "        if mask1.any() or mask2.any():\n",
    "            # We need to track which tool is which for score assignment\n",
    "            # Include score information and tool identity\n",
    "            columns_to_include = ['drugbank_id', 'uniprot_id', 'cavity_index', 'RMSD', 'Tool1', 'Tool2'] + score_columns[:2]\n",
    "            \n",
    "            # Separate the two directions to maintain tool identity\n",
    "            comparisons_direction1 = df.loc[mask1, columns_to_include].copy() if mask1.any() else pd.DataFrame()\n",
    "            comparisons_direction2 = df.loc[mask2, columns_to_include].copy() if mask2.any() else pd.DataFrame()\n",
    "            \n",
    "            # For direction 2, we need to swap the tools and scores to maintain consistency\n",
    "            if not comparisons_direction2.empty:\n",
    "                # Create a copy and swap Tool1/Tool2 and Score1/Score2\n",
    "                comparisons_direction2_swapped = comparisons_direction2.copy()\n",
    "                comparisons_direction2_swapped['Tool1'], comparisons_direction2_swapped['Tool2'] = comparisons_direction2['Tool2'], comparisons_direction2['Tool1']\n",
    "                if 'Score1' in comparisons_direction2_swapped.columns and 'Score2' in comparisons_direction2_swapped.columns:\n",
    "                    comparisons_direction2_swapped['Score1'], comparisons_direction2_swapped['Score2'] = comparisons_direction2['Score2'], comparisons_direction2['Score1']\n",
    "                comparisons_direction2 = comparisons_direction2_swapped\n",
    "            \n",
    "            # Combine both directions\n",
    "            if not comparisons_direction1.empty and not comparisons_direction2.empty:\n",
    "                comparisons = pd.concat([comparisons_direction1, comparisons_direction2])\n",
    "            elif not comparisons_direction1.empty:\n",
    "                comparisons = comparisons_direction1\n",
    "            elif not comparisons_direction2.empty:\n",
    "                comparisons = comparisons_direction2\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Group by unique drug-target combinations and get the lowest RMSD for each\n",
    "            grouped = comparisons.groupby(['drugbank_id', 'uniprot_id'])\n",
    "            \n",
    "            best_agreements = []\n",
    "            for (drug, target), group in grouped:\n",
    "                best_idx = group['RMSD'].idxmin()\n",
    "                best_row = group.loc[best_idx]\n",
    "                \n",
    "                agreement_data = {\n",
    "                    'drug': drug,\n",
    "                    'target': target,\n",
    "                    'rmsd': best_row['RMSD'],\n",
    "                    'tool1_name': best_row['Tool1'],  # This should be tool1\n",
    "                    'tool2_name': best_row['Tool2'],  # This should be tool2\n",
    "                }\n",
    "                \n",
    "                # Add scores for each tool\n",
    "                if 'Score1' in best_row and pd.notna(best_row['Score1']):\n",
    "                    agreement_data['tool1_score'] = best_row['Score1']  # Score for tool1\n",
    "                if 'Score2' in best_row and pd.notna(best_row['Score2']):\n",
    "                    agreement_data['tool2_score'] = best_row['Score2']  # Score for tool2\n",
    "                \n",
    "                best_agreements.append(agreement_data)\n",
    "            \n",
    "            if len(best_agreements) > 0:\n",
    "                rmsds = [item['rmsd'] for item in best_agreements]\n",
    "                \n",
    "                tool_pair_data.append({\n",
    "                    'tool_pair': f\"{tool1} vs {tool2}\",\n",
    "                    'tool1': tool1,\n",
    "                    'tool2': tool2,\n",
    "                    'rmsds': np.array(rmsds),\n",
    "                    'agreements': best_agreements,\n",
    "                    'n_pairs': len(best_agreements),\n",
    "                    'mean_rmsd': np.mean(rmsds),\n",
    "                    'median_rmsd': np.median(rmsds),\n",
    "                    'std_rmsd': np.std(rmsds)\n",
    "                })\n",
    "                print(f\"   {tool1} vs {tool2}: {len(best_agreements):,} unique drug-target pairs, Mean: {np.mean(rmsds):.2f} √Ö\")\n",
    "    \n",
    "    if tool_pair_data:\n",
    "        # Create main figure with custom layout: top plot + 3 horizontal ridge plots\n",
    "        fig = plt.figure(figsize=(15, 25))\n",
    "        gs = fig.add_gridspec(2, 3, height_ratios=[1, 1.5], hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # ========================================================================\n",
    "        # PLOT 1: RMSD Distribution Violin Plots (Top - spans all columns)\n",
    "        # ========================================================================\n",
    "        ax_rmsd = fig.add_subplot(gs[0, :])\n",
    "        \n",
    "        tool_pair_names = [data['tool_pair'] for data in tool_pair_data]\n",
    "        rmsd_distributions = [data['rmsds'] for data in tool_pair_data]\n",
    "        \n",
    "        # Create violin plot with clear borders\n",
    "        violins = ax_rmsd.violinplot(rmsd_distributions, positions=range(len(tool_pair_names)), \n",
    "                                    showmeans=False, showmedians=False, showextrema=False)\n",
    "        \n",
    "        # Customize violin colors with clear borders\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(tool_pair_names)))\n",
    "        for i, violin in enumerate(violins['bodies']):\n",
    "            violin.set_facecolor(colors[i])\n",
    "            violin.set_alpha(0.7)\n",
    "            violin.set_edgecolor('black')  # Clear borders\n",
    "            violin.set_linewidth(2)       # Thick borders\n",
    "        \n",
    "        ax_rmsd.set_xticks(range(len(tool_pair_names)))\n",
    "        # Simplified x-axis labels - just the tool pair names\n",
    "        simplified_labels = [name.replace(' vs ', '\\nvs\\n') for name in tool_pair_names]\n",
    "        ax_rmsd.set_xticklabels(simplified_labels, rotation=0, ha='center', fontsize=16)\n",
    "        ax_rmsd.set_ylabel('RMSD (√Ö)', fontsize=20, fontweight='bold')  # Add y-axis label\n",
    "        ax_rmsd.grid(True, alpha=0.3)\n",
    "        ax_rmsd.set_ylim(0, min(15, max([data['rmsds'].max() for data in tool_pair_data])))\n",
    "        \n",
    "        # ========================================================================\n",
    "        # PLOTS 2-4: Ridge-style Score Distributions (Bottom - 3 horizontal plots)\n",
    "        # ========================================================================\n",
    "        \n",
    "        # Check if we have score data\n",
    "        has_score_data = any('tool1_score' in agreement or 'tool2_score' in agreement \n",
    "                           for data in tool_pair_data for agreement in data['agreements'])\n",
    "        \n",
    "        if has_score_data and len(tool_pair_data) > 0:\n",
    "            print(\"üìä Creating ridge-style score distribution analysis...\")\n",
    "            \n",
    "            # Define partner tool colors (color-blind friendly palette)\n",
    "            partner_tool_colors = {\n",
    "                'GOLD': '#E69F00',     # Orange (deuteranopia/protanopia safe)\n",
    "                'LeDock': '#56B4E9',   # Sky Blue (tritanopia safe)\n",
    "                'Smina': '#009E73'     # Bluish Green (all color-blind types safe)\n",
    "            }\n",
    "            \n",
    "            # Define RMSD ranges with actual numeric values\n",
    "            rmsd_ranges = [\n",
    "                (0, 2, \"<2√Ö\"),\n",
    "                (2, 5, \"2-5√Ö\"),\n",
    "                (5, float('inf'), \">5√Ö\")\n",
    "            ]\n",
    "            \n",
    "            # Create horizontal ridge plots for each tool\n",
    "            for tool_idx, tool in enumerate(all_tools):\n",
    "                if tool_idx > 2:  # Only use first 3 tools (we have 3 horizontal subplots)\n",
    "                    break\n",
    "                    \n",
    "                ax_tool = fig.add_subplot(gs[1, tool_idx])\n",
    "                \n",
    "                print(f\"   Creating ridge plot for {tool}...\")\n",
    "                \n",
    "                # Collect all score data for this tool across all tool pairs and RMSD ranges\n",
    "                y_position = 0\n",
    "                y_spacing = 0.8  # Overlapping ridges\n",
    "                max_y = 0\n",
    "                ridge_data = []  # Store ridge data for annotations\n",
    "                \n",
    "                # For each tool pair involving this tool\n",
    "                for pair_idx, data in enumerate(tool_pair_data):\n",
    "                    # Check if this tool is tool1 or tool2 in the pair\n",
    "                    if data['tool1'] == tool:\n",
    "                        partner_tool = data['tool2']\n",
    "                        score_key = 'tool1_score'\n",
    "                    elif data['tool2'] == tool:\n",
    "                        partner_tool = data['tool1']\n",
    "                        score_key = 'tool2_score'\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get partner tool color (use partner tool's color, not current tool's color)\n",
    "                    ridge_color = partner_tool_colors.get(partner_tool, '#ADD8E6')\n",
    "                    \n",
    "                    # For each RMSD range\n",
    "                    for range_idx, (rmsd_min, rmsd_max, range_name) in enumerate(rmsd_ranges):\n",
    "                        # Collect scores for this tool in this RMSD range\n",
    "                        tool_scores = []\n",
    "                        for agreement in data['agreements']:\n",
    "                            if (rmsd_min <= agreement['rmsd'] < rmsd_max and \n",
    "                                score_key in agreement and pd.notna(agreement[score_key])):\n",
    "                                tool_scores.append(agreement[score_key])\n",
    "                        \n",
    "                        if len(tool_scores) >= 5:  # Only plot if we have sufficient data\n",
    "                            try:\n",
    "                                from scipy.stats import gaussian_kde\n",
    "                                \n",
    "                                # Create density curve\n",
    "                                kde = gaussian_kde(tool_scores)\n",
    "                                score_range = np.linspace(min(tool_scores), max(tool_scores), 100)\n",
    "                                density = kde(score_range)\n",
    "                                \n",
    "                                # Normalize density for ridge plot appearance\n",
    "                                density = density / density.max() * 0.6  # Scale height\n",
    "                                \n",
    "                                # Plot the ridge with clear borders\n",
    "                                ax_tool.fill_between(score_range, y_position, y_position + density, \n",
    "                                                    alpha=0.8, color=ridge_color, \n",
    "                                                    edgecolor='black', linewidth=1.5)\n",
    "                                \n",
    "                                # Store ridge data for annotation (removed RMSD annotations)\n",
    "                                ridge_data.append({\n",
    "                                    'y_center': y_position + 0.3,\n",
    "                                    'x_center': np.median(tool_scores),\n",
    "                                    'partner_tool': partner_tool\n",
    "                                })\n",
    "                                \n",
    "                                y_position += y_spacing\n",
    "                                max_y = max(max_y, y_position)\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"Warning: Could not create KDE for {tool} vs {partner_tool} in {range_name} range: {e}\")\n",
    "                                continue\n",
    "                \n",
    "                # Customize the subplot - add x-axis label with units\n",
    "                ax_tool.set_ylabel('')  # Remove y-axis title\n",
    "                if tool in ['LeDock', 'Smina']:\n",
    "                    ax_tool.set_xlabel(f'{tool} Score (kcal/mol)', fontsize=18, fontweight='bold')  # Add units\n",
    "                else:  # GOLD has no unit\n",
    "                    ax_tool.set_xlabel(f'{tool} Score', fontsize=18, fontweight='bold')  # No unit for GOLD\n",
    "                ax_tool.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Set appropriate x-axis limits based on tool - extend LeDock/Smina to include positive values\n",
    "                if tool == 'GOLD':\n",
    "                    ax_tool.set_xlim(0, 100)\n",
    "                else:  # LeDock and Smina use negative scores, but extend to positive values up to 5\n",
    "                    ax_tool.set_xlim(-12, 5)\n",
    "                \n",
    "                # Set y-axis limits and remove ticks\n",
    "                if max_y > 0:\n",
    "                    ax_tool.set_ylim(-0.2, max_y + 0.2)\n",
    "                ax_tool.set_yticks([])\n",
    "                \n",
    "                # Remove RMSD range annotations (simplified for cleaner look)\n",
    "                # for ridge_info in ridge_data:\n",
    "                #     annotation_text = f\"{ridge_info['range_name']} vs {ridge_info['partner_tool']}\"\n",
    "                #     ax_tool.text(ridge_info['x_center'], ridge_info['y_center'], annotation_text,\n",
    "                #                ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                #                color=range_colors.get(ridge_info['range_name'], 'black'),\n",
    "                #                bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
    "                \n",
    "                if max_y == 0:  # No data was plotted\n",
    "                    ax_tool.text(0.5, 0.5, f'Insufficient score data\\nfor {tool}', \n",
    "                               ha='center', va='center', transform=ax_tool.transAxes, \n",
    "                               fontsize=16, fontweight='bold')\n",
    "        \n",
    "        else:\n",
    "            # No score data - show message in remaining subplots\n",
    "            for i in range(3):\n",
    "                ax_tool = fig.add_subplot(gs[1, i])\n",
    "                ax_tool.text(0.5, 0.5, 'No score data available', \n",
    "                           ha='center', va='center', transform=ax_tool.transAxes, \n",
    "                           fontsize=18, fontweight='bold')\n",
    "                # Add x-axis labels even when no data\n",
    "                if i < len(all_tools):\n",
    "                    if all_tools[i] in ['LeDock', 'Smina']:\n",
    "                        ax_tool.set_xlabel(f'{all_tools[i]} Score (kcal/mol)', fontsize=18, fontweight='bold')\n",
    "                    else:\n",
    "                        ax_tool.set_xlabel(f'{all_tools[i]} Score', fontsize=18, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout(pad=1.5)  # Add padding for better spacing\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed statistics - also update the printed ranges to use actual values\n",
    "        print(f\"\\nüìà SCORE DISTRIBUTION BY TOOL PAIR AND RMSD RANGE:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for data in tool_pair_data:\n",
    "            print(f\"\\nüîß {data['tool_pair']}:\")\n",
    "            \n",
    "            # Define RMSD ranges\n",
    "            rmsd_ranges = [\n",
    "                (0, 2, \"<2√Ö\"),\n",
    "                (2, 5, \"2-5√Ö\"),\n",
    "                (5, float('inf'), \">5√Ö\")\n",
    "            ]\n",
    "            \n",
    "            for i, (min_rmsd, max_rmsd, range_name) in enumerate(rmsd_ranges):\n",
    "                # Count scores for each tool in this range\n",
    "                tool1_scores = []\n",
    "                tool2_scores = []\n",
    "                \n",
    "                for agreement in data['agreements']:\n",
    "                    if min_rmsd <= agreement['rmsd'] < max_rmsd:\n",
    "                        if 'tool1_score' in agreement and pd.notna(agreement['tool1_score']):\n",
    "                            tool1_scores.append(agreement['tool1_score'])\n",
    "                        if 'tool2_score' in agreement and pd.notna(agreement['tool2_score']):\n",
    "                            tool2_scores.append(agreement['tool2_score'])\n",
    "                \n",
    "                print(f\"   {range_name}:\")\n",
    "                if tool1_scores:\n",
    "                    print(f\"     {data['tool1']}: n={len(tool1_scores)}, Mean={np.mean(tool1_scores):.2f}¬±{np.std(tool1_scores):.2f}\")\n",
    "                else:\n",
    "                    print(f\"     {data['tool1']}: No data\")\n",
    "                    \n",
    "                if tool2_scores:\n",
    "                    print(f\"     {data['tool2']}: n={len(tool2_scores)}, Mean={np.mean(tool2_scores):.2f}¬±{np.std(tool2_scores):.2f}\")\n",
    "                else:\n",
    "                    print(f\"     {data['tool2']}: No data\")\n",
    "        \n",
    "        # Summary statistics for RMSD agreement\n",
    "        print(f\"\\nüìà TOOL AGREEMENT SUMMARY:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Sort by mean agreement (best to worst)\n",
    "        sorted_pairs = sorted(tool_pair_data, key=lambda x: x['mean_rmsd'])\n",
    "        \n",
    "        print(\"üèÜ Tool pairs ranked by agreement (best to worst):\")\n",
    "        for i, data in enumerate(sorted_pairs, 1):\n",
    "            print(f\"   {i}. {data['tool_pair']}: {data['mean_rmsd']:.2f} ¬± {data['std_rmsd']:.2f} √Ö \"\n",
    "                  f\"(median: {data['median_rmsd']:.2f} √Ö, n={data['n_pairs']:,})\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        all_means = [data['mean_rmsd'] for data in tool_pair_data]\n",
    "        best_agreement = min(all_means)\n",
    "        worst_agreement = max(all_means)\n",
    "        \n",
    "        print(f\"\\nüìä Overall agreement statistics:\")\n",
    "        print(f\"   Best tool pair agreement: {best_agreement:.2f} √Ö\")\n",
    "        print(f\"   Worst tool pair agreement: {worst_agreement:.2f} √Ö\")\n",
    "        print(f\"   Average across all pairs: {np.mean(all_means):.2f} ¬± {np.std(all_means):.2f} √Ö\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No valid tool pairs found for agreement analysis\")\n",
    "    \n",
    "    # Reset matplotlib parameters to defaults\n",
    "    plt.rcdefaults()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot generate tool agreement distribution - missing required data\")\n",
    "    print(\"   Required: RMSD, Tool1, Tool2 columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6c921",
   "metadata": {},
   "source": [
    "## Research Question 2: Cluster-Based Distinction of Preferred vs Non-Preferred Cavities\n",
    "\n",
    "This section investigates whether **cavity cluster membership** can distinguish between **preferred and non-preferred binding sites** for each drug-target combination.\n",
    "\n",
    "### Key Research Question:\n",
    "**Can we distinguish between preferred cavities and other cavities based on their cluster membership patterns?**\n",
    "\n",
    "### Analysis Strategy:\n",
    "1. **Define Preferred Cavities**: For each unique drug-uniprot combination, identify preferred cavities using multiple criteria:\n",
    "   - **Lowest RMSD** (best structural agreement between tools)\n",
    "   - **Best Score1** (optimal docking score from primary tool)\n",
    "   - **Best Score2** (optimal docking score from secondary tool)\n",
    "\n",
    "2. **Cluster Comparison**: Compare the cluster membership patterns between:\n",
    "   - **Preferred cavities** (selected by each criterion)\n",
    "   - **Non-preferred cavities** (all other cavities for the same drug-target pair)\n",
    "\n",
    "3. **Statistical Analysis**: Test whether preferred and non-preferred cavities come from significantly different cluster distributions\n",
    "\n",
    "4. **Visualization**: Generate comparative plots showing cluster characteristics for preferred vs non-preferred cavities\n",
    "\n",
    "### Expected Insights:\n",
    "- Whether certain cavity clusters are systematically preferred across drug-target pairs\n",
    "- How different preference criteria (RMSD vs scores) affect cluster selection patterns\n",
    "- Statistical significance of cluster-based cavity preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Research Question 2: Preferred vs Non-Preferred Cavity Analysis ===\")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "df_rq2 = combined_results.to_pandas()\n",
    "\n",
    "# Check if we have cluster information\n",
    "if 'cavity_cluster_id' not in df_rq2.columns:\n",
    "    print(\"‚ùå No cluster information available - cannot perform cluster-based analysis\")\n",
    "    rq2_analysis = None\n",
    "else:\n",
    "    print(\"‚úÖ Cluster information available - proceeding with analysis\")\n",
    "    \n",
    "    # Remove rows without cluster information\n",
    "    df_rq2_clean = df_rq2.dropna(subset=['cavity_cluster_id']).copy()\n",
    "    print(f\"Dataset for RQ2: {len(df_rq2_clean):,} rows with cluster information\")\n",
    "    \n",
    "    # First, let's extract individual tool scores for each cavity\n",
    "    print(\"\\nüîç Extracting individual tool scores for each cavity...\")\n",
    "    \n",
    "    # Create a comprehensive dataset with all tool scores per cavity\n",
    "    cavity_scores = {}  # Key: (drug, target, cavity), Value: {tool_scores, rmsd_info, cluster}\n",
    "    \n",
    "    for _, row in df_rq2_clean.iterrows():\n",
    "        key = (row['drugbank_id'], row['uniprot_id'], row['cavity_index'])\n",
    "        \n",
    "        if key not in cavity_scores:\n",
    "            cavity_scores[key] = {\n",
    "                'drug': row['drugbank_id'],\n",
    "                'target': row['uniprot_id'],\n",
    "                'cavity_index': row['cavity_index'],\n",
    "                'cluster_id': row['cavity_cluster_id'],\n",
    "                'tool_scores': {},\n",
    "                'rmsd_data': []\n",
    "            }\n",
    "        \n",
    "        # Store individual tool scores\n",
    "        tool1 = row['Tool1']\n",
    "        tool2 = row['Tool2']\n",
    "        \n",
    "        if pd.notna(tool1) and pd.notna(row['Score1']):\n",
    "            cavity_scores[key]['tool_scores'][tool1] = row['Score1']\n",
    "        if pd.notna(tool2) and pd.notna(row['Score2']):\n",
    "            cavity_scores[key]['tool_scores'][tool2] = row['Score2']\n",
    "        \n",
    "        # Store RMSD information\n",
    "        cavity_scores[key]['rmsd_data'].append({\n",
    "            'tool1': tool1,\n",
    "            'tool2': tool2,\n",
    "            'rmsd': row['RMSD']\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Extracted scores for {len(cavity_scores):,} unique cavities\")\n",
    "    \n",
    "    # Calculate minimum RMSD for each cavity (across all tool pairs)\n",
    "    # This represents the best consensus pose for each cavity\n",
    "    for key in cavity_scores:\n",
    "        rmsd_values = [item['rmsd'] for item in cavity_scores[key]['rmsd_data'] if pd.notna(item['rmsd'])]\n",
    "        cavity_scores[key]['min_rmsd'] = np.min(rmsd_values) if rmsd_values else np.nan\n",
    "        cavity_scores[key]['avg_rmsd'] = np.mean(rmsd_values) if rmsd_values else np.nan  # Keep for reference\n",
    "    \n",
    "    # Define preference criteria with proper tool names\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': 'Best Consensus (Min RMSD)',\n",
    "        'best_gold': 'Best GOLD Score',\n",
    "        'best_ledock': 'Best LeDock Score',\n",
    "        'best_smina': 'Best Smina Score'\n",
    "    }\n",
    "    \n",
    "    # Initialize results storage\n",
    "    rq2_analysis = {\n",
    "        'preferred_cavities': {},\n",
    "        'cluster_comparisons': {},\n",
    "        'statistical_tests': {},\n",
    "        'summary_stats': {}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(preference_criteria)} preference criteria...\")\n",
    "    \n",
    "    # For each preference criterion\n",
    "    for criterion_key, criterion_name in preference_criteria.items():\n",
    "        print(f\"\\n--- Analyzing {criterion_name} preference ---\")\n",
    "        \n",
    "        # Storage for this criterion\n",
    "        preferred_cavities = []\n",
    "        non_preferred_cavities = []\n",
    "        preferred_clusters = []\n",
    "        non_preferred_clusters = []\n",
    "        \n",
    "        # Group cavities by drug-target pairs\n",
    "        drug_target_cavities = {}\n",
    "        for key, cavity_data in cavity_scores.items():\n",
    "            dt_key = (cavity_data['drug'], cavity_data['target'])\n",
    "            if dt_key not in drug_target_cavities:\n",
    "                drug_target_cavities[dt_key] = []\n",
    "            drug_target_cavities[dt_key].append(cavity_data)\n",
    "        \n",
    "        valid_pairs = 0\n",
    "        \n",
    "        for (drug, target), cavities in drug_target_cavities.items():\n",
    "            if len(cavities) < 2:  # Need at least 2 cavities to distinguish preferred vs non-preferred\n",
    "                continue\n",
    "            \n",
    "            # Filter cavities based on available data for this criterion\n",
    "            if criterion_key == 'lowest_rmsd':\n",
    "                valid_cavities = [c for c in cavities if pd.notna(c['min_rmsd'])]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with lowest minimum RMSD (best consensus)\n",
    "                preferred_cavity = min(valid_cavities, key=lambda x: x['min_rmsd'])\n",
    "            elif criterion_key == 'best_gold':\n",
    "                valid_cavities = [c for c in cavities if 'GOLD' in c['tool_scores']]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with best (highest) GOLD score\n",
    "                preferred_cavity = max(valid_cavities, key=lambda x: x['tool_scores']['GOLD'])\n",
    "            elif criterion_key == 'best_ledock':\n",
    "                valid_cavities = [c for c in cavities if 'LeDock' in c['tool_scores']]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with best (most negative) LeDock score\n",
    "                preferred_cavity = min(valid_cavities, key=lambda x: x['tool_scores']['LeDock'])\n",
    "            elif criterion_key == 'best_smina':\n",
    "                valid_cavities = [c for c in cavities if 'Smina' in c['tool_scores']]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with best (most negative) Smina score\n",
    "                preferred_cavity = min(valid_cavities, key=lambda x: x['tool_scores']['Smina'])\n",
    "            \n",
    "            # Store preferred cavity info\n",
    "            preferred_cavities.append(preferred_cavity)\n",
    "            preferred_clusters.append(preferred_cavity['cluster_id'])\n",
    "            \n",
    "            # Store non-preferred cavity info\n",
    "            non_preferred = [c for c in valid_cavities if c != preferred_cavity]\n",
    "            non_preferred_cavities.extend(non_preferred)\n",
    "            non_preferred_clusters.extend([c['cluster_id'] for c in non_preferred])\n",
    "            \n",
    "            valid_pairs += 1\n",
    "        \n",
    "        # Store results for this criterion\n",
    "        rq2_analysis['preferred_cavities'][criterion_key] = {\n",
    "            'preferred': preferred_cavities,\n",
    "            'non_preferred': non_preferred_cavities,\n",
    "            'preferred_clusters': preferred_clusters,\n",
    "            'non_preferred_clusters': non_preferred_clusters,\n",
    "            'n_drug_target_pairs': valid_pairs\n",
    "        }\n",
    "        \n",
    "        print(f\"   Analyzed {valid_pairs:,} drug-target pairs\")\n",
    "        print(f\"   Preferred cavities: {len(preferred_cavities):,}\")\n",
    "        print(f\"   Non-preferred cavities: {len(non_preferred_cavities):,}\")\n",
    "        print(f\"   Unique preferred clusters: {len(set(preferred_clusters)):,}\")\n",
    "        print(f\"   Unique non-preferred clusters: {len(set(non_preferred_clusters)):,}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed preferred cavity identification for all criteria\")\n",
    "    \n",
    "    # Calculate cluster statistics for comparison\n",
    "    print(f\"\\n=== CLUSTER COMPARISON STATISTICS ===\")\n",
    "    \n",
    "    for criterion_key, criterion_name in preference_criteria.items():\n",
    "        data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "        \n",
    "        if len(data['preferred_clusters']) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{criterion_name}:\")\n",
    "        \n",
    "        # Convert to pandas Series for analysis\n",
    "        preferred_clusters_series = pd.Series(data['preferred_clusters'])\n",
    "        non_preferred_clusters_series = pd.Series(data['non_preferred_clusters'])\n",
    "        \n",
    "        # Cluster frequency analysis\n",
    "        preferred_cluster_counts = preferred_clusters_series.value_counts()\n",
    "        non_preferred_cluster_counts = non_preferred_clusters_series.value_counts()\n",
    "        \n",
    "        # Top preferred clusters\n",
    "        top_preferred = preferred_cluster_counts.head(5)\n",
    "        print(f\"   Top 5 preferred clusters: {dict(top_preferred)}\")\n",
    "        \n",
    "        # Statistical comparison using Chi-square test\n",
    "        from scipy.stats import chi2_contingency\n",
    "        \n",
    "        # Create contingency table\n",
    "        all_clusters = set(data['preferred_clusters'] + data['non_preferred_clusters'])\n",
    "        \n",
    "        contingency_data = []\n",
    "        for cluster in all_clusters:\n",
    "            pref_count = preferred_cluster_counts.get(cluster, 0)\n",
    "            non_pref_count = non_preferred_cluster_counts.get(cluster, 0)\n",
    "            contingency_data.append([pref_count, non_pref_count])\n",
    "        \n",
    "        if len(contingency_data) > 1:\n",
    "            contingency_table = np.array(contingency_data)\n",
    "            \n",
    "            # Perform chi-square test\n",
    "            try:\n",
    "                chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "                \n",
    "                rq2_analysis['statistical_tests'][criterion_key] = {\n",
    "                    'chi2_statistic': chi2,\n",
    "                    'p_value': p_value,\n",
    "                    'degrees_of_freedom': dof,\n",
    "                    'significant': p_value < 0.05\n",
    "                }\n",
    "                \n",
    "                print(f\"   Chi-square test: œá¬≤ = {chi2:.3f}, p = {p_value:.6f}\")\n",
    "                print(f\"   {'‚úÖ Significant' if p_value < 0.05 else '‚ùå Not significant'} difference in cluster distributions\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Could not perform chi-square test: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Statistical analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523de80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# üìä RQ2: Violin Plots for Preferred vs Non-Preferred Cavities (Individual Tool Scores)\n",
    "# ===============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Check if RQ2 analysis exists\n",
    "if 'rq2_analysis' not in globals() or rq2_analysis is None:\n",
    "    print(\"‚ùå RQ2 analysis not found. Please run the previous RQ2 analysis cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ2 analysis found. Creating violin plots...\")\n",
    "    \n",
    "    # Set up poster-quality plotting parameters (matching RQ1)\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 24,                 # Larger base font size\n",
    "        'axes.titlesize': 28,            # Larger title font\n",
    "        'axes.labelsize': 22,            # Larger axis labels\n",
    "        'xtick.labelsize': 22,           # Larger tick labels\n",
    "        'ytick.labelsize': 20,           # Larger tick labels\n",
    "        'legend.fontsize': 22,           # Larger legend font\n",
    "        'figure.titlesize': 32           # Larger figure title\n",
    "    })\n",
    "    \n",
    "    # Define preference criteria with their corresponding metrics\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus\\n(Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD\\nScore', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock\\nScore', 'metric': 'LeDock Score (kcal/mol)'},\n",
    "        'best_smina': {'name': 'Best Smina\\nScore', 'metric': 'Smina Score (kcal/mol)'}\n",
    "    }\n",
    "    \n",
    "    # Count how many criteria have data\n",
    "    valid_criteria = []\n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        if criterion_key in rq2_analysis['preferred_cavities'] and len(rq2_analysis['preferred_cavities'][criterion_key]['preferred']) > 0:\n",
    "            valid_criteria.append((criterion_key, criterion_info))\n",
    "    \n",
    "    if len(valid_criteria) == 0:\n",
    "        print(\"‚ùå No valid criteria with data found\")\n",
    "    else:\n",
    "        # Create 2x2 subplot grid\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()  # Flatten for easy indexing\n",
    "        \n",
    "        # Loop over each valid preference criterion\n",
    "        for idx, (criterion_key, criterion_info) in enumerate(valid_criteria):\n",
    "            if idx >= 4:  # Only show first 4 criteria\n",
    "                break\n",
    "                \n",
    "            ax = axes[idx]\n",
    "            data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "            criterion_name = criterion_info['name']\n",
    "            metric = criterion_info['metric']\n",
    "            \n",
    "            # Prepare data for this specific metric\n",
    "            metric_data = []\n",
    "            \n",
    "            if metric == 'RMSD (√Ö)':\n",
    "                # RMSD data - use minimum RMSD for each cavity\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'min_rmsd' in cavity and pd.notna(cavity['min_rmsd']):\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['min_rmsd']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'min_rmsd' in cavity and pd.notna(cavity['min_rmsd']):\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['min_rmsd']})\n",
    "            \n",
    "            elif metric == 'GOLD Score':\n",
    "                # GOLD scores\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'tool_scores' in cavity and 'GOLD' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['tool_scores']['GOLD']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'tool_scores' in cavity and 'GOLD' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['tool_scores']['GOLD']})\n",
    "            \n",
    "            elif metric == 'LeDock Score (kcal/mol)':\n",
    "                # LeDock scores\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'tool_scores' in cavity and 'LeDock' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['tool_scores']['LeDock']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'tool_scores' in cavity and 'LeDock' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['tool_scores']['LeDock']})\n",
    "            \n",
    "            elif metric == 'Smina Score (kcal/mol)':\n",
    "                # Smina scores\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'tool_scores' in cavity and 'Smina' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['tool_scores']['Smina']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'tool_scores' in cavity and 'Smina' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['tool_scores']['Smina']})\n",
    "            \n",
    "            # Plot if we have data\n",
    "            if len(metric_data) > 0:\n",
    "                df_metric = pd.DataFrame(metric_data)\n",
    "                \n",
    "                # Check if we have both preferred and non-preferred data\n",
    "                prefs = df_metric['Preference'].unique()\n",
    "                if len(prefs) > 1:\n",
    "                    try:\n",
    "                        # Create split violin plot using seaborn\n",
    "                        df_metric['dummy'] = 'All'\n",
    "                        sns.violinplot(\n",
    "                            data=df_metric,\n",
    "                            y='Value',\n",
    "                            x='dummy',\n",
    "                            hue='Preference',\n",
    "                            ax=ax,\n",
    "                            palette=['lightcoral', 'lightblue'],\n",
    "                            inner='quartile',\n",
    "                            split=True\n",
    "                        )\n",
    "                        \n",
    "                        # Customize subplot\n",
    "                        ax.set_title(f'{criterion_name}', fontsize=24, fontweight='bold', pad=20)\n",
    "                        ax.set_ylabel(metric, fontsize=22, fontweight='bold')\n",
    "                        ax.set_xlabel(\"\")  # Remove x-axis label\n",
    "                        ax.set_xticklabels([])  # Remove x-axis tick labels\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        \n",
    "                        # Remove legend (it's clear from context)\n",
    "                        if ax.legend_:\n",
    "                            ax.legend_.remove()\n",
    "                        \n",
    "                        # Set appropriate y-axis limits\n",
    "                        if metric == 'RMSD (√Ö)':\n",
    "                            ax.set_ylim(0, max(15, df_metric['Value'].max() * 1.1))\n",
    "                        elif metric == 'GOLD Score':\n",
    "                            ax.set_ylim(0, max(100, df_metric['Value'].max() * 1.1))\n",
    "                        elif metric in ['LeDock Score (kcal/mol)', 'Smina Score (kcal/mol)']:\n",
    "                            ax.set_ylim(min(-15, df_metric['Value'].min() * 1.1), \n",
    "                                       max(5, df_metric['Value'].max() * 1.1))\n",
    "                        \n",
    "                        # Enhanced statistical testing for non-normal data\n",
    "                        preferred_vals = df_metric[df_metric['Preference'] == 'Preferred']['Value'].values\n",
    "                        non_preferred_vals = df_metric[df_metric['Preference'] == 'Non-Preferred']['Value'].values\n",
    "                        \n",
    "                        if len(preferred_vals) > 0 and len(non_preferred_vals) > 0:\n",
    "                            try:\n",
    "                                # Mann-Whitney U test (appropriate for non-normal data)\n",
    "                                stat, p_val = stats.mannwhitneyu(preferred_vals, non_preferred_vals, alternative='two-sided')\n",
    "                                \n",
    "                                # Calculate effect size (rank biserial correlation)\n",
    "                                n1, n2 = len(preferred_vals), len(non_preferred_vals)\n",
    "                                effect_size = 1 - (2 * stat) / (n1 * n2)\n",
    "                                \n",
    "                                # Interpret effect size\n",
    "                                if abs(effect_size) < 0.1:\n",
    "                                    effect_interpretation = \"negligible\"\n",
    "                                elif abs(effect_size) < 0.3:\n",
    "                                    effect_interpretation = \"small\"\n",
    "                                elif abs(effect_size) < 0.5:\n",
    "                                    effect_interpretation = \"medium\"\n",
    "                                else:\n",
    "                                    effect_interpretation = \"large\"\n",
    "                                \n",
    "                                # Determine significance\n",
    "                                if p_val < 0.001:\n",
    "                                    significance = \"***\"\n",
    "                                elif p_val < 0.01:\n",
    "                                    significance = \"**\"\n",
    "                                elif p_val < 0.05:\n",
    "                                    significance = \"*\"\n",
    "                                else:\n",
    "                                    significance = \"ns\"\n",
    "                                \n",
    "                                # Add statistical annotation with effect size\n",
    "                                ax.text(0.5, 0.95, f'p = {p_val:.3f} {significance}', \n",
    "                                       transform=ax.transAxes, ha='center', va='top', \n",
    "                                       fontsize=18, fontweight='bold')\n",
    "                                ax.text(0.5, 0.88, f'effect size = {effect_size:.3f} ({effect_interpretation})', \n",
    "                                       transform=ax.transAxes, ha='center', va='top', \n",
    "                                       fontsize=16, style='italic')\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ö†Ô∏è Statistical test failed for {criterion_name} - {metric}: {e}\")\n",
    "                        \n",
    "                        print(f\"‚úÖ Created split violin plot for {criterion_name}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error creating violin plot for {criterion_name}: {e}\")\n",
    "                        ax.text(0.5, 0.5, f'Error creating plot', \n",
    "                               ha='center', va='center', transform=ax.transAxes, \n",
    "                               fontsize=20, fontweight='bold')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, f'Only {prefs[0]} data available', \n",
    "                           ha='center', va='center', transform=ax.transAxes, \n",
    "                           fontsize=20, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'No data available', \n",
    "                       ha='center', va='center', transform=ax.transAxes, \n",
    "                       fontsize=20, fontweight='bold')\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for idx in range(len(valid_criteria), 4):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary statistics with improved statistical analysis\n",
    "        print(f\"\\nüìä COMPREHENSIVE SUMMARY STATISTICS:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Statistical Analysis Notes:\")\n",
    "        print(\"‚Ä¢ Mann-Whitney U test used (appropriate for non-normal distributions)\")\n",
    "        print(\"‚Ä¢ Effect size calculated using rank biserial correlation\")\n",
    "        print(\"‚Ä¢ Effect size interpretation: negligible (<0.1), small (0.1-0.3), medium (0.3-0.5), large (>0.5)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for criterion_key, criterion_info in valid_criteria:\n",
    "            data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "            criterion_name = criterion_info['name'].replace('\\n', ' ')  # Remove newlines for clean printing\n",
    "            metric = criterion_info['metric']\n",
    "            \n",
    "            print(f\"\\n{criterion_name} - {metric}:\")\n",
    "            print(f\"  Drug-target pairs analyzed: {data['n_drug_target_pairs']:,}\")\n",
    "            print(f\"  Preferred cavities: {len(data['preferred']):,}\")\n",
    "            print(f\"  Non-preferred cavities: {len(data['non_preferred']):,}\")\n",
    "            \n",
    "            # Get the appropriate statistics based on the metric\n",
    "            if metric == 'RMSD (√Ö)':\n",
    "                # Min RMSD statistics\n",
    "                pref_vals = [c['min_rmsd'] for c in data['preferred'] if 'min_rmsd' in c and pd.notna(c['min_rmsd'])]\n",
    "                non_pref_vals = [c['min_rmsd'] for c in data['non_preferred'] if 'min_rmsd' in c and pd.notna(c['min_rmsd'])]\n",
    "                unit = \"√Ö\"\n",
    "            elif metric == 'GOLD Score':\n",
    "                # GOLD Score statistics\n",
    "                pref_vals = [c['tool_scores']['GOLD'] for c in data['preferred'] if 'tool_scores' in c and 'GOLD' in c['tool_scores']]\n",
    "                non_pref_vals = [c['tool_scores']['GOLD'] for c in data['non_preferred'] if 'tool_scores' in c and 'GOLD' in c['tool_scores']]\n",
    "                unit = \"\"\n",
    "            elif metric == 'LeDock Score (kcal/mol)':\n",
    "                # LeDock Score statistics\n",
    "                pref_vals = [c['tool_scores']['LeDock'] for c in data['preferred'] if 'tool_scores' in c and 'LeDock' in c['tool_scores']]\n",
    "                non_pref_vals = [c['tool_scores']['LeDock'] for c in data['non_preferred'] if 'tool_scores' in c and 'LeDock' in c['tool_scores']]\n",
    "                unit = \"kcal/mol\"\n",
    "            elif metric == 'Smina Score (kcal/mol)':\n",
    "                # Smina Score statistics\n",
    "                pref_vals = [c['tool_scores']['Smina'] for c in data['preferred'] if 'tool_scores' in c and 'Smina' in c['tool_scores']]\n",
    "                non_pref_vals = [c['tool_scores']['Smina'] for c in data['non_preferred'] if 'tool_scores' in c and 'Smina' in c['tool_scores']]\n",
    "                unit = \"kcal/mol\"\n",
    "            \n",
    "            if len(pref_vals) > 0 and len(non_pref_vals) > 0:\n",
    "                print(f\"\\n  {metric} Statistics:\")\n",
    "                print(f\"    Preferred: n={len(pref_vals)}, median={np.median(pref_vals):.3f} {unit} (IQR: {np.percentile(pref_vals, 25):.3f}-{np.percentile(pref_vals, 75):.3f})\")\n",
    "                print(f\"    Non-Preferred: n={len(non_pref_vals)}, median={np.median(non_pref_vals):.3f} {unit} (IQR: {np.percentile(non_pref_vals, 25):.3f}-{np.percentile(non_pref_vals, 75):.3f})\")\n",
    "        \n",
    "        # Reset matplotlib parameters\n",
    "        plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42844a",
   "metadata": {},
   "source": [
    "## üî¨ Research Question 3: Do Cavity Clusters Separate Preferred from Non-Preferred Cavities?\n",
    "\n",
    "**Research Question 3:** *Within each drug-target interaction, do preferred and non-preferred cavities belong to the same or different cavity clusters?*\n",
    "\n",
    "### üéØ Specific Analysis Goal:\n",
    "For each **drugbank_id - uniprot_id** combination (drug-target pair) with multiple cavities:\n",
    "1. **Identify the preferred cavity** based on each scoring criterion (lowest RMSD, best GOLD/LeDock/Smina scores)\n",
    "2. **Compare cluster membership** between the preferred cavity and all non-preferred cavities\n",
    "3. **Determine cluster separation**: Does the preferred cavity belong to a different cluster than the non-preferred cavities?\n",
    "4. **Calculate separation rates** and test for statistical significance\n",
    "\n",
    "### üìä Key Metrics:\n",
    "- **Cluster separation rate**: Percentage of drug-target pairs where preferred cavity is in a different cluster\n",
    "- **Statistical significance**: Binomial test against 50% random separation expectation\n",
    "- **Separation patterns**: Analysis across different preference criteria\n",
    "\n",
    "### üß© Analysis Logic:\n",
    "- **Same cluster**: Preferred cavity shares its cluster ID with at least one non-preferred cavity\n",
    "- **Different clusters**: Preferred cavity has a unique cluster ID from all non-preferred cavities\n",
    "- **Separation rate**: Proportion of drug-target pairs showing different clusters\n",
    "\n",
    "### üìà Main Visualizations:\n",
    "- **Separation rate bar plots** showing cluster separation effectiveness for each criterion\n",
    "- **Count plots** showing distribution of same vs different cluster cases\n",
    "- **Statistical significance indicators** for each preference criterion\n",
    "\n",
    "This analysis directly addresses whether cavity clustering is useful for distinguishing high-quality binding sites from lower-quality alternatives in drug discovery.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üî¨ RQ3: Cluster Separation Analysis - Do Preferred and Non-Preferred Cavities \n",
    "#      Belong to Same or Different Clusters?\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç RESEARCH QUESTION 3: Do cavity clusters separate preferred from non-preferred cavities?\")\n",
    "print(\"=\" * 85)\n",
    "print(\"üéØ SPECIFIC QUESTION: Within each drug-target pair, do preferred and non-preferred\")\n",
    "print(\"   cavities belong to the same cluster or different clusters?\")\n",
    "\n",
    "# Check if RQ2 analysis exists and cavity cluster data is available\n",
    "if 'rq2_analysis' not in globals() or rq2_analysis is None:\n",
    "    print(\"‚ùå RQ2 analysis not found. Please run the RQ2 analysis first.\")\n",
    "elif 'cavity_cluster_id' not in combined_results.columns:\n",
    "    print(\"‚ùå Cavity cluster data not found. Please ensure cavity cluster information is loaded.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ2 analysis and cavity cluster data found. Proceeding with RQ3 cluster separation analysis...\")\n",
    "    \n",
    "    # Initialize RQ3 analysis storage\n",
    "    rq3_analysis = {\n",
    "        'drug_target_analysis': {},\n",
    "        'cluster_separation_stats': {},\n",
    "        'detailed_results': {}\n",
    "    }\n",
    "    \n",
    "    # Define the same preference criteria as RQ2\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus (Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD Score', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock Score', 'metric': 'LeDock Score (kcal/mol)'},\n",
    "        'best_smina': {'name': 'Best Smina Score', 'metric': 'Smina Score (kcal/mol)'}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß© Analyzing cluster separation for {len(preference_criteria)} preference criteria...\")\n",
    "    print(\"üìã Analysis approach:\")\n",
    "    print(\"   1. For each drug-target pair with multiple cavities\")\n",
    "    print(\"   2. Identify the preferred cavity based on each criterion\")\n",
    "    print(\"   3. Check if preferred cavity's cluster differs from non-preferred cavities' clusters\")\n",
    "    print(\"   4. Calculate separation rates and statistical significance\")\n",
    "    \n",
    "    # For each preference criterion, analyze cluster separation within drug-target pairs\n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        print(f\"\\n--- Analyzing {criterion_info['name']} ---\")\n",
    "        \n",
    "        # Get the preferred and non-preferred cavity data from RQ2\n",
    "        if criterion_key not in rq2_analysis['preferred_cavities']:\n",
    "            print(f\"   ‚ö†Ô∏è No RQ2 data available for {criterion_key}\")\n",
    "            continue\n",
    "            \n",
    "        rq2_data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "        \n",
    "        # Prepare data structure for this criterion\n",
    "        drug_target_cluster_analysis = {}\n",
    "        \n",
    "        # Group cavities by drug-target pairs\n",
    "        dt_pairs = {}\n",
    "        \n",
    "        # Process preferred cavities\n",
    "        for cavity in rq2_data['preferred']:\n",
    "            dt_key = (cavity['drug'], cavity['target'])\n",
    "            if dt_key not in dt_pairs:\n",
    "                dt_pairs[dt_key] = {'preferred': None, 'non_preferred': []}\n",
    "            dt_pairs[dt_key]['preferred'] = cavity\n",
    "        \n",
    "        # Process non-preferred cavities\n",
    "        for cavity in rq2_data['non_preferred']:\n",
    "            dt_key = (cavity['drug'], cavity['target'])\n",
    "            if dt_key not in dt_pairs:\n",
    "                dt_pairs[dt_key] = {'preferred': None, 'non_preferred': []}\n",
    "            dt_pairs[dt_key]['non_preferred'].append(cavity)\n",
    "        \n",
    "        # Analyze cluster separation for each drug-target pair\n",
    "        separation_results = []\n",
    "        valid_pairs = 0\n",
    "        \n",
    "        for dt_key, pair_data in dt_pairs.items():\n",
    "            drug, target = dt_key\n",
    "            \n",
    "            # Skip if no preferred cavity or no non-preferred cavities\n",
    "            if pair_data['preferred'] is None or len(pair_data['non_preferred']) == 0:\n",
    "                continue\n",
    "            \n",
    "            preferred_cavity = pair_data['preferred']\n",
    "            non_preferred_cavities = pair_data['non_preferred']\n",
    "            \n",
    "            # Check if cluster data is available\n",
    "            if 'cluster_id' not in preferred_cavity or pd.isna(preferred_cavity['cluster_id']):\n",
    "                continue\n",
    "            \n",
    "            preferred_cluster = preferred_cavity['cluster_id']\n",
    "            non_preferred_clusters = [c['cluster_id'] for c in non_preferred_cavities \n",
    "                                    if 'cluster_id' in c and pd.notna(c['cluster_id'])]\n",
    "            \n",
    "            if len(non_preferred_clusters) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Determine separation status\n",
    "            if preferred_cluster in non_preferred_clusters:\n",
    "                separation_status = 'same_cluster'\n",
    "            else:\n",
    "                separation_status = 'different_cluster'\n",
    "            \n",
    "            separation_results.append({\n",
    "                'drug': drug,\n",
    "                'target': target,\n",
    "                'preferred_cluster': preferred_cluster,\n",
    "                'non_preferred_clusters': non_preferred_clusters,\n",
    "                'separation_status': separation_status,\n",
    "                'n_non_preferred': len(non_preferred_clusters),\n",
    "                'unique_non_preferred_clusters': len(set(non_preferred_clusters))\n",
    "            })\n",
    "            \n",
    "            valid_pairs += 1\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if len(separation_results) > 0:\n",
    "            same_cluster_count = sum(1 for r in separation_results if r['separation_status'] == 'same_cluster')\n",
    "            different_cluster_count = sum(1 for r in separation_results if r['separation_status'] == 'different_cluster')\n",
    "            total_pairs = same_cluster_count + different_cluster_count\n",
    "            separation_rate = different_cluster_count / total_pairs if total_pairs > 0 else 0\n",
    "            \n",
    "            # Store results\n",
    "            rq3_analysis['drug_target_analysis'][criterion_key] = separation_results\n",
    "            rq3_analysis['cluster_separation_stats'][criterion_key] = {\n",
    "                'total_pairs': total_pairs,\n",
    "                'same_cluster': same_cluster_count,\n",
    "                'different_cluster': different_cluster_count,\n",
    "                'separation_rate': separation_rate\n",
    "            }\n",
    "            \n",
    "            print(f\"   Valid drug-target pairs analyzed: {total_pairs}\")\n",
    "            print(f\"   Same cluster (preferred shares cluster with non-preferred): {same_cluster_count} ({same_cluster_count/total_pairs*100:.1f}%)\")\n",
    "            print(f\"   Different clusters (preferred in unique cluster): {different_cluster_count} ({different_cluster_count/total_pairs*100:.1f}%)\")\n",
    "            print(f\"   Cluster separation rate: {separation_rate:.3f}\")\n",
    "            \n",
    "            # Statistical significance test\n",
    "            # Binomial test: null hypothesis is 50% separation rate (random)\n",
    "            from scipy.stats import binomtest\n",
    "            p_value = binomtest(different_cluster_count, total_pairs, 0.5, alternative='two-sided').pvalue\n",
    "            \n",
    "            rq3_analysis['cluster_separation_stats'][criterion_key]['binomial_test_p'] = p_value\n",
    "            rq3_analysis['cluster_separation_stats'][criterion_key]['significant'] = p_value < 0.05\n",
    "            \n",
    "            print(f\"   üìä Binomial test (H0: separation rate = 50%): p = {p_value:.6f}\")\n",
    "            print(f\"   üìä Result: {'‚úÖ Significant separation' if p_value < 0.05 else '‚ùå No significant separation'}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No valid drug-target pairs found for analysis\")\n",
    "    \n",
    "    # Summary of findings\n",
    "    print(f\"\\nüìä RQ3 CLUSTER SEPARATION ANALYSIS SUMMARY:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    valid_criteria = [k for k in rq3_analysis['cluster_separation_stats'] \n",
    "                     if rq3_analysis['cluster_separation_stats'][k]['total_pairs'] > 0]\n",
    "    \n",
    "    if len(valid_criteria) > 0:\n",
    "        print(f\"‚úÖ Successfully analyzed {len(valid_criteria)} preference criteria\")\n",
    "        \n",
    "        # Show summary statistics\n",
    "        for criterion_key in valid_criteria:\n",
    "            stats = rq3_analysis['cluster_separation_stats'][criterion_key]\n",
    "            criterion_name = preference_criteria[criterion_key]['name']\n",
    "            \n",
    "            print(f\"\\n{criterion_name}:\")\n",
    "            print(f\"  Separation rate: {stats['separation_rate']:.3f}\")\n",
    "            print(f\"  Statistical significance: {'‚úÖ Yes' if stats['significant'] else '‚ùå No'} (p = {stats['binomial_test_p']:.6f})\")\n",
    "        \n",
    "        # Overall conclusion\n",
    "        significant_criteria = [k for k in valid_criteria \n",
    "                              if rq3_analysis['cluster_separation_stats'][k]['significant']]\n",
    "        \n",
    "        print(f\"\\nüéØ OVERALL CONCLUSION:\")\n",
    "        if len(significant_criteria) > 0:\n",
    "            print(f\"   ‚úÖ {len(significant_criteria)} out of {len(valid_criteria)} criteria show significant cluster separation\")\n",
    "            print(f\"   üî• Cavity clusters CAN effectively separate preferred from non-preferred cavities\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå No criteria show significant cluster separation\")\n",
    "            print(f\"   üî• Cavity clusters do NOT effectively separate preferred from non-preferred cavities\")\n",
    "        \n",
    "        print(f\"\\nüéØ Ready for RQ3 visualization...\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No valid criteria found for cluster separation analysis\")\n",
    "        print(\"   Please check data availability and RQ2 analysis results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä RQ3: Cluster Separation Analysis for Preferred vs Non-Preferred Cavities\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Analyzing cluster separation for preferred vs non-preferred cavities...\")\n",
    "print(\"üéØ Research Question: Do preferred and non-preferred cavities belong to same or different clusters?\")\n",
    "\n",
    "# Check if RQ3 analysis exists\n",
    "if 'rq3_analysis' not in globals() or rq3_analysis is None:\n",
    "    print(\"‚ùå RQ3 analysis not found. Please run the previous RQ3 analysis cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ3 analysis found. Analyzing cluster separation within drug-target pairs...\")\n",
    "    \n",
    "    # Initialize cluster separation analysis\n",
    "    cluster_separation_analysis = {\n",
    "        'same_cluster': {},\n",
    "        'different_cluster': {},\n",
    "        'separation_rates': {},\n",
    "        'statistical_tests': {}\n",
    "    }\n",
    "    \n",
    "    # Define the same preference criteria as before\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus (Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD Score', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock Score', 'metric': 'LeDock Score (kcal/mol)'},\n",
    "        'best_smina': {'name': 'Best Smina Score', 'metric': 'Smina Score (kcal/mol)'}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüîç Analyzing cluster separation for {len(preference_criteria)} preference criteria...\")\n",
    "    \n",
    "    # For each preference criterion, analyze cluster separation\n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        print(f\"\\n--- Analyzing {criterion_info['name']} ---\")\n",
    "        \n",
    "        # Get the RQ2 data for this criterion\n",
    "        if criterion_key not in rq2_analysis['preferred_cavities']:\n",
    "            print(f\"   ‚ö†Ô∏è No RQ2 data available for {criterion_key}\")\n",
    "            continue\n",
    "            \n",
    "        rq2_data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "        \n",
    "        # Group cavities by drug-target pairs to analyze cluster separation\n",
    "        drug_target_cavities = {}\n",
    "        for key, cavity_data in cavity_scores.items():\n",
    "            dt_key = (cavity_data['drug'], cavity_data['target'])\n",
    "            if dt_key not in drug_target_cavities:\n",
    "                drug_target_cavities[dt_key] = []\n",
    "            drug_target_cavities[dt_key].append(cavity_data)\n",
    "        \n",
    "        # Analyze cluster separation for each drug-target pair\n",
    "        same_cluster_count = 0\n",
    "        different_cluster_count = 0\n",
    "        separation_details = []\n",
    "        \n",
    "        for (drug, target), cavities in drug_target_cavities.items():\n",
    "            if len(cavities) < 2:  # Need at least 2 cavities to analyze separation\n",
    "                continue\n",
    "            \n",
    "            # Find the preferred cavity for this drug-target pair and criterion\n",
    "            preferred_cavity = None\n",
    "            for cavity in rq2_data['preferred']:\n",
    "                if cavity['drug'] == drug and cavity['target'] == target:\n",
    "                    preferred_cavity = cavity\n",
    "                    break\n",
    "            \n",
    "            if preferred_cavity is None:\n",
    "                continue\n",
    "            \n",
    "            # Get non-preferred cavities for this drug-target pair\n",
    "            non_preferred_cavities = []\n",
    "            for cavity in rq2_data['non_preferred']:\n",
    "                if cavity['drug'] == drug and cavity['target'] == target:\n",
    "                    non_preferred_cavities.append(cavity)\n",
    "            \n",
    "            if len(non_preferred_cavities) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check cluster separation\n",
    "            preferred_cluster = preferred_cavity['cluster_id']\n",
    "            non_preferred_clusters = [c['cluster_id'] for c in non_preferred_cavities if pd.notna(c['cluster_id'])]\n",
    "            \n",
    "            if pd.notna(preferred_cluster) and len(non_preferred_clusters) > 0:\n",
    "                # Check if preferred cavity is in a different cluster from ALL non-preferred cavities\n",
    "                if preferred_cluster in non_preferred_clusters:\n",
    "                    same_cluster_count += 1\n",
    "                    separation_status = 'same_cluster'\n",
    "                else:\n",
    "                    different_cluster_count += 1\n",
    "                    separation_status = 'different_cluster'\n",
    "                \n",
    "                separation_details.append({\n",
    "                    'drug': drug,\n",
    "                    'target': target,\n",
    "                    'preferred_cluster': preferred_cluster,\n",
    "                    'non_preferred_clusters': non_preferred_clusters,\n",
    "                    'separation_status': separation_status,\n",
    "                    'n_cavities': len(cavities),\n",
    "                    'n_non_preferred': len(non_preferred_cavities)\n",
    "                })\n",
    "        \n",
    "        # Store results for this criterion\n",
    "        cluster_separation_analysis['same_cluster'][criterion_key] = same_cluster_count\n",
    "        cluster_separation_analysis['different_cluster'][criterion_key] = different_cluster_count\n",
    "        \n",
    "        total_analyzed = same_cluster_count + different_cluster_count\n",
    "        if total_analyzed > 0:\n",
    "            separation_rate = different_cluster_count / total_analyzed\n",
    "            cluster_separation_analysis['separation_rates'][criterion_key] = separation_rate\n",
    "            \n",
    "            print(f\"   Drug-target pairs analyzed: {total_analyzed}\")\n",
    "            print(f\"   Same cluster (preferred = non-preferred): {same_cluster_count} ({same_cluster_count/total_analyzed*100:.1f}%)\")\n",
    "            print(f\"   Different clusters (preferred ‚â† non-preferred): {different_cluster_count} ({different_cluster_count/total_analyzed*100:.1f}%)\")\n",
    "            print(f\"   Cluster separation rate: {separation_rate:.3f}\")\n",
    "            \n",
    "            # Statistical test: binomial test against null hypothesis of 50% separation\n",
    "            from scipy.stats import binomtest\n",
    "            \n",
    "            # Test if separation rate is significantly different from 50% (random)\n",
    "            p_value = binomtest(different_cluster_count, total_analyzed, 0.5, alternative='two-sided').pvalue\n",
    "            \n",
    "            cluster_separation_analysis['statistical_tests'][criterion_key] = {\n",
    "                'binomial_test_p': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'separation_rate': separation_rate,\n",
    "                'total_pairs': total_analyzed\n",
    "            }\n",
    "            \n",
    "            print(f\"   üìä Binomial test (H0: separation rate = 50%): p = {p_value:.6f}\")\n",
    "            print(f\"   üìä {'‚úÖ Significant' if p_value < 0.05 else '‚ùå Not significant'} deviation from random\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No valid drug-target pairs found for analysis\")\n",
    "    \n",
    "    # Create visualization: Bar plot showing separation rates\n",
    "    print(f\"\\nüìä Creating cluster separation visualization...\")\n",
    "    \n",
    "    # Set up plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 16,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 16,\n",
    "        'xtick.labelsize': 14,\n",
    "        'ytick.labelsize': 14,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    criteria_names = []\n",
    "    separation_rates = []\n",
    "    same_cluster_counts = []\n",
    "    different_cluster_counts = []\n",
    "    significance_markers = []\n",
    "    \n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        if criterion_key in cluster_separation_analysis['separation_rates']:\n",
    "            criteria_names.append(criterion_info['name'])\n",
    "            separation_rates.append(cluster_separation_analysis['separation_rates'][criterion_key])\n",
    "            same_cluster_counts.append(cluster_separation_analysis['same_cluster'][criterion_key])\n",
    "            different_cluster_counts.append(cluster_separation_analysis['different_cluster'][criterion_key])\n",
    "            \n",
    "            # Add significance marker\n",
    "            if criterion_key in cluster_separation_analysis['statistical_tests']:\n",
    "                p_val = cluster_separation_analysis['statistical_tests'][criterion_key]['binomial_test_p']\n",
    "                if p_val < 0.001:\n",
    "                    significance_markers.append('***')\n",
    "                elif p_val < 0.01:\n",
    "                    significance_markers.append('**')\n",
    "                elif p_val < 0.05:\n",
    "                    significance_markers.append('*')\n",
    "                else:\n",
    "                    significance_markers.append('ns')\n",
    "            else:\n",
    "                significance_markers.append('ns')\n",
    "    \n",
    "    if len(criteria_names) > 0:\n",
    "        # Create stacked bar plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: Separation rates\n",
    "        bars1 = ax1.bar(criteria_names, separation_rates, color='lightblue', edgecolor='black', linewidth=1)\n",
    "        ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random expectation (50%)')\n",
    "        ax1.set_ylabel('Cluster Separation Rate', fontweight='bold')\n",
    "        ax1.set_title('RQ3: Cluster Separation Rates\\n(Preferred vs Non-Preferred Cavities)', fontweight='bold')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add significance markers\n",
    "        for i, (rate, sig) in enumerate(zip(separation_rates, significance_markers)):\n",
    "            ax1.text(i, rate + 0.05, sig, ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        ax1.set_xticklabels(criteria_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 2: Counts (stacked bar)\n",
    "        width = 0.6\n",
    "        bars2 = ax2.bar(criteria_names, same_cluster_counts, width, label='Same Cluster', color='lightcoral', edgecolor='black')\n",
    "        bars3 = ax2.bar(criteria_names, different_cluster_counts, width, bottom=same_cluster_counts, \n",
    "                       label='Different Clusters', color='lightgreen', edgecolor='black')\n",
    "        \n",
    "        ax2.set_ylabel('Number of Drug-Target Pairs', fontweight='bold')\n",
    "        ax2.set_title('RQ3: Cluster Separation Counts\\n(Preferred vs Non-Preferred Cavities)', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for i, (same, diff) in enumerate(zip(same_cluster_counts, different_cluster_counts)):\n",
    "            total = same + diff\n",
    "            if same > 0:\n",
    "                ax2.text(i, same/2, str(same), ha='center', va='center', fontweight='bold')\n",
    "            if diff > 0:\n",
    "                ax2.text(i, same + diff/2, str(diff), ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        ax2.set_xticklabels(criteria_names, rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        print(f\"\\nüìä RQ3 CLUSTER SEPARATION SUMMARY:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"üìã Analysis Summary:\")\n",
    "        print(\"‚Ä¢ For each drug-target pair, compared cluster IDs of preferred vs non-preferred cavities\")\n",
    "        print(\"‚Ä¢ 'Same cluster': Preferred cavity shares cluster with at least one non-preferred cavity\")\n",
    "        print(\"‚Ä¢ 'Different clusters': Preferred cavity is in a unique cluster from all non-preferred cavities\")\n",
    "        print(\"‚Ä¢ Binomial test: Tests if separation rate differs significantly from 50% (random)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for criterion_key, criterion_info in preference_criteria.items():\n",
    "            if criterion_key in cluster_separation_analysis['separation_rates']:\n",
    "                criterion_name = criterion_info['name']\n",
    "                same_count = cluster_separation_analysis['same_cluster'][criterion_key]\n",
    "                diff_count = cluster_separation_analysis['different_cluster'][criterion_key]\n",
    "                total = same_count + diff_count\n",
    "                sep_rate = cluster_separation_analysis['separation_rates'][criterion_key]\n",
    "                \n",
    "                print(f\"\\n{criterion_name}:\")\n",
    "                print(f\"  Total drug-target pairs: {total}\")\n",
    "                print(f\"  Same cluster: {same_count} ({same_count/total*100:.1f}%)\")\n",
    "                print(f\"  Different clusters: {diff_count} ({diff_count/total*100:.1f}%)\")\n",
    "                print(f\"  Separation rate: {sep_rate:.3f}\")\n",
    "                \n",
    "                if criterion_key in cluster_separation_analysis['statistical_tests']:\n",
    "                    test_data = cluster_separation_analysis['statistical_tests'][criterion_key]\n",
    "                    print(f\"  üìä Binomial test p-value: {test_data['binomial_test_p']:.6f}\")\n",
    "                    print(f\"  üìä Significantly different from random: {'‚úÖ Yes' if test_data['significant'] else '‚ùå No'}\")\n",
    "        \n",
    "        # Overall conclusion\n",
    "        significant_criteria = [k for k in cluster_separation_analysis['statistical_tests'] \n",
    "                              if cluster_separation_analysis['statistical_tests'][k]['significant']]\n",
    "        \n",
    "        print(f\"\\n‚úÖ RQ3 CONCLUSION:\")\n",
    "        if len(significant_criteria) > 0:\n",
    "            print(f\"üéØ Cavity clusters DO show significant separation for {len(significant_criteria)} out of {len(criteria_names)} criteria\")\n",
    "            print(f\"üî• Significant criteria: {[preference_criteria[k]['name'] for k in significant_criteria]}\")\n",
    "        else:\n",
    "            print(f\"üéØ Cavity clusters do NOT show significant separation for any criteria\")\n",
    "        \n",
    "        # Store results globally for potential further analysis\n",
    "        globals()['cluster_separation_analysis'] = cluster_separation_analysis\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No valid criteria found for cluster separation analysis\")\n",
    "    \n",
    "    # Reset matplotlib parameters\n",
    "    plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdef493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä RQ3: KDE Plots for Cluster Ratio Distributions\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Creating KDE plots for cluster ratio distributions...\")\n",
    "print(\"üéØ Analysis: For each drug-target pair, calculate ratios of non-preferred cavities\")\n",
    "print(\"   in same vs different clusters compared to the preferred cavity\")\n",
    "\n",
    "# Check if RQ3 analysis exists\n",
    "if 'rq3_analysis' not in globals() or rq3_analysis is None:\n",
    "    print(\"‚ùå RQ3 analysis not found. Please run the previous RQ3 analysis cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ3 analysis found. Calculating cluster ratios for each drug-target pair...\")\n",
    "    \n",
    "    # Set up poster-quality plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,                 # Larger base font size\n",
    "        'axes.titlesize': 22,            # Larger title font\n",
    "        'axes.labelsize': 20,            # Larger axis labels\n",
    "        'xtick.labelsize': 18,           # Larger tick labels\n",
    "        'ytick.labelsize': 18,           # Larger tick labels\n",
    "        'legend.fontsize': 18,           # Larger legend font\n",
    "        'figure.titlesize': 24           # Larger figure title\n",
    "    })\n",
    "    \n",
    "    # Define the preference criteria\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus\\n(Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD\\nScore', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock\\nScore', 'metric': 'LeDock Score'},\n",
    "        'best_smina': {'name': 'Best Smina\\nScore', 'metric': 'Smina Score'}\n",
    "    }\n",
    "    \n",
    "    # Calculate cluster ratios for each drug-target pair\n",
    "    ratio_data = {}\n",
    "    \n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        print(f\"\\n--- Calculating ratios for {criterion_info['name'].replace(chr(10), ' ')} ---\")\n",
    "        \n",
    "        # Get the drug-target analysis from RQ3\n",
    "        if criterion_key not in rq3_analysis['drug_target_analysis']:\n",
    "            print(f\"   ‚ö†Ô∏è No analysis data available for {criterion_key}\")\n",
    "            continue\n",
    "        \n",
    "        separation_results = rq3_analysis['drug_target_analysis'][criterion_key]\n",
    "        \n",
    "        # Calculate ratios for each drug-target pair\n",
    "        same_cluster_ratios = []\n",
    "        different_cluster_ratios = []\n",
    "        \n",
    "        for result in separation_results:\n",
    "            drug = result['drug']\n",
    "            target = result['target']\n",
    "            preferred_cluster = result['preferred_cluster']\n",
    "            non_preferred_clusters = result['non_preferred_clusters']\n",
    "            \n",
    "            # Count non-preferred cavities in same vs different clusters\n",
    "            same_cluster_count = non_preferred_clusters.count(preferred_cluster)\n",
    "            different_cluster_count = len(non_preferred_clusters) - same_cluster_count\n",
    "            total_non_preferred = len(non_preferred_clusters)\n",
    "            \n",
    "            # Calculate ratios\n",
    "            if total_non_preferred > 0:\n",
    "                same_cluster_ratio = same_cluster_count / total_non_preferred\n",
    "                different_cluster_ratio = different_cluster_count / total_non_preferred\n",
    "                \n",
    "                same_cluster_ratios.append(same_cluster_ratio)\n",
    "                different_cluster_ratios.append(different_cluster_ratio)\n",
    "        \n",
    "        # Store ratio data\n",
    "        ratio_data[criterion_key] = {\n",
    "            'same_cluster_ratios': same_cluster_ratios,\n",
    "            'different_cluster_ratios': different_cluster_ratios,\n",
    "            'n_drug_target_pairs': len(same_cluster_ratios)\n",
    "        }\n",
    "        \n",
    "        print(f\"   Drug-target pairs analyzed: {len(same_cluster_ratios)}\")\n",
    "        if len(same_cluster_ratios) > 0:\n",
    "            print(f\"   Same cluster ratios: mean={np.mean(same_cluster_ratios):.3f}, std={np.std(same_cluster_ratios):.3f}\")\n",
    "            print(f\"   Different cluster ratios: mean={np.mean(different_cluster_ratios):.3f}, std={np.std(different_cluster_ratios):.3f}\")\n",
    "    \n",
    "    # Create split violin plots\n",
    "    valid_criteria = [(k, v) for k, v in preference_criteria.items() if k in ratio_data and ratio_data[k]['n_drug_target_pairs'] > 0]\n",
    "    \n",
    "    if len(valid_criteria) == 0:\n",
    "        print(\"‚ùå No valid criteria with ratio data found\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Found {len(valid_criteria)} criteria with ratio data\")\n",
    "        \n",
    "        # Create 2x2 subplot grid for ratio distribution plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()  # Flatten for easy indexing\n",
    "        \n",
    "        # Loop over each valid preference criterion\n",
    "        for idx, (criterion_key, criterion_info) in enumerate(valid_criteria):\n",
    "            if idx >= 4:  # Only show first 4 criteria\n",
    "                break\n",
    "                \n",
    "            ax = axes[idx]\n",
    "            ratios = ratio_data[criterion_key]\n",
    "            criterion_name = criterion_info['name']\n",
    "            \n",
    "            # Prepare data for split violin plot\n",
    "            plot_data = []\n",
    "            \n",
    "            # Add same cluster ratios\n",
    "            for ratio in ratios['same_cluster_ratios']:\n",
    "                plot_data.append({'Cluster_Type': 'Same Cluster', 'Ratio': ratio})\n",
    "            \n",
    "            # Add different cluster ratios\n",
    "            for ratio in ratios['different_cluster_ratios']:\n",
    "                plot_data.append({'Cluster_Type': 'Different Cluster', 'Ratio': ratio})\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if len(plot_data) > 0:\n",
    "                df_ratios = pd.DataFrame(plot_data)\n",
    "                \n",
    "                # Check if we have both types of data\n",
    "                ratio_types = df_ratios['Cluster_Type'].unique()\n",
    "                if len(ratio_types) > 1:\n",
    "                    try:\n",
    "                        # Create KDE plot using seaborn with clear outlines\n",
    "                        # First create the filled areas\n",
    "                        sns.kdeplot(\n",
    "                            data=df_ratios,\n",
    "                            x='Ratio',\n",
    "                            hue='Cluster_Type',\n",
    "                            ax=ax,\n",
    "                            palette=['lightcoral', 'lightblue'],\n",
    "                            fill=True,\n",
    "                            alpha=0.6,\n",
    "                            common_norm=False,\n",
    "                            legend=False  # We'll add legend manually\n",
    "                        )\n",
    "                        \n",
    "                        # Then add the outlines with distinct colors\n",
    "                        for cluster_type, color in zip(['Same Cluster', 'Different Cluster'], ['darkred', 'darkblue']):\n",
    "                            subset = df_ratios[df_ratios['Cluster_Type'] == cluster_type]\n",
    "                            sns.kdeplot(\n",
    "                                data=subset,\n",
    "                                x='Ratio',\n",
    "                                ax=ax,\n",
    "                                color=color,\n",
    "                                fill=False,\n",
    "                                linewidth=3,\n",
    "                                label=cluster_type\n",
    "                            )\n",
    "                        \n",
    "                        # Customize subplot\n",
    "                        ax.set_title(f'{criterion_name}', fontsize=22, fontweight='bold', pad=20)\n",
    "                        ax.set_xlabel('Ratio of Non-Preferred Cavities', fontsize=20, fontweight='bold')\n",
    "                        ax.set_ylabel('Density', fontsize=20, fontweight='bold')\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        ax.set_xlim(0, 1.05)  # Ratios are between 0 and 1\n",
    "                        \n",
    "                        # Add vertical line at 0.5 for reference\n",
    "                        ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Equal Split (0.5)')\n",
    "                        \n",
    "                        # Customize legend\n",
    "                        handles, labels = ax.get_legend_handles_labels()\n",
    "                        ax.legend(handles, labels, loc='best', fontsize=12)\n",
    "                        \n",
    "                        # Statistical comparison\n",
    "                        same_ratios = np.array(ratios['same_cluster_ratios'])\n",
    "                        diff_ratios = np.array(ratios['different_cluster_ratios'])\n",
    "                        \n",
    "                        if len(same_ratios) > 0 and len(diff_ratios) > 0:\n",
    "                            # Mann-Whitney U test\n",
    "                            from scipy.stats import mannwhitneyu\n",
    "                            try:\n",
    "                                stat, p_val = mannwhitneyu(same_ratios, diff_ratios, alternative='two-sided')\n",
    "                                \n",
    "                                # Determine significance\n",
    "                                if p_val < 0.001:\n",
    "                                    significance = \"***\"\n",
    "                                elif p_val < 0.01:\n",
    "                                    significance = \"**\"\n",
    "                                elif p_val < 0.05:\n",
    "                                    significance = \"*\"\n",
    "                                else:\n",
    "                                    significance = \"ns\"\n",
    "                                \n",
    "                                # Add statistical annotation\n",
    "                                ax.text(0.5, 0.95, f'p = {p_val:.3f} {significance}', \n",
    "                                       transform=ax.transAxes, ha='center', va='top', \n",
    "                                       fontsize=18, fontweight='bold', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"   ‚ö†Ô∏è Statistical test failed for {criterion_name}: {e}\")\n",
    "                        \n",
    "                        print(f\"‚úÖ Created ratio KDE plot for {criterion_name.replace(chr(10), ' ')}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error creating ratio KDE plot for {criterion_name}: {e}\")\n",
    "                        ax.text(0.5, 0.5, f'Error creating plot\\n{str(e)[:50]}...', \n",
    "                               ha='center', va='center', transform=ax.transAxes, \n",
    "                               fontsize=16, fontweight='bold')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, f'Only {ratio_types[0]} data available', \n",
    "                           ha='center', va='center', transform=ax.transAxes, \n",
    "                           fontsize=18, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'No ratio data available', \n",
    "                       ha='center', va='center', transform=ax.transAxes, \n",
    "                       fontsize=18, fontweight='bold')\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for idx in range(len(valid_criteria), 4):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        # Add overall title\n",
    "        fig.suptitle('Same vs Different Cluster Ratios for Each Drug-Target Pair', \n",
    "                    fontsize=24, fontweight='bold', y=0.98)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.88)  # Make room for suptitle\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary statistics\n",
    "        print(f\"\\nüìä RQ3 CLUSTER RATIO ANALYSIS SUMMARY:\")\n",
    "        print(\"=\" * 90)\n",
    "        print(\"üìã Analysis Logic:\")\n",
    "        print(\"‚Ä¢ For each drug-target pair with multiple cavities:\")\n",
    "        print(\"  - Identify 1 preferred cavity (based on each criterion)\")\n",
    "        print(\"  - Calculate ratios of non-preferred cavities in same vs different clusters\")\n",
    "        print(\"  - Same cluster ratio: (# non-preferred in preferred's cluster) / (total # non-preferred)\")\n",
    "        print(\"  - Different cluster ratio: (# non-preferred in other clusters) / (total # non-preferred)\")\n",
    "        print(\"‚Ä¢ Statistical test: Mann-Whitney U test comparing same vs different cluster ratios\")\n",
    "        print(\"=\" * 90)\n",
    "        \n",
    "        for criterion_key, criterion_info in valid_criteria:\n",
    "            ratios = ratio_data[criterion_key]\n",
    "            criterion_name = criterion_info['name'].replace('\\n', ' ')\n",
    "            \n",
    "            same_ratios = np.array(ratios['same_cluster_ratios'])\n",
    "            diff_ratios = np.array(ratios['different_cluster_ratios'])\n",
    "            \n",
    "            print(f\"\\n{criterion_name}:\")\n",
    "            print(f\"  Drug-target pairs analyzed: {ratios['n_drug_target_pairs']}\")\n",
    "            print(f\"  Same cluster ratios: mean={np.mean(same_ratios):.3f}, median={np.median(same_ratios):.3f}\")\n",
    "            print(f\"  Different cluster ratios: mean={np.mean(diff_ratios):.3f}, median={np.median(diff_ratios):.3f}\")\n",
    "            \n",
    "            # Statistical comparison\n",
    "            if len(same_ratios) > 0 and len(diff_ratios) > 0:\n",
    "                from scipy.stats import mannwhitneyu\n",
    "                try:\n",
    "                    stat, p_val = mannwhitneyu(same_ratios, diff_ratios, alternative='two-sided')\n",
    "                    print(f\"  üìä Mann-Whitney U test p-value: {p_val:.6f}\")\n",
    "                    print(f\"  üìä Significant difference: {'‚úÖ Yes' if p_val < 0.05 else '‚ùå No'}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è Statistical test failed: {e}\")\n",
    "        \n",
    "        # Overall interpretation\n",
    "        print(f\"\\n‚úÖ RQ3 INTERPRETATION:\")\n",
    "        print(\"üéØ If cavities cluster meaningfully:\")\n",
    "        print(\"   - Same cluster ratios should be LOW (preferred and non-preferred in different clusters)\")\n",
    "        print(\"   - Different cluster ratios should be HIGH (good separation)\")\n",
    "        print(\"üéØ If clustering is random:\")\n",
    "        print(\"   - Both ratios should be around 0.5 (no meaningful separation)\")\n",
    "        \n",
    "        # Store results globally\n",
    "        globals()['cluster_ratio_analysis'] = ratio_data\n",
    "        \n",
    "    # Reset matplotlib parameters\n",
    "    plt.rcdefaults()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachopencadd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
