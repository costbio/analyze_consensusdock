#!/usr/bin/env python3
"""
CavitySpace AlphaFold Structure Downloader
Downloads AlphaFold models from CavitySpace database for UniProt IDs found in cavity mapping.

Features:
- Parallel downloading using multiple threads (with respectful delays)
- Resume capability: Skips already downloaded files
- Progress tracking and detailed logging
- Downloads AlphaFold PDB files from CavitySpace
- Integrates with existing cavity mapping workflow
- Rate limiting to be respectful to CavitySpace servers

Prerequisites:
- cavity_mapping.csv (generated by extract_cavities.py)
- Internet connection to CavitySpace database

Usage:
    python download_cavityspace_structures.py

Output:
    - cavityspace_structures/ folder with downloaded AlphaFold structures
    - cavityspace_mapping.csv: Updated mapping with structure paths
    - cavityspace_download.log
"""

import os
import sys
import pandas as pd
import logging
import time
import subprocess
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import requests

# --- Configuration ---
CAVITY_MAPPING_CSV = "cavity_mapping.csv"  # Input file from extract_cavities.py
OUTPUT_AF_DIR = "cavityspace_structures"   # Output directory for CavitySpace structures
OUTPUT_MAPPING_CSV = "cavityspace_mapping.csv"  # Output mapping with structure paths
LOG_FILE = "cavityspace_download.log"
NUM_THREADS = 8  # Reduced to be respectful to CavitySpace servers
DATABASE_VERSION = "v1"  # CavitySpace uses v1 AlphaFold models
REQUEST_DELAY = 1.0  # Delay between requests in seconds (be respectful!)
CAVITYSPACE_BASE_URL = "http://60.205.184.249:8000/csservice/item"

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

def download_file(url, output_path, max_retries=3):
    """
    Download a file from URL with retry logic and respectful delays.
    
    Parameters
    ----------
    url: str
        URL to download from
    output_path: str
        Local path to save file
    max_retries: int
        Maximum number of retry attempts
        
    Returns
    -------
    bool: True if download successful, False otherwise
    """
    for attempt in range(max_retries):
        try:
            # Check if file already exists
            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                return True
                
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # Add respectful delay before making request
            if attempt > 0:
                time.sleep(REQUEST_DELAY * (2 ** attempt))  # Exponential backoff
            else:
                time.sleep(REQUEST_DELAY)  # Base delay
            
            # Download using requests with streaming
            headers = {
                'User-Agent': 'Consensus-Docking-Workflow/1.0 (Research Project)',
                'Accept': 'application/octet-stream'
            }
            response = requests.get(url, stream=True, timeout=60, headers=headers)
            response.raise_for_status()
            
            # Write file
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # Verify file was downloaded and has content
            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                return True
            else:
                logging.warning(f"Downloaded file is empty: {output_path}")
                return False
                
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                logging.warning(f"Structure not found in CavitySpace for {url}")
                return False
            logging.warning(f"Download attempt {attempt + 1} failed for {url}: {e}")
        except Exception as e:
            logging.warning(f"Download attempt {attempt + 1} failed for {url}: {e}")
            
        if attempt < max_retries - 1:
            wait_time = REQUEST_DELAY * (2 ** attempt)
            logging.info(f"Waiting {wait_time:.1f}s before retry...")
            time.sleep(wait_time)
            
    return False

def process_download_task(download_task):
    """
    Process a single CavitySpace download task.
    
    Parameters
    ----------
    download_task: dict
        Dictionary containing download information
        
    Returns
    -------
    dict: Result of the download
    """
    uniprot_id = download_task['uniprot_id']
    fragment_id = download_task['fragment_id']
    model_url = download_task['model_url']
    model_path = download_task['model_path']
    
    try:
        # Download model PDB file
        model_success = download_file(model_url, model_path)
        
        return {
            'success': model_success,
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'model_path': model_path if model_success else None,
            'model_downloaded': model_success,
            'error': None if model_success else 'Download failed or structure not available'
        }
        
    except Exception as e:
        return {
            'success': False,
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'model_path': None,
            'model_downloaded': False,
            'error': str(e)
        }

def load_cavity_mapping(mapping_file):
    """Load cavity mapping from CSV file."""
    if not os.path.exists(mapping_file):
        logging.error(f"Cavity mapping file not found: {mapping_file}")
        logging.error("Please run extract_cavities.py first.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['UniProt_ID', 'Fragment_ID']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.error(f"Required columns missing from cavity mapping: {missing_columns}")
            sys.exit(1)
        
        logging.info(f"Loaded {len(df)} cavity mappings from {mapping_file}")
        return df
        
    except Exception as e:
        logging.error(f"Error loading cavity mapping: {e}")
        sys.exit(1)

def prepare_download_tasks(cavity_df, output_dir):
    """
    Prepare CavitySpace download tasks.
    
    Returns
    -------
    list: List of download tasks
    dict: Statistics about downloads to be performed
    """
    logging.info("Preparing CavitySpace download tasks...")
    
    # Get unique UniProt ID and Fragment ID combinations
    unique_combinations = cavity_df[['UniProt_ID', 'Fragment_ID']].drop_duplicates()
    download_tasks = []
    
    for _, row in tqdm(unique_combinations.iterrows(), total=len(unique_combinations), desc="Preparing download tasks"):
        uniprot_id = row['UniProt_ID']
        fragment_id = row['Fragment_ID']
        
        # Generate AlphaFold ID for CavitySpace with fragment info
        alphafold_id = f"AF-{uniprot_id}-F{fragment_id}"
        
        # Generate CavitySpace URL
        # Format: http://60.205.184.249:8000/csservice/item/{uniprot_id}/structure/alphafold/{alphafold_id}-model_{version}/{alphafold_id}-model_{version}.pdb
        model_url = f"{CAVITYSPACE_BASE_URL}/{uniprot_id}/structure/alphafold/{alphafold_id}-model_{DATABASE_VERSION}/{alphafold_id}-model_{DATABASE_VERSION}.pdb"
        
        # Generate local paths
        fragment_dir = os.path.join(output_dir, uniprot_id, f"F{fragment_id}")
        model_path = os.path.join(fragment_dir, f"{alphafold_id}-model_{DATABASE_VERSION}.pdb")
        
        # Check if file already exists
        model_exists = os.path.exists(model_path) and os.path.getsize(model_path) > 0
        
        if model_exists:
            logging.debug(f"CavitySpace structure already exists for {uniprot_id} F{fragment_id}")
            continue
        
        download_tasks.append({
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'alphafold_id': alphafold_id,
            'model_url': model_url,
            'model_path': model_path
        })
    
    stats = {
        'total_combinations': len(unique_combinations),
        'unique_uniprot_ids': len(cavity_df['UniProt_ID'].unique()),
        'download_tasks': len(download_tasks),
        'already_downloaded': len(unique_combinations) - len(download_tasks)
    }
    
    logging.info(f"Found {stats['unique_uniprot_ids']} unique UniProt IDs with {stats['total_combinations']} fragments")
    logging.info(f"Prepared {stats['download_tasks']} download tasks")
    logging.info(f"Already downloaded: {stats['already_downloaded']} structures")
    
    return download_tasks, stats

def create_cavityspace_mapping(cavity_df, output_dir):
    """Create mapping CSV with CavitySpace structure paths."""
    logging.info("Creating CavitySpace mapping...")
    
    updated_rows = []
    for _, row in cavity_df.iterrows():
        uniprot_id = row['UniProt_ID']
        fragment_id = row['Fragment_ID']
        pocket_pdb = row['Pocket_PDB']
        
        # Generate CavitySpace structure paths
        alphafold_id = f"AF-{uniprot_id}-F{fragment_id}"
        fragment_dir = os.path.join(output_dir, uniprot_id, f"F{fragment_id}")
        structure_path = os.path.join(fragment_dir, f"{alphafold_id}-model_{DATABASE_VERSION}.pdb")
        
        # Check if structure file exists
        structure_exists = os.path.exists(structure_path) and os.path.getsize(structure_path) > 0
        
        updated_rows.append({
            'UniProt_ID': uniprot_id,
            'Fragment_ID': fragment_id,
            'Cavity_Index': row.get('Cavity_Index', 1),
            'Receptor_PDB': structure_path if structure_exists else None,
            'Pocket_PDB': pocket_pdb,
            'AlphaFold_ID': alphafold_id,
            'CavitySpace_Available': structure_exists
        })
    
    updated_df = pd.DataFrame(updated_rows)
    return updated_df

def main():
    """Main function for CavitySpace model download."""
    start_time = time.time()
    
    logging.info("Starting CavitySpace AlphaFold model download workflow")
    logging.info(f"Input: {CAVITY_MAPPING_CSV}")
    logging.info(f"Output directory: {OUTPUT_AF_DIR}")
    logging.info(f"Output mapping: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Number of threads: {NUM_THREADS}")
    logging.info(f"Database version: {DATABASE_VERSION}")
    logging.info(f"Request delay: {REQUEST_DELAY}s (being respectful to CavitySpace)")
    
    # Create output directory
    os.makedirs(OUTPUT_AF_DIR, exist_ok=True)
    
    # Load cavity mapping
    cavity_df = load_cavity_mapping(CAVITY_MAPPING_CSV)
    
    # Prepare download tasks
    download_tasks, stats = prepare_download_tasks(cavity_df, OUTPUT_AF_DIR)
    
    if not download_tasks:
        logging.info("No download tasks needed. All CavitySpace structures already downloaded.")
    else:
        # Perform parallel downloads (with reduced threads to be respectful)
        logging.info(f"Starting parallel download of {len(download_tasks)} CavitySpace structures...")
        logging.info("Using reduced thread count and delays to be respectful to CavitySpace servers")
        
        successful_downloads = 0
        failed_downloads = 0
        
        with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
            # Submit all tasks
            future_to_task = {executor.submit(process_download_task, task): task for task in download_tasks}
            
            # Process results with progress bar
            for future in tqdm(as_completed(future_to_task), total=len(download_tasks), 
                             desc="Downloading CavitySpace structures"):
                result = future.result()
                
                if result['success']:
                    successful_downloads += 1
                    logging.debug(f"Downloaded: {result['uniprot_id']} F{result['fragment_id']}")
                else:
                    failed_downloads += 1
                    logging.warning(f"Failed to download {result['uniprot_id']} F{result['fragment_id']}: {result['error']}")
        
        logging.info(f"Download completed: {successful_downloads} successful, {failed_downloads} failed")
    
    # Create CavitySpace mapping CSV
    cavityspace_df = create_cavityspace_mapping(cavity_df, OUTPUT_AF_DIR)
    cavityspace_df.to_csv(OUTPUT_MAPPING_CSV, index=False)
    
    # Summary statistics
    total_structure_files = len([f for f in Path(OUTPUT_AF_DIR).rglob("*.pdb")])
    available_structures = len(cavityspace_df[cavityspace_df['CavitySpace_Available'] == True])
    
    elapsed_time = time.time() - start_time
    
    logging.info(f"\n--- Download Summary ---")
    logging.info(f"Total cavity mappings processed: {len(cavity_df)}")
    logging.info(f"Unique UniProt IDs: {stats['unique_uniprot_ids']}")
    logging.info(f"Total fragments: {stats['total_combinations']}")
    logging.info(f"CavitySpace PDB files downloaded/found: {total_structure_files}")
    logging.info(f"Available structures: {available_structures}")
    logging.info(f"Updated mapping saved to: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Total processing time: {elapsed_time:.2f} seconds")
    
    # Verify the output
    missing_structures = cavityspace_df[cavityspace_df['CavitySpace_Available'] == False]
    if len(missing_structures) > 0:
        logging.warning(f"Warning: {len(missing_structures)} entries missing CavitySpace structures")
        logging.warning("These UniProt IDs may not be available in the CavitySpace database")
    else:
        logging.info("All entries have corresponding CavitySpace structures!")

if __name__ == "__main__":
    main()
