#!/usr/bin/env python3
"""
Selective PDB Fixer for Consensus Docking
Adds hydrogens to AlphaFold structures that will be used in docking.

This script processes only the AlphaFold structures identified as required for docking,
making the workflow more efficient. It adds hydrogens needed for accurate docking
without modifying the complete AlphaFold2 structures. Uses automatic CPU detection 
for optimal parallelization.

Features:
- Parallel processing with auto-detected thread count
- Selective processing of only required structures
- Hydrogen addition only (AlphaFold2 structures are already complete)
- Robust error handling and progress reporting
- Atomic file writes for safe parallel execution
- Resume capability: skips already fixed files

Prerequisites:
- required_structures.csv (generated by identify_required_structures.py)
- alphafold_mapping.csv (generated by extract_alphafold_models.py)

Usage:
    python fix_required_pdbs.py

Output:
    - fixed_structures/ folder with hydrogenated PDB files
    - fixed_mapping.csv: Updated mapping with fixed PDB paths
    - pdb_fixing.log
"""

import os
import sys
import pandas as pd
import logging
import time
import multiprocessing
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

try:
    from pdbfixer import PDBFixer
    from openmm.app import PDBFile
    import openmm
except ImportError:
    logging.error("Required packages not installed. Please install:")
    logging.error("conda install -c conda-forge pdbfixer openmm")
    sys.exit(1)

# --- Configuration ---
REQUIRED_STRUCTURES_CSV = "required_structures.csv"  # Input from identify_required_structures.py
ALPHAFOLD_MAPPING_CSV = "alphafold_mapping.csv"      # AlphaFold structures from extract_alphafold_models.py
OUTPUT_DIR = "fixed_structures"                      # Output directory for fixed PDBs
OUTPUT_MAPPING_CSV = "fixed_mapping.csv"           # Output mapping with fixed paths
LOG_FILE = "pdb_fixing.log"

# Auto-detect optimal thread count (75% of available CPUs, minimum 2, maximum 16)
AVAILABLE_CPUS = multiprocessing.cpu_count()
NUM_THREADS = max(2, min(16, int(AVAILABLE_CPUS * 0.75)))

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

def fix_pdb_file(input_pdb_path, output_pdb_path):
    """
    Add hydrogens to a single AlphaFold PDB file using PDBFixer.
    
    AlphaFold2 structures are complete computational predictions, so we only
    need to add hydrogens for docking. No missing residues/atoms to fix.
    
    Parameters
    ----------
    input_pdb_path: str
        Path to input AlphaFold PDB file
    output_pdb_path: str
        Path to output hydrogenated PDB file
        
    Returns
    -------
    bool: True if successful, False otherwise
    """
    try:
        # Check if output already exists and is valid
        if os.path.exists(output_pdb_path) and os.path.getsize(output_pdb_path) > 0:
            return True
        
        # Validate input file
        if not os.path.exists(input_pdb_path):
            logging.warning(f"Input PDB file not found: {input_pdb_path}")
            return False
            
        if os.path.getsize(input_pdb_path) == 0:
            logging.warning(f"Input PDB file is empty: {input_pdb_path}")
            return False
            
        # Create output directory
        os.makedirs(os.path.dirname(output_pdb_path), exist_ok=True)
        
        # Initialize PDBFixer with better error handling
        try:
            fixer = PDBFixer(filename=input_pdb_path)
        except Exception as e:
            logging.warning(f"Failed to load PDB file {input_pdb_path}: {e}")
            return False
        
        # Only add hydrogens - AlphaFold2 structures are already complete
        # No need to find/fix missing residues or atoms since these are computational predictions
        fixer.addMissingHydrogens(7.0)  # pH 7.0
        
        # Write the fixed PDB with atomic write (safer for parallel execution)
        temp_output = output_pdb_path + '.tmp'
        try:
            with open(temp_output, 'w') as output_file:
                PDBFile.writeFile(fixer.topology, fixer.positions, output_file)
            
            # Atomic move to final location
            os.rename(temp_output, output_pdb_path)
            
        except Exception as e:
            # Clean up temp file if it exists
            if os.path.exists(temp_output):
                os.remove(temp_output)
            raise e
        
        # Verify the output file was created and has content
        if os.path.exists(output_pdb_path) and os.path.getsize(output_pdb_path) > 100:  # Minimum reasonable size
            return True
        else:
            logging.warning(f"Fixed PDB file is empty or too small: {output_pdb_path}")
            return False
            
    except Exception as e:
        logging.error(f"Error fixing PDB {input_pdb_path}: {e}")
        # Clean up any partial files
        if os.path.exists(output_pdb_path + '.tmp'):
            os.remove(output_pdb_path + '.tmp')
        return False

def process_fixing_task(fixing_task):
    """
    Process a single PDB fixing task.
    
    Parameters
    ----------
    fixing_task: dict
        Dictionary containing fixing information
        
    Returns
    -------
    dict: Result of the fixing
    """
    uniprot_id = fixing_task['uniprot_id']
    fragment_id = fixing_task['fragment_id']
    input_pdb = fixing_task['input_pdb']
    output_pdb = fixing_task['output_pdb']
    
    try:
        success = fix_pdb_file(input_pdb, output_pdb)
        
        return {
            'success': success,
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'input_pdb': input_pdb,
            'output_pdb': output_pdb if success else None,
            'error': None if success else 'PDB fixing failed'
        }
        
    except Exception as e:
        return {
            'success': False,
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'input_pdb': input_pdb,
            'output_pdb': None,
            'error': str(e)
        }

def load_required_structures():
    """Load the list of required structures and find corresponding AlphaFold files."""
    # Load required structures list
    if not os.path.exists(REQUIRED_STRUCTURES_CSV):
        logging.error(f"Required structures file not found: {REQUIRED_STRUCTURES_CSV}")
        logging.error("Run identify_required_structures.py first")
        sys.exit(1)
    
    # AlphaFold mapping is required for this script
    if not os.path.exists(ALPHAFOLD_MAPPING_CSV):
        logging.error(f"AlphaFold mapping file not found: {ALPHAFOLD_MAPPING_CSV}")
        logging.error("Run extract_alphafold_models.py first")
        sys.exit(1)
    
    try:
        # Load required structures (without Receptor_PDB column)
        required_df = pd.read_csv(REQUIRED_STRUCTURES_CSV)
        logging.info(f"Loaded {len(required_df)} required structures")
        
        # Ensure required columns exist in required structures
        required_columns = ['UniProt_ID', 'Fragment_ID']
        missing_columns = [col for col in required_columns if col not in required_df.columns]
        if missing_columns:
            logging.error(f"Missing required columns in {REQUIRED_STRUCTURES_CSV}: {missing_columns}")
            sys.exit(1)
        
        # Use AlphaFold mapping to get Receptor_PDB paths for only the required structures
        return load_from_alphafold_mapping(required_df)
        
    except Exception as e:
        logging.error(f"Error loading required structures: {e}")
        sys.exit(1)

def load_from_alphafold_mapping(required_df):
    """Load required structures using alphafold_mapping.csv, filtering to only required UniProt IDs."""
    try:
        # Load AlphaFold mapping (with Receptor_PDB paths)
        alphafold_df = pd.read_csv(ALPHAFOLD_MAPPING_CSV)
        logging.info(f"Loaded {len(alphafold_df)} total AlphaFold mappings")
        
        # Ensure required columns exist in AlphaFold mapping
        alphafold_columns = ['UniProt_ID', 'Fragment_ID', 'Receptor_PDB', 'AlphaFold_Available']
        missing_alphafold_columns = [col for col in alphafold_columns if col not in alphafold_df.columns]
        if missing_alphafold_columns:
            logging.error(f"Missing required columns in {ALPHAFOLD_MAPPING_CSV}: {missing_alphafold_columns}")
            sys.exit(1)
        
        # Get unique combinations of UniProt ID and Fragment ID from required structures
        required_combinations = required_df[['UniProt_ID', 'Fragment_ID']].drop_duplicates()
        logging.info(f"Looking for {len(required_combinations)} unique protein-fragment combinations")
        
        # Filter AlphaFold mapping to only include required structures
        # Use merge to filter alphafold_df to only the required combinations
        filtered_alphafold = alphafold_df.merge(
            required_combinations,
            on=['UniProt_ID', 'Fragment_ID'],
            how='inner'  # Only keep combinations that exist in both dataframes
        )
        
        logging.info(f"Found {len(filtered_alphafold)} AlphaFold mappings for required structures")
        
        # Now merge the required structures with the filtered AlphaFold mapping to get complete information
        merged_df = required_df.merge(
            filtered_alphafold[['UniProt_ID', 'Fragment_ID', 'Receptor_PDB', 'AlphaFold_Available']],
            on=['UniProt_ID', 'Fragment_ID'],
            how='left'
        )
        
        # Filter to only include structures that have AlphaFold files available
        available_structures = merged_df[merged_df['AlphaFold_Available'] == True].copy()
        missing_structures = merged_df[merged_df['AlphaFold_Available'] != True]
        
        if len(missing_structures) > 0:
            logging.warning(f"Warning: {len(missing_structures)} required structures don't have AlphaFold files available")
            logging.warning("These will be skipped during processing")
        
        # Get unique protein-fragment combinations that will actually be processed
        unique_to_process = available_structures[['UniProt_ID', 'Fragment_ID']].drop_duplicates()
        
        logging.info(f"Found {len(available_structures)} cavity mappings for structures that will be processed")
        logging.info(f"Unique protein-fragment combinations to process: {len(unique_to_process)}")
        
        return available_structures
        
    except Exception as e:
        logging.error(f"Error loading from AlphaFold mapping: {e}")
        sys.exit(1)




def prepare_fixing_tasks(required_df, output_dir):
    """
    Prepare PDB fixing tasks with deduplication by unique receptor structures.
    
    Returns
    -------
    list: List of fixing tasks
    dict: Statistics about fixes to be performed
    """
    logging.info("Preparing PDB fixing tasks with deduplication...")
    
    # All structures in required_df are available and needed
    available_df = required_df.copy()
    
    logging.info(f"Required structures: {len(required_df)}")
    logging.info(f"Available AlphaFold structures for required targets: {len(available_df)}")
    
    # Get unique combinations of UniProt ID, Fragment ID, and Receptor_PDB
    # This deduplicates cases where the same receptor appears multiple times for different cavities
    unique_receptors = available_df[['UniProt_ID', 'Fragment_ID', 'Receptor_PDB']].drop_duplicates()
    
    logging.info(f"Unique receptor structures to process: {len(unique_receptors)}")
    logging.info(f"Deduplication saved processing {len(available_df) - len(unique_receptors)} duplicate structures")
    
    fixing_tasks = []
    skipped_existing = 0
    missing_files = 0
    
    for _, row in tqdm(unique_receptors.iterrows(), total=len(unique_receptors), desc="Preparing fixing tasks"):
        uniprot_id = row['UniProt_ID']
        fragment_id = row['Fragment_ID']
        input_pdb = row['Receptor_PDB']
        
        # Check if input file exists
        if not os.path.exists(input_pdb):
            logging.warning(f"Input PDB not found: {input_pdb}")
            missing_files += 1
            continue
        
        # Generate output path
        output_subdir = os.path.join(output_dir, uniprot_id, f"F{fragment_id}")
        output_filename = f"AF-{uniprot_id}-F{fragment_id}-model_v4_H.pdb"
        output_pdb = os.path.join(output_subdir, output_filename)
        
        # Check if already processed
        if os.path.exists(output_pdb) and os.path.getsize(output_pdb) > 0:
            logging.debug(f"Hydrogenated PDB already exists: {output_pdb}")
            skipped_existing += 1
            continue
        
        fixing_tasks.append({
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'input_pdb': input_pdb,
            'output_pdb': output_pdb
        })
    
    stats = {
        'required_structures': len(required_df),
        'available_structures': len(available_df),
        'unique_receptors': len(unique_receptors),
        'duplicates_avoided': len(available_df) - len(unique_receptors),
        'fixing_tasks': len(fixing_tasks),
        'already_fixed': skipped_existing,
        'missing_files': missing_files
    }
    
    logging.info(f"Required structures: {stats['required_structures']}")
    logging.info(f"Available AlphaFold structures for required targets: {stats['available_structures']}")
    logging.info(f"Unique receptor structures: {stats['unique_receptors']}")
    logging.info(f"Duplicates avoided: {stats['duplicates_avoided']}")
    logging.info(f"Fixing tasks to perform: {stats['fixing_tasks']}")
    logging.info(f"Already hydrogenated: {stats['already_fixed']}")
    
    return fixing_tasks, stats, available_df

def create_fixed_mapping(required_df, output_dir):
    """Create mapping CSV with fixed PDB paths."""
    logging.info("Creating fixed structure mapping...")
    
    # Work directly with the required structures
    updated_rows = []
    for _, row in required_df.iterrows():
        uniprot_id = row['UniProt_ID']
        fragment_id = row['Fragment_ID']
        
        # Use the required structure's data
        pocket_pdb = row.get('Pocket_PDB', '')
        original_receptor = row.get('Receptor_PDB', '')
        
        # Generate hydrogenated PDB path
        output_subdir = os.path.join(output_dir, uniprot_id, f"F{fragment_id}")
        output_filename = f"AF-{uniprot_id}-F{fragment_id}-model_v4_H.pdb"
        hydrogenated_receptor = os.path.join(output_subdir, output_filename)
        
        # Check if hydrogenated file exists
        hydrogenated_exists = os.path.exists(hydrogenated_receptor) and os.path.getsize(hydrogenated_receptor) > 0
        
        updated_rows.append({
            'UniProt_ID': uniprot_id,
            'Fragment_ID': fragment_id,
            'Gene_Name': row.get('Gene_Name', ''),
            'Interaction_Count': row.get('Interaction_Count', 0),
            'Cavity_Index': row.get('Cavity_Index', 1),
            'Receptor_PDB': hydrogenated_receptor if hydrogenated_exists else original_receptor,
            'Receptor_Fixed': hydrogenated_exists,
            'Pocket_PDB': pocket_pdb,
            'AlphaFold_ID': f"AF-{uniprot_id}-F{fragment_id}",
            'AlphaFold_Available': True
        })
    
    updated_df = pd.DataFrame(updated_rows)
    
    # Log the filtering effect
    logging.info(f"Created fixed mapping with {len(updated_df)} entries (only required structures)")
    
    return updated_df

def main():
    """Main function for selective PDB fixing."""
    start_time = time.time()
    
    logging.info("Starting selective PDB hydrogenation workflow")
    logging.info(f"Required structures: {REQUIRED_STRUCTURES_CSV}")
    logging.info(f"AlphaFold mapping: {ALPHAFOLD_MAPPING_CSV}")
    logging.info(f"Output directory: {OUTPUT_DIR}")
    logging.info(f"Available CPUs: {AVAILABLE_CPUS}")
    logging.info(f"Using threads: {NUM_THREADS} (auto-detected)")
    
    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Load input data
    required_df = load_required_structures()
    
    # Prepare fixing tasks
    fixing_tasks, stats, available_df = prepare_fixing_tasks(required_df, OUTPUT_DIR)
    
    if not fixing_tasks:
        logging.info("No hydrogenation tasks needed. All required structures already hydrogenated.")
    else:
        # Perform parallel PDB fixing
        logging.info(f"Starting parallel hydrogenation of {len(fixing_tasks)} PDB structures...")
        logging.info(f"Using {NUM_THREADS} parallel threads")
        
        successful_fixes = 0
        failed_fixes = 0
        
        with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
            # Submit all tasks
            future_to_task = {executor.submit(process_fixing_task, task): task for task in fixing_tasks}
            
            # Process results with progress bar and detailed logging
            with tqdm(total=len(fixing_tasks), desc="Adding hydrogens to structures", 
                     unit="structures", ncols=100) as pbar:
                
                for future in as_completed(future_to_task):
                    try:
                        result = future.result()
                        
                        if result['success']:
                            successful_fixes += 1
                            pbar.set_postfix({
                                'Success': successful_fixes, 
                                'Failed': failed_fixes,
                                'Rate': f"{successful_fixes/(successful_fixes+failed_fixes)*100:.1f}%"
                            })
                            logging.debug(f"âœ“ Hydrogenated: {result['uniprot_id']} F{result['fragment_id']}")
                        else:
                            failed_fixes += 1
                            pbar.set_postfix({
                                'Success': successful_fixes, 
                                'Failed': failed_fixes,
                                'Rate': f"{successful_fixes/(successful_fixes+failed_fixes)*100:.1f}%"
                            })
                            logging.warning(f"âœ— Failed to hydrogenate {result['uniprot_id']} F{result['fragment_id']}: {result['error']}")
                        
                        pbar.update(1)
                        
                        # Log progress every 50 completed tasks
                        if (successful_fixes + failed_fixes) % 50 == 0:
                            completion_rate = (successful_fixes + failed_fixes) / len(fixing_tasks) * 100
                            success_rate = successful_fixes / (successful_fixes + failed_fixes) * 100
                            logging.info(f"Progress: {completion_rate:.1f}% complete, {success_rate:.1f}% success rate")
                    
                    except Exception as e:
                        failed_fixes += 1
                        logging.error(f"Unexpected error in fixing task: {e}")
                        pbar.update(1)
        
        elapsed_fixing = time.time() - start_time
        structures_per_second = len(fixing_tasks) / elapsed_fixing if elapsed_fixing > 0 else 0
        
        logging.info(f"Hydrogenation completed: {successful_fixes} successful, {failed_fixes} failed")
        logging.info(f"Hydrogenation performance: {structures_per_second:.2f} structures/second")
        
        if failed_fixes > 0:
            logging.warning(f"Failed to hydrogenate {failed_fixes}/{len(fixing_tasks)} structures ({failed_fixes/len(fixing_tasks)*100:.1f}%)")
        else:
            logging.info("ðŸŽ‰ All structures hydrogenated successfully!")
    
    # Create fixed mapping CSV
    fixed_df = create_fixed_mapping(required_df, OUTPUT_DIR)
    fixed_df.to_csv(OUTPUT_MAPPING_CSV, index=False)
    
    # Summary statistics
    total_hydrogenated_files = len([f for f in Path(OUTPUT_DIR).rglob("*_H.pdb")])
    available_hydrogenated = len(fixed_df[fixed_df['Receptor_Fixed'] == True])
    
    elapsed_time = time.time() - start_time
    
    logging.info(f"\n--- Hydrogenation Summary ---")
    logging.info(f"Required structures processed: {len(required_df)}")
    logging.info(f"Available AlphaFold structures: {stats['available_structures']}")
    logging.info(f"Hydrogenated PDB files created/found: {total_hydrogenated_files}")
    logging.info(f"Structures with hydrogenated receptors: {available_hydrogenated}")
    logging.info(f"Updated mapping saved to: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Total processing time: {elapsed_time:.2f} seconds")
    
    # Show completion status
    missing_hydrogenated = fixed_df[fixed_df['Receptor_Fixed'] == False]
    if len(missing_hydrogenated) > 0:
        logging.warning(f"Warning: {len(missing_hydrogenated)} structures missing hydrogenated PDB files")
    else:
        logging.info("All required structures have hydrogenated PDB files!")

if __name__ == "__main__":
    main()
