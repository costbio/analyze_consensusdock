#!/usr/bin/env python3
"""
Selective PDB Fixer for Consensus Docking
Fixes PDB files (adds hydrogens) only for structures that will be used in docking.

This script processes only the AlphaFold structures identified as required for docking,
making the workflow more efficient. It adds hydrogens and performs other PDB fixes
needed for accurate docking.

Prerequisites:
- required_structures.csv (generated by identify_required_structures.py)
- alphafold_mapping.csv (generated by extract_alphafold_models.py)

Usage:
    python fix_required_pdbs.py

Output:
    - fixed_structures/ folder with fixed PDB files
    - fixed_mapping.csv: Updated mapping with fixed PDB paths
    - pdb_fixing.log
"""

import os
import sys
import pandas as pd
import logging
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

try:
    from pdbfixer import PDBFixer
    from openmm.app import PDBFile
    import openmm
except ImportError:
    logging.error("Required packages not installed. Please install:")
    logging.error("conda install -c conda-forge pdbfixer openmm")
    sys.exit(1)

# --- Configuration ---
REQUIRED_STRUCTURES_CSV = "required_structures.csv"  # Input from identify_required_structures.py
ALPHAFOLD_MAPPING_CSV = "alphafold_mapping.csv"     # Input from extract_alphafold_models.py
OUTPUT_DIR = "fixed_structures"                      # Output directory for fixed PDBs
OUTPUT_MAPPING_CSV = "fixed_mapping.csv"           # Output mapping with fixed paths
LOG_FILE = "pdb_fixing.log"
NUM_THREADS = 4  # Number of parallel fixing threads

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

def fix_pdb_file(input_pdb_path, output_pdb_path):
    """
    Fix a single PDB file using PDBFixer.
    
    Parameters
    ----------
    input_pdb_path: str
        Path to input PDB file
    output_pdb_path: str
        Path to output fixed PDB file
        
    Returns
    -------
    bool: True if successful, False otherwise
    """
    try:
        # Check if output already exists and is valid
        if os.path.exists(output_pdb_path) and os.path.getsize(output_pdb_path) > 0:
            return True
            
        # Create output directory
        os.makedirs(os.path.dirname(output_pdb_path), exist_ok=True)
        
        # Initialize PDBFixer
        fixer = PDBFixer(filename=input_pdb_path)
        
        # Find and fix missing residues
        fixer.findMissingResidues()
        
        # Find and fix missing atoms
        fixer.findNonstandardResidues()
        fixer.replaceNonstandardResidues()
        fixer.findMissingAtoms()
        fixer.addMissingAtoms()
        
        # Add hydrogens
        fixer.addMissingHydrogens(7.0)  # pH 7.0
        
        # Write the fixed PDB
        with open(output_pdb_path, 'w') as output_file:
            PDBFile.writeFile(fixer.topology, fixer.positions, output_file)
        
        # Verify the output file was created and has content
        if os.path.exists(output_pdb_path) and os.path.getsize(output_pdb_path) > 0:
            return True
        else:
            logging.warning(f"Fixed PDB file is empty: {output_pdb_path}")
            return False
            
    except Exception as e:
        logging.error(f"Error fixing PDB {input_pdb_path}: {e}")
        return False

def process_fixing_task(fixing_task):
    """
    Process a single PDB fixing task.
    
    Parameters
    ----------
    fixing_task: dict
        Dictionary containing fixing information
        
    Returns
    -------
    dict: Result of the fixing
    """
    uniprot_id = fixing_task['uniprot_id']
    fragment_id = fixing_task['fragment_id']
    input_pdb = fixing_task['input_pdb']
    output_pdb = fixing_task['output_pdb']
    
    try:
        success = fix_pdb_file(input_pdb, output_pdb)
        
        return {
            'success': success,
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'input_pdb': input_pdb,
            'output_pdb': output_pdb if success else None,
            'error': None if success else 'PDB fixing failed'
        }
        
    except Exception as e:
        return {
            'success': False,
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'input_pdb': input_pdb,
            'output_pdb': None,
            'error': str(e)
        }

def load_required_structures():
    """Load the list of required structures."""
    if not os.path.exists(REQUIRED_STRUCTURES_CSV):
        logging.error(f"Required structures file not found: {REQUIRED_STRUCTURES_CSV}")
        logging.error("Run identify_required_structures.py first")
        sys.exit(1)
    
    try:
        df = pd.read_csv(REQUIRED_STRUCTURES_CSV)
        logging.info(f"Loaded {len(df)} required structures")
        return df
    except Exception as e:
        logging.error(f"Error loading required structures: {e}")
        sys.exit(1)

def load_alphafold_mapping():
    """Load AlphaFold structure mapping."""
    if not os.path.exists(ALPHAFOLD_MAPPING_CSV):
        logging.error(f"AlphaFold mapping not found: {ALPHAFOLD_MAPPING_CSV}")
        logging.error("Run extract_alphafold_models.py first")
        sys.exit(1)
    
    try:
        df = pd.read_csv(ALPHAFOLD_MAPPING_CSV)
        logging.info(f"Loaded {len(df)} AlphaFold mappings")
        return df
    except Exception as e:
        logging.error(f"Error loading AlphaFold mapping: {e}")
        sys.exit(1)

def prepare_fixing_tasks(required_df, alphafold_df, output_dir):
    """
    Prepare PDB fixing tasks for required structures.
    
    Returns
    -------
    list: List of fixing tasks
    dict: Statistics about fixes to be performed
    """
    logging.info("Preparing PDB fixing tasks...")
    
    # Merge required structures with AlphaFold mapping
    merged_df = required_df.merge(
        alphafold_df[['UniProt_ID', 'Fragment_ID', 'Receptor_PDB', 'AlphaFold_Available']],
        on=['UniProt_ID', 'Fragment_ID'],
        how='inner'
    )
    
    # Filter to only available AlphaFold structures
    available_df = merged_df[
        (merged_df['AlphaFold_Available'] == True) & 
        (merged_df['Receptor_PDB'].notna())
    ]
    
    logging.info(f"Found {len(available_df)} available AlphaFold structures to fix")
    
    fixing_tasks = []
    for _, row in tqdm(available_df.iterrows(), total=len(available_df), desc="Preparing fixing tasks"):
        uniprot_id = row['UniProt_ID']
        fragment_id = row['Fragment_ID']
        input_pdb = row['Receptor_PDB']
        
        # Check if input file exists
        if not os.path.exists(input_pdb):
            logging.warning(f"Input PDB not found: {input_pdb}")
            continue
        
        # Generate output path
        output_subdir = os.path.join(output_dir, uniprot_id, f"F{fragment_id}")
        output_filename = f"AF-{uniprot_id}-F{fragment_id}-model_v4_fixed.pdb"
        output_pdb = os.path.join(output_subdir, output_filename)
        
        # Check if already fixed
        if os.path.exists(output_pdb) and os.path.getsize(output_pdb) > 0:
            logging.debug(f"Fixed PDB already exists: {output_pdb}")
            continue
        
        fixing_tasks.append({
            'uniprot_id': uniprot_id,
            'fragment_id': fragment_id,
            'input_pdb': input_pdb,
            'output_pdb': output_pdb
        })
    
    stats = {
        'required_structures': len(required_df),
        'available_structures': len(available_df),
        'fixing_tasks': len(fixing_tasks),
        'already_fixed': len(available_df) - len(fixing_tasks)
    }
    
    logging.info(f"Required structures: {stats['required_structures']}")
    logging.info(f"Available AlphaFold structures: {stats['available_structures']}")
    logging.info(f"Fixing tasks to perform: {stats['fixing_tasks']}")
    logging.info(f"Already fixed: {stats['already_fixed']}")
    
    return fixing_tasks, stats, available_df

def create_fixed_mapping(required_df, alphafold_df, output_dir):
    """Create mapping CSV with fixed PDB paths."""
    logging.info("Creating fixed structure mapping...")
    
    # Merge required structures with AlphaFold mapping
    merged_df = required_df.merge(
        alphafold_df,
        on=['UniProt_ID', 'Fragment_ID'],
        how='inner'
    )
    
    updated_rows = []
    for _, row in merged_df.iterrows():
        uniprot_id = row['UniProt_ID']
        fragment_id = row['Fragment_ID']
        pocket_pdb = row['Pocket_PDB']
        original_receptor = row['Receptor_PDB']
        
        # Generate fixed PDB path
        output_subdir = os.path.join(output_dir, uniprot_id, f"F{fragment_id}")
        output_filename = f"AF-{uniprot_id}-F{fragment_id}-model_v4_fixed.pdb"
        fixed_receptor = os.path.join(output_subdir, output_filename)
        
        # Check if fixed file exists
        fixed_exists = os.path.exists(fixed_receptor) and os.path.getsize(fixed_receptor) > 0
        
        updated_rows.append({
            'UniProt_ID': uniprot_id,
            'Fragment_ID': fragment_id,
            'Gene_Name': row.get('Gene_Name', ''),
            'Interaction_Count': row.get('Interaction_Count', 0),
            'Cavity_Index': row.get('Cavity_Index', 1),
            'Receptor_PDB': fixed_receptor if fixed_exists else original_receptor,
            'Receptor_Fixed': fixed_exists,
            'Pocket_PDB': pocket_pdb,
            'AlphaFold_ID': row.get('AlphaFold_ID', f"AF-{uniprot_id}-F{fragment_id}"),
            'AlphaFold_Available': row.get('AlphaFold_Available', False)
        })
    
    updated_df = pd.DataFrame(updated_rows)
    return updated_df

def main():
    """Main function for selective PDB fixing."""
    start_time = time.time()
    
    logging.info("Starting selective PDB fixing workflow")
    logging.info(f"Required structures: {REQUIRED_STRUCTURES_CSV}")
    logging.info(f"AlphaFold mapping: {ALPHAFOLD_MAPPING_CSV}")
    logging.info(f"Output directory: {OUTPUT_DIR}")
    logging.info(f"Number of threads: {NUM_THREADS}")
    
    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Load input data
    required_df = load_required_structures()
    alphafold_df = load_alphafold_mapping()
    
    # Prepare fixing tasks
    fixing_tasks, stats, available_df = prepare_fixing_tasks(required_df, alphafold_df, OUTPUT_DIR)
    
    if not fixing_tasks:
        logging.info("No fixing tasks needed. All required structures already fixed.")
    else:
        # Perform parallel PDB fixing
        logging.info(f"Starting parallel fixing of {len(fixing_tasks)} PDB structures...")
        
        successful_fixes = 0
        failed_fixes = 0
        
        with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
            # Submit all tasks
            future_to_task = {executor.submit(process_fixing_task, task): task for task in fixing_tasks}
            
            # Process results with progress bar
            for future in tqdm(as_completed(future_to_task), total=len(fixing_tasks), 
                             desc="Fixing PDB structures"):
                result = future.result()
                
                if result['success']:
                    successful_fixes += 1
                    logging.debug(f"Fixed: {result['uniprot_id']} F{result['fragment_id']}")
                else:
                    failed_fixes += 1
                    logging.warning(f"Failed to fix {result['uniprot_id']} F{result['fragment_id']}: {result['error']}")
        
        logging.info(f"Fixing completed: {successful_fixes} successful, {failed_fixes} failed")
    
    # Create fixed mapping CSV
    fixed_df = create_fixed_mapping(required_df, alphafold_df, OUTPUT_DIR)
    fixed_df.to_csv(OUTPUT_MAPPING_CSV, index=False)
    
    # Summary statistics
    total_fixed_files = len([f for f in Path(OUTPUT_DIR).rglob("*_fixed.pdb")])
    available_fixed = len(fixed_df[fixed_df['Receptor_Fixed'] == True])
    
    elapsed_time = time.time() - start_time
    
    logging.info(f"\n--- Fixing Summary ---")
    logging.info(f"Required structures processed: {len(required_df)}")
    logging.info(f"Available AlphaFold structures: {stats['available_structures']}")
    logging.info(f"Fixed PDB files created/found: {total_fixed_files}")
    logging.info(f"Structures with fixed receptors: {available_fixed}")
    logging.info(f"Updated mapping saved to: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Total processing time: {elapsed_time:.2f} seconds")
    
    # Show completion status
    missing_fixed = fixed_df[fixed_df['Receptor_Fixed'] == False]
    if len(missing_fixed) > 0:
        logging.warning(f"Warning: {len(missing_fixed)} structures missing fixed PDB files")
    else:
        logging.info("All required structures have fixed PDB files!")

if __name__ == "__main__":
    main()
