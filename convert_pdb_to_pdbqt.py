#!/usr/bin/env python3
"""
PDB to PDBQT Conversion Script
This script converts receptor PDB files to PDBQT format with parallel processing.

Features:
- Parallel processing using multiple processes (ProcessPoolExecutor)
- Deduplication: Same receptor (UniProt ID) only converted once
- Resume capability: Skips already converted files
- Progress tracking and detailed logging
- Timeout handling for stuck conversions
- Automatic system resource detection
- Optimized for OpenBabel thread safety using multiprocessing

Prerequisites:
- fixed_mapping.csv (preferred, generated by fix_required_pdbs.py)
- alphafold_mapping.csv (alternative, generated by extract_alphafold_models.py)
- cavity_mapping.csv (fallback, generated by extract_cavities.py)
- openbabel/pybel installed

Usage:
    python convert_pdb_to_pdbqt.py

Environment Variables:
    PDBQT_THREADS: Number of worker processes (default: 60)
    PDBQT_TIMEOUT: Timeout per conversion in seconds (default: 300)

Output:
    - converted_pdbqt/ folder with PDBQT files
    - pdbqt_mapping.csv: Updated mapping with PDBQT paths
    - pdb_to_pdbqt_conversion.log

Note: This script now uses multiprocessing by default for better OpenBabel compatibility
and should properly utilize all CPU cores for parallel conversion.
"""

import os
import sys
import pandas as pd
import logging
import time
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, TimeoutError
from tqdm import tqdm
import hashlib
import shutil
import multiprocessing
import psutil

try:
    from openbabel import pybel, openbabel
except ImportError:
    logging.error("OpenBabel/pybel not found. Please install openbabel-python.")
    sys.exit(1)

def check_system_resources():
    """Check system resources and provide configuration recommendations."""
    try:
        # Get CPU and memory info
        cpu_count = multiprocessing.cpu_count()
        
        # Try to get memory info if psutil is available
        try:
            import psutil
            memory_gb = psutil.virtual_memory().total / (1024**3)
            logging.info(f"System resources: {cpu_count} CPU cores, {memory_gb:.1f} GB RAM")
            
            # Provide recommendations based on system resources
            if memory_gb < 4:
                recommended_workers = min(cpu_count, 2)
                logging.warning(f"Low memory detected. Recommended workers: {recommended_workers}")
            elif memory_gb < 8:
                recommended_workers = min(cpu_count, 4)
                logging.info(f"Moderate memory detected. Recommended workers: {recommended_workers}")
            else:
                recommended_workers = min(cpu_count, 8)
                logging.info(f"Sufficient memory detected. Recommended workers: {recommended_workers}")
                
        except ImportError:
            logging.info(f"System resources: {cpu_count} CPU cores (memory info unavailable)")
            recommended_workers = min(cpu_count, 4)
            
        return recommended_workers
        
    except Exception as e:
        logging.warning(f"Could not check system resources: {e}")
        return 2  # Conservative fallback

# --- Configuration ---
FIXED_MAPPING_CSV = "fixed_mapping.csv"      # Preferred input from fix_required_pdbs.py
ALPHAFOLD_MAPPING_CSV = "alphafold_mapping.csv"  # Alternative input from extract_alphafold_models.py
CAVITY_MAPPING_CSV = "cavity_mapping.csv"    # Fallback input from extract_cavities.py
OUTPUT_PDBQT_DIR = "converted_pdbqt"         # Output directory for PDBQT files
OUTPUT_MAPPING_CSV = "pdbqt_mapping.csv"     # Output mapping with PDBQT paths
LOG_FILE = "pdb_to_pdbqt_conversion.log"

# Dynamic thread/process configuration
CPU_COUNT = multiprocessing.cpu_count()
RECOMMENDED_WORKERS = check_system_resources()
NUM_THREADS = int(os.environ.get('PDBQT_THREADS', 60))  # Default to 60 threads
USE_PROCESSES = True  # Always use processes instead of threads for better OpenBabel compatibility
CONVERSION_TIMEOUT = int(os.environ.get('PDBQT_TIMEOUT', 300))  # 5 minutes default timeout
PH_VALUE = 7.4   # Protonation pH

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

def get_file_hash(file_path):
    """Calculate MD5 hash of a file to check for duplicates."""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def pdb_to_pdbqt_safe(pdb_path, pdbqt_path, pH=7.4):
    """
    Thread-safe wrapper for PDB to PDBQT conversion.
    
    Parameters
    ----------
    pdb_path: str or pathlib.Path
        Path to input PDB file.
    pdbqt_path: str or pathlib.Path
        Path to output PDBQT file.
    pH: float
        Protonation at given pH.
        
    Returns
    -------
    tuple: (success: bool, error_message: str or None)
    """
    try:
        # Import pybel in the function to avoid thread safety issues
        from openbabel import pybel
        
        # Read the PDB file
        molecules = list(pybel.readfile("pdb", str(pdb_path)))
        if not molecules:
            return False, "No molecules found in PDB file"
        
        molecule = molecules[0]
        
        # Add hydrogens and partial charges
        # Note: Commenting out pH correction as it might cause issues
        # molecule.OBMol.CorrectForPH(pH)
        # molecule.addh()
        
        # Add partial charges to each atom
        for atom in molecule.atoms:
            atom.OBAtom.GetPartialCharge()
        
        # Write PDBQT file
        molecule.write("pdbqt", str(pdbqt_path), overwrite=True)
        return True, None
        
    except Exception as e:
        return False, str(e)

def pdb_to_pdbqt(pdb_path, pdbqt_path, pH=7.4):
    """
    Convert a PDB file to a PDBQT file needed by docking programs of the AutoDock family.
    
    Parameters
    ----------
    pdb_path: str or pathlib.Path
        Path to input PDB file.
    pdbqt_path: str or pathlib.Path
        Path to output PDBQT file.
    pH: float
        Protonation at given pH.
        
    Returns
    -------
    bool: True if conversion successful, False otherwise
    """
    try:
        success, error = pdb_to_pdbqt_safe(pdb_path, pdbqt_path, pH)
        if not success:
            logging.error(f"Error converting {pdb_path} to PDBQT: {error}")
        return success
    except Exception as e:
        logging.error(f"Error converting {pdb_path} to PDBQT: {e}")
        return False

def convert_pdb_to_pdbqt_standalone(args):
    """
    Standalone function for PDB to PDBQT conversion that works with multiprocessing.
    
    Parameters
    ----------
    args: tuple
        (pdb_path, pdbqt_path, uniprot_id, pH_value)
        
    Returns
    -------
    dict: Result of the conversion
    """
    pdb_path, pdbqt_path, uniprot_id, pH_value = args
    process_id = os.getpid()
    
    try:
        # Import pybel in the function to avoid issues
        from openbabel import pybel
        
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(pdbqt_path), exist_ok=True)
        
        # Read the PDB file
        molecules = list(pybel.readfile("pdb", str(pdb_path)))
        if not molecules:
            return {
                'success': False,
                'pdb_path': pdb_path,
                'pdbqt_path': pdbqt_path,
                'uniprot_id': uniprot_id,
                'action': 'failed',
                'error': 'No molecules found in PDB file',
                'process_id': process_id
            }
        
        molecule = molecules[0]
        
        # Add partial charges to each atom
        for atom in molecule.atoms:
            atom.OBAtom.GetPartialCharge()
        
        # Write PDBQT file
        molecule.write("pdbqt", str(pdbqt_path), overwrite=True)
        
        return {
            'success': True,
            'pdb_path': pdb_path,
            'pdbqt_path': pdbqt_path,
            'uniprot_id': uniprot_id,
            'action': 'converted',
            'error': None,
            'process_id': process_id
        }
        
    except Exception as e:
        return {
            'success': False,
            'pdb_path': pdb_path,
            'pdbqt_path': pdbqt_path,
            'uniprot_id': uniprot_id,
            'action': 'failed',
            'error': str(e),
            'process_id': process_id
        }

def process_conversion(conversion_task):
    """
    Process a single PDB to PDBQT conversion task.
    This function now wraps the standalone function for compatibility.
    
    Parameters
    ----------
    conversion_task: dict
        Dictionary containing conversion information
        
    Returns
    -------
    dict: Result of the conversion
    """
    pdb_path = conversion_task['pdb_path']
    pdbqt_path = conversion_task['pdbqt_path']
    uniprot_id = conversion_task['uniprot_id']
    is_copy = conversion_task.get('is_copy', False)
    source_pdbqt = conversion_task.get('source_pdbqt', None)
    
    if is_copy and source_pdbqt:
        # Handle copying separately since it doesn't need OpenBabel
        try:
            os.makedirs(os.path.dirname(pdbqt_path), exist_ok=True)
            shutil.copy2(source_pdbqt, pdbqt_path)
            return {
                'success': True,
                'pdb_path': pdb_path,
                'pdbqt_path': pdbqt_path,
                'uniprot_id': uniprot_id,
                'action': 'copied',
                'error': None,
                'process_id': os.getpid()
            }
        except Exception as e:
            return {
                'success': False,
                'pdb_path': pdb_path,
                'pdbqt_path': pdbqt_path,
                'uniprot_id': uniprot_id,
                'action': 'failed',
                'error': str(e),
                'process_id': os.getpid()
            }
    else:
        # Use the standalone function for actual conversion
        return convert_pdb_to_pdbqt_standalone((pdb_path, pdbqt_path, uniprot_id, PH_VALUE))

def load_cavity_mapping(mapping_file):
    """Load cavity mapping from CSV file."""
    if not os.path.exists(mapping_file):
        logging.error(f"Cavity mapping file not found: {mapping_file}")
        logging.error("Please run extract_cavities.py first.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['UniProt_ID', 'Receptor_PDB', 'Pocket_PDB']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.error(f"Required columns missing from cavity mapping: {missing_columns}")
            sys.exit(1)
        
        logging.info(f"Loaded {len(df)} cavity mappings from {mapping_file}")
        return df
        
    except Exception as e:
        logging.error(f"Error loading cavity mapping: {e}")
        sys.exit(1)

def load_best_cavity_mapping():
    """
    Load the best available cavity mapping file.
    Prioritizes AlphaFold mapping over original cavity mapping.
    
    Returns
    -------
    pd.DataFrame: Cavity mapping dataframe
    str: Source file used
    """
    # Check for fixed mapping first (preferred)
    if os.path.exists(FIXED_MAPPING_CSV):
        logging.info(f"Using fixed mapping: {FIXED_MAPPING_CSV}")
        df = load_cavity_mapping(FIXED_MAPPING_CSV)
        return df, FIXED_MAPPING_CSV
    
    # Check for AlphaFold mapping second
    elif os.path.exists(ALPHAFOLD_MAPPING_CSV):
        logging.info(f"Using AlphaFold mapping: {ALPHAFOLD_MAPPING_CSV}")
        df = load_cavity_mapping(ALPHAFOLD_MAPPING_CSV)
        return df, ALPHAFOLD_MAPPING_CSV
    
    # Fall back to original cavity mapping
    elif os.path.exists(CAVITY_MAPPING_CSV):
        logging.info(f"Using original cavity mapping: {CAVITY_MAPPING_CSV}")
        logging.info("Consider running fix_required_pdbs.py first for fixed structures with hydrogens")
        df = load_cavity_mapping(CAVITY_MAPPING_CSV)
        return df, CAVITY_MAPPING_CSV
    
    else:
        logging.error("No cavity mapping file found!")
        logging.error("Please run extract_cavities.py (and optionally fix_required_pdbs.py) first.")
        sys.exit(1)

def prepare_conversion_tasks(cavity_df, output_dir):
    """
    Prepare conversion tasks with deduplication.
    
    Returns
    -------
    list: List of conversion tasks
    dict: Mapping of UniProt IDs to representative PDBQT files
    """
    logging.info("Analyzing PDB files for deduplication...")
    
    # Group by UniProt ID and find unique receptor files
    uniprot_groups = cavity_df.groupby('UniProt_ID')
    unique_receptors = {}  # uniprot_id -> (pdb_path, pdb_hash, pdbqt_path)
    conversion_tasks = []
    
    total_files = len(cavity_df)
    processed_files = 0
    
    for uniprot_id, group in tqdm(uniprot_groups, desc="Analyzing receptors"):
        receptor_files = group['Receptor_PDB'].unique()
        
        # Find the representative file (first unique one)
        representative_pdb = None
        representative_hash = None
        representative_pdbqt = None
        
        for pdb_path in receptor_files:
            if not os.path.exists(pdb_path):
                logging.warning(f"PDB file not found: {pdb_path}")
                continue
            
            # Calculate hash to check if this is a unique structure
            file_hash = get_file_hash(pdb_path)
            
            if representative_pdb is None:
                # This is our representative file
                representative_pdb = pdb_path
                representative_hash = file_hash
                
                # Generate PDBQT path
                pdb_filename = Path(pdb_path).stem
                representative_pdbqt = os.path.join(output_dir, f"{uniprot_id}", f"{pdb_filename}.pdbqt")
                
                # Check if PDBQT already exists
                if os.path.exists(representative_pdbqt):
                    logging.debug(f"PDBQT already exists for {uniprot_id}: {representative_pdbqt}")
                    # Still track it in unique_receptors for mapping generation
                    unique_receptors[uniprot_id] = (representative_pdb, representative_hash, representative_pdbqt)
                else:
                    # Add conversion task
                    conversion_tasks.append({
                        'pdb_path': representative_pdb,
                        'pdbqt_path': representative_pdbqt,
                        'uniprot_id': uniprot_id,
                        'is_copy': False
                    })
                    # Track it for mapping generation
                    unique_receptors[uniprot_id] = (representative_pdb, representative_hash, representative_pdbqt)
            
            # For additional files with the same UniProt ID, check if they're identical
            elif file_hash != representative_hash:
                logging.warning(f"Different receptor structure found for {uniprot_id}: {pdb_path}")
                logging.warning("This suggests multiple conformations. Converting separately.")
                
                # Generate unique PDBQT path for this variant
                pdb_filename = Path(pdb_path).stem
                variant_pdbqt = os.path.join(output_dir, f"{uniprot_id}", f"{pdb_filename}.pdbqt")
                
                if not os.path.exists(variant_pdbqt):
                    conversion_tasks.append({
                        'pdb_path': pdb_path,
                        'pdbqt_path': variant_pdbqt,
                        'uniprot_id': uniprot_id,
                        'is_copy': False
                    })
            
            processed_files += 1
    
    logging.info(f"Found {len(unique_receptors)} unique UniProt IDs")
    logging.info(f"Prepared {len(conversion_tasks)} conversion tasks")
    
    return conversion_tasks, unique_receptors

def create_updated_mapping(cavity_df, unique_receptors, output_dir):
    """Create updated mapping CSV with PDBQT paths."""
    logging.info("Creating updated mapping with PDBQT paths...")
    
    updated_rows = []
    for _, row in cavity_df.iterrows():
        uniprot_id = row['UniProt_ID']
        receptor_pdb = row['Receptor_PDB']
        pocket_pdb = row['Pocket_PDB']
        
        # Find corresponding PDBQT file
        if uniprot_id in unique_receptors:
            # Check if this specific PDB file has a PDBQT counterpart
            pdb_filename = Path(receptor_pdb).stem
            potential_pdbqt = os.path.join(output_dir, f"{uniprot_id}", f"{pdb_filename}.pdbqt")
            
            if os.path.exists(potential_pdbqt):
                receptor_pdbqt = potential_pdbqt
            else:
                # Use the representative PDBQT
                _, _, representative_pdbqt = unique_receptors[uniprot_id]
                receptor_pdbqt = representative_pdbqt
        else:
            receptor_pdbqt = None
        
        updated_rows.append({
            'UniProt_ID': uniprot_id,
            'Cavity_Index': row.get('Cavity_Index', 1),
            'Receptor_PDB': receptor_pdb,
            'Receptor_PDBQT': receptor_pdbqt,
            'Pocket_PDB': pocket_pdb
        })
    
    updated_df = pd.DataFrame(updated_rows)
    return updated_df

def main():
    """Main function for PDB to PDBQT conversion."""
    start_time = time.time()
    
    logging.info("Starting PDB to PDBQT conversion workflow")
    logging.info(f"Input: {CAVITY_MAPPING_CSV}")
    logging.info(f"Output directory: {OUTPUT_PDBQT_DIR}")
    logging.info(f"Output mapping: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Number of workers: {NUM_THREADS}")
    logging.info(f"Using processes: {USE_PROCESSES}")
    logging.info(f"Conversion timeout: {CONVERSION_TIMEOUT}s")
    logging.info(f"pH value: {PH_VALUE}")
    logging.info(f"CPU count: {CPU_COUNT}")
    logging.info("Note: Now using ProcessPoolExecutor for better OpenBabel compatibility")
    
    # Create output directory
    os.makedirs(OUTPUT_PDBQT_DIR, exist_ok=True)
    
    # Load cavity mapping (prioritizes fixed PDB mapping if available)
    cavity_df, source_file = load_best_cavity_mapping()
    logging.info(f"Loaded {len(cavity_df)} cavity mappings from {source_file}")
    
    # Prepare conversion tasks with deduplication
    conversion_tasks, unique_receptors = prepare_conversion_tasks(cavity_df, OUTPUT_PDBQT_DIR)
    
    if not conversion_tasks:
        logging.info("No conversion tasks needed. All PDBQT files already exist.")
    else:
        # Perform parallel conversion with timeout handling
        logging.info(f"Starting parallel conversion of {len(conversion_tasks)} files...")
        
        successful_conversions = 0
        failed_conversions = 0
        timeout_conversions = 0
        
        # Choose executor based on configuration
        executor_class = ProcessPoolExecutor  # Always use ProcessPoolExecutor for better OpenBabel compatibility
        executor_name = "processes"
        
        logging.info(f"Using {executor_name} for parallelization")
        
        print('num_workers: ', NUM_THREADS)
        with executor_class(max_workers=NUM_THREADS) as executor:
            # Submit all tasks
            future_to_task = {executor.submit(process_conversion, task): task for task in conversion_tasks}
            
            # Track unique processes used
            unique_processes = set()
            
            # Process results with progress bar and timeout
            for future in tqdm(as_completed(future_to_task), total=len(conversion_tasks), 
                             desc="Converting PDB to PDBQT"):
                try:
                    # Wait for result with timeout
                    result = future.result(timeout=CONVERSION_TIMEOUT)
                    
                    # Track process usage
                    if 'process_id' in result:
                        unique_processes.add(result['process_id'])
                    
                    if result['success']:
                        successful_conversions += 1
                        if result['action'] == 'converted':
                            logging.debug(f"Converted: {result['pdb_path']} -> {result['pdbqt_path']}")
                        elif result['action'] == 'copied':
                            logging.debug(f"Copied: {result['pdbqt_path']}")
                    else:
                        failed_conversions += 1
                        logging.error(f"Failed to convert {result['pdb_path']}: {result['error']}")
                        
                except TimeoutError:
                    timeout_conversions += 1
                    task = future_to_task[future]
                    logging.error(f"Timeout converting {task['pdb_path']} (>{CONVERSION_TIMEOUT}s)")
                    future.cancel()
                except Exception as e:
                    failed_conversions += 1
                    task = future_to_task[future]
                    logging.error(f"Exception converting {task['pdb_path']}: {e}")
        
        # Log process usage statistics
        logging.info(f"Used {len(unique_processes)} unique processes out of {NUM_THREADS} configured")
        if len(unique_processes) == 1:
            logging.warning("Only 1 process was used! Parallelization may not be working correctly.")
        else:
            logging.info(f"✓ Parallel execution working with {len(unique_processes)} processes")
        
        logging.info(f"Conversion completed: {successful_conversions} successful, {failed_conversions} failed, {timeout_conversions} timeout")
    
    # Create updated mapping CSV
    updated_df = create_updated_mapping(cavity_df, unique_receptors, OUTPUT_PDBQT_DIR)
    updated_df.to_csv(OUTPUT_MAPPING_CSV, index=False)
    
    # Summary statistics
    total_pdbqt_files = len([f for f in Path(OUTPUT_PDBQT_DIR).rglob("*.pdbqt")])
    
    elapsed_time = time.time() - start_time
    
    logging.info(f"\n--- Conversion Summary ---")
    logging.info(f"Total cavity mappings processed: {len(cavity_df)}")
    logging.info(f"Unique UniProt IDs: {len(unique_receptors)}")
    logging.info(f"PDBQT files created/found: {total_pdbqt_files}")
    logging.info(f"Updated mapping saved to: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Total processing time: {elapsed_time:.2f} seconds")
    
    # Verify the output
    missing_pdbqt = updated_df[updated_df['Receptor_PDBQT'].isna()]
    if len(missing_pdbqt) > 0:
        logging.warning(f"Warning: {len(missing_pdbqt)} entries missing PDBQT files")
    else:
        logging.info("All entries have corresponding PDBQT files!")

if __name__ == "__main__":
    main()
