#!/usr/bin/env python3
"""
PDB to PDBQT Conversion Script
This script converts receptor PDB files to PDBQT format with parallel processing.

Features:
- Parallel processing using multiple threads
- Deduplication: Same receptor (UniProt ID) only converted once
- Resume capability: Skips already converted files
- Progress tracking and detailed logging

Prerequisites:
- fixed_mapping.csv (preferred, generated by fix_required_pdbs.py)
- alphafold_mapping.csv (alternative, generated by extract_alphafold_models.py)
- cavity_mapping.csv (fallback, generated by extract_cavities.py)
- openbabel/pybel installed

Usage:
    python convert_pdb_to_pdbqt.py

Output:
    - converted_pdbqt/ folder with PDBQT files
    - pdbqt_mapping.csv: Updated mapping with PDBQT paths
    - pdb_to_pdbqt_conversion.log
"""

import os
import sys
import pandas as pd
import logging
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import hashlib
import shutil

try:
    from openbabel import pybel, openbabel
except ImportError:
    logging.error("OpenBabel/pybel not found. Please install openbabel-python.")
    sys.exit(1)

# --- Configuration ---
FIXED_MAPPING_CSV = "fixed_mapping.csv"      # Preferred input from fix_required_pdbs.py
ALPHAFOLD_MAPPING_CSV = "alphafold_mapping.csv"  # Alternative input from extract_alphafold_models.py
CAVITY_MAPPING_CSV = "cavity_mapping.csv"    # Fallback input from extract_cavities.py
OUTPUT_PDBQT_DIR = "converted_pdbqt"         # Output directory for PDBQT files
OUTPUT_MAPPING_CSV = "pdbqt_mapping.csv"     # Output mapping with PDBQT paths
LOG_FILE = "pdb_to_pdbqt_conversion.log"
NUM_THREADS = 8  # Adjust based on your CPU cores
PH_VALUE = 7.4   # Protonation pH

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

def get_file_hash(file_path):
    """Calculate MD5 hash of a file to check for duplicates."""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def pdb_to_pdbqt(pdb_path, pdbqt_path, pH=7.4):
    """
    Convert a PDB file to a PDBQT file needed by docking programs of the AutoDock family.
    
    Parameters
    ----------
    pdb_path: str or pathlib.Path
        Path to input PDB file.
    pdbqt_path: str or pathlib.Path
        Path to output PDBQT file.
    pH: float
        Protonation at given pH.
        
    Returns
    -------
    bool: True if conversion successful, False otherwise
    """
    try:
        molecule = list(pybel.readfile("pdb", str(pdb_path)))[0]
        # add hydrogens at given pH
        #molecule.OBMol.CorrectForPH(pH)
        #molecule.addh()
        # add partial charges to each atom
        for atom in molecule.atoms:
            atom.OBAtom.GetPartialCharge()
        molecule.write("pdbqt", str(pdbqt_path), overwrite=True)
        return True
    except Exception as e:
        logging.error(f"Error converting {pdb_path} to PDBQT: {e}")
        return False

def process_conversion(conversion_task):
    """
    Process a single PDB to PDBQT conversion task.
    
    Parameters
    ----------
    conversion_task: dict
        Dictionary containing conversion information
        
    Returns
    -------
    dict: Result of the conversion
    """
    pdb_path = conversion_task['pdb_path']
    pdbqt_path = conversion_task['pdbqt_path']
    uniprot_id = conversion_task['uniprot_id']
    is_copy = conversion_task.get('is_copy', False)
    source_pdbqt = conversion_task.get('source_pdbqt', None)
    
    try:
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(pdbqt_path), exist_ok=True)
        
        if is_copy and source_pdbqt:
            # Copy existing PDBQT file
            shutil.copy2(source_pdbqt, pdbqt_path)
            return {
                'success': True,
                'pdb_path': pdb_path,
                'pdbqt_path': pdbqt_path,
                'uniprot_id': uniprot_id,
                'action': 'copied',
                'error': None
            }
        else:
            # Convert PDB to PDBQT
            success = pdb_to_pdbqt(pdb_path, pdbqt_path, PH_VALUE)
            return {
                'success': success,
                'pdb_path': pdb_path,
                'pdbqt_path': pdbqt_path,
                'uniprot_id': uniprot_id,
                'action': 'converted',
                'error': None if success else 'Conversion failed'
            }
    except Exception as e:
        return {
            'success': False,
            'pdb_path': pdb_path,
            'pdbqt_path': pdbqt_path,
            'uniprot_id': uniprot_id,
            'action': 'failed',
            'error': str(e)
        }

def load_cavity_mapping(mapping_file):
    """Load cavity mapping from CSV file."""
    if not os.path.exists(mapping_file):
        logging.error(f"Cavity mapping file not found: {mapping_file}")
        logging.error("Please run extract_cavities.py first.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['UniProt_ID', 'Receptor_PDB', 'Pocket_PDB']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.error(f"Required columns missing from cavity mapping: {missing_columns}")
            sys.exit(1)
        
        logging.info(f"Loaded {len(df)} cavity mappings from {mapping_file}")
        return df
        
    except Exception as e:
        logging.error(f"Error loading cavity mapping: {e}")
        sys.exit(1)

def load_best_cavity_mapping():
    """
    Load the best available cavity mapping file.
    Prioritizes AlphaFold mapping over original cavity mapping.
    
    Returns
    -------
    pd.DataFrame: Cavity mapping dataframe
    str: Source file used
    """
    # Check for fixed mapping first (preferred)
    if os.path.exists(FIXED_MAPPING_CSV):
        logging.info(f"Using fixed mapping: {FIXED_MAPPING_CSV}")
        df = load_cavity_mapping(FIXED_MAPPING_CSV)
        return df, FIXED_MAPPING_CSV
    
    # Check for AlphaFold mapping second
    elif os.path.exists(ALPHAFOLD_MAPPING_CSV):
        logging.info(f"Using AlphaFold mapping: {ALPHAFOLD_MAPPING_CSV}")
        df = load_cavity_mapping(ALPHAFOLD_MAPPING_CSV)
        return df, ALPHAFOLD_MAPPING_CSV
    
    # Fall back to original cavity mapping
    elif os.path.exists(CAVITY_MAPPING_CSV):
        logging.info(f"Using original cavity mapping: {CAVITY_MAPPING_CSV}")
        logging.info("Consider running fix_required_pdbs.py first for fixed structures with hydrogens")
        df = load_cavity_mapping(CAVITY_MAPPING_CSV)
        return df, CAVITY_MAPPING_CSV
    
    else:
        logging.error("No cavity mapping file found!")
        logging.error("Please run extract_cavities.py (and optionally fix_required_pdbs.py) first.")
        sys.exit(1)

def prepare_conversion_tasks(cavity_df, output_dir):
    """
    Prepare conversion tasks with deduplication.
    
    Returns
    -------
    list: List of conversion tasks
    dict: Mapping of UniProt IDs to representative PDBQT files
    """
    logging.info("Analyzing PDB files for deduplication...")
    
    # Group by UniProt ID and find unique receptor files
    uniprot_groups = cavity_df.groupby('UniProt_ID')
    unique_receptors = {}  # uniprot_id -> (pdb_path, pdb_hash, pdbqt_path)
    conversion_tasks = []
    
    total_files = len(cavity_df)
    processed_files = 0
    
    for uniprot_id, group in tqdm(uniprot_groups, desc="Analyzing receptors"):
        receptor_files = group['Receptor_PDB'].unique()
        
        # Find the representative file (first unique one)
        representative_pdb = None
        representative_hash = None
        representative_pdbqt = None
        
        for pdb_path in receptor_files:
            if not os.path.exists(pdb_path):
                logging.warning(f"PDB file not found: {pdb_path}")
                continue
            
            # Calculate hash to check if this is a unique structure
            file_hash = get_file_hash(pdb_path)
            
            if representative_pdb is None:
                # This is our representative file
                representative_pdb = pdb_path
                representative_hash = file_hash
                
                # Generate PDBQT path
                pdb_filename = Path(pdb_path).stem
                representative_pdbqt = os.path.join(output_dir, f"{uniprot_id}", f"{pdb_filename}.pdbqt")
                
                # Check if PDBQT already exists
                if os.path.exists(representative_pdbqt):
                    logging.info(f"PDBQT already exists for {uniprot_id}: {representative_pdbqt}")
                else:
                    # Add conversion task
                    conversion_tasks.append({
                        'pdb_path': representative_pdb,
                        'pdbqt_path': representative_pdbqt,
                        'uniprot_id': uniprot_id,
                        'is_copy': False
                    })
                
                unique_receptors[uniprot_id] = (representative_pdb, representative_hash, representative_pdbqt)
            
            # For additional files with the same UniProt ID, check if they're identical
            elif file_hash != representative_hash:
                logging.warning(f"Different receptor structure found for {uniprot_id}: {pdb_path}")
                logging.warning("This suggests multiple conformations. Converting separately.")
                
                # Generate unique PDBQT path for this variant
                pdb_filename = Path(pdb_path).stem
                variant_pdbqt = os.path.join(output_dir, f"{uniprot_id}", f"{pdb_filename}.pdbqt")
                
                if not os.path.exists(variant_pdbqt):
                    conversion_tasks.append({
                        'pdb_path': pdb_path,
                        'pdbqt_path': variant_pdbqt,
                        'uniprot_id': uniprot_id,
                        'is_copy': False
                    })
            
            processed_files += 1
    
    logging.info(f"Found {len(unique_receptors)} unique UniProt IDs")
    logging.info(f"Prepared {len(conversion_tasks)} conversion tasks")
    
    return conversion_tasks, unique_receptors

def create_updated_mapping(cavity_df, unique_receptors, output_dir):
    """Create updated mapping CSV with PDBQT paths."""
    logging.info("Creating updated mapping with PDBQT paths...")
    
    updated_rows = []
    for _, row in cavity_df.iterrows():
        uniprot_id = row['UniProt_ID']
        receptor_pdb = row['Receptor_PDB']
        pocket_pdb = row['Pocket_PDB']
        
        # Find corresponding PDBQT file
        if uniprot_id in unique_receptors:
            # Check if this specific PDB file has a PDBQT counterpart
            pdb_filename = Path(receptor_pdb).stem
            potential_pdbqt = os.path.join(output_dir, f"{uniprot_id}", f"{pdb_filename}.pdbqt")
            
            if os.path.exists(potential_pdbqt):
                receptor_pdbqt = potential_pdbqt
            else:
                # Use the representative PDBQT
                _, _, representative_pdbqt = unique_receptors[uniprot_id]
                receptor_pdbqt = representative_pdbqt
        else:
            receptor_pdbqt = None
        
        updated_rows.append({
            'UniProt_ID': uniprot_id,
            'Cavity_Index': row.get('Cavity_Index', 1),
            'Receptor_PDB': receptor_pdb,
            'Receptor_PDBQT': receptor_pdbqt,
            'Pocket_PDB': pocket_pdb
        })
    
    updated_df = pd.DataFrame(updated_rows)
    return updated_df

def main():
    """Main function for PDB to PDBQT conversion."""
    start_time = time.time()
    
    logging.info("Starting PDB to PDBQT conversion workflow")
    logging.info(f"Input: {CAVITY_MAPPING_CSV}")
    logging.info(f"Output directory: {OUTPUT_PDBQT_DIR}")
    logging.info(f"Output mapping: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Number of threads: {NUM_THREADS}")
    logging.info(f"pH value: {PH_VALUE}")
    
    # Create output directory
    os.makedirs(OUTPUT_PDBQT_DIR, exist_ok=True)
    
    # Load cavity mapping (prioritizes fixed PDB mapping if available)
    cavity_df, source_file = load_best_cavity_mapping()
    logging.info(f"Loaded {len(cavity_df)} cavity mappings from {source_file}")
    
    # Prepare conversion tasks with deduplication
    conversion_tasks, unique_receptors = prepare_conversion_tasks(cavity_df, OUTPUT_PDBQT_DIR)
    
    if not conversion_tasks:
        logging.info("No conversion tasks needed. All PDBQT files already exist.")
    else:
        # Perform parallel conversion
        logging.info(f"Starting parallel conversion of {len(conversion_tasks)} files...")
        
        successful_conversions = 0
        failed_conversions = 0
        
        with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
            # Submit all tasks
            future_to_task = {executor.submit(process_conversion, task): task for task in conversion_tasks}
            
            # Process results with progress bar
            for future in tqdm(as_completed(future_to_task), total=len(conversion_tasks), 
                             desc="Converting PDB to PDBQT"):
                result = future.result()
                
                if result['success']:
                    successful_conversions += 1
                    if result['action'] == 'converted':
                        logging.debug(f"Converted: {result['pdb_path']} -> {result['pdbqt_path']}")
                    elif result['action'] == 'copied':
                        logging.debug(f"Copied: {result['pdbqt_path']}")
                else:
                    failed_conversions += 1
                    logging.error(f"Failed to convert {result['pdb_path']}: {result['error']}")
        
        logging.info(f"Conversion completed: {successful_conversions} successful, {failed_conversions} failed")
    
    # Create updated mapping CSV
    updated_df = create_updated_mapping(cavity_df, unique_receptors, OUTPUT_PDBQT_DIR)
    updated_df.to_csv(OUTPUT_MAPPING_CSV, index=False)
    
    # Summary statistics
    total_pdbqt_files = len([f for f in Path(OUTPUT_PDBQT_DIR).rglob("*.pdbqt")])
    
    elapsed_time = time.time() - start_time
    
    logging.info(f"\n--- Conversion Summary ---")
    logging.info(f"Total cavity mappings processed: {len(cavity_df)}")
    logging.info(f"Unique UniProt IDs: {len(unique_receptors)}")
    logging.info(f"PDBQT files created/found: {total_pdbqt_files}")
    logging.info(f"Updated mapping saved to: {OUTPUT_MAPPING_CSV}")
    logging.info(f"Total processing time: {elapsed_time:.2f} seconds")
    
    # Verify the output
    missing_pdbqt = updated_df[updated_df['Receptor_PDBQT'].isna()]
    if len(missing_pdbqt) > 0:
        logging.warning(f"Warning: {len(missing_pdbqt)} entries missing PDBQT files")
    else:
        logging.info("All entries have corresponding PDBQT files!")

if __name__ == "__main__":
    main()
