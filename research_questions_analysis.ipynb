{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97cbded5",
   "metadata": {},
   "source": [
    "# Consensus Docking Research Questions Analysis\n",
    "\n",
    "This notebook addresses key research questions about docking tool reliability, cavity preferences, and sample discrimination using the prepared consensus docking dataset.\n",
    "\n",
    "## üìã Research Questions\n",
    "\n",
    "1. **RQ1: Tool Reliability & Consensus** - How reliable is consensus between different docking tools?\n",
    "2. **RQ2: Cavity Preferences** - Can we identify preferred vs non-preferred binding cavities?\n",
    "3. **RQ3: Cluster Separation** - Do cavity clusters separate preferred from non-preferred cavities?\n",
    "4. **RQ4: Sample Discrimination** - Can scoring metrics separate positive from negative samples?\n",
    "\n",
    "## üöÄ Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have completed data preparation:\n",
    "1. Run `prepare_data_for_analysis.ipynb`\n",
    "2. This will create: `combined_filtered_annotated_docking_results.parquet`\n",
    "\n",
    "## üìä Analysis Workflow\n",
    "\n",
    "1. **Data Loading** - Load the prepared dataset\n",
    "2. **RQ1 Analysis** - Tool consensus and reliability\n",
    "3. **RQ2 Analysis** - Preferred cavity identification\n",
    "4. **RQ3 Analysis** - Cluster-based separation\n",
    "5. **RQ4 Analysis** - Positive vs negative discrimination\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a38726",
   "metadata": {},
   "source": [
    "## üì• Step 1: Load Prepared Data\n",
    "\n",
    "Load the filtered and annotated consensus docking dataset that was prepared in the data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üì• LOAD PREPARED CONSENSUS DOCKING DATA\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Configuration\n",
    "DATA_FILE = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/combined_filtered_annotated_docking_results.parquet\"\n",
    "\n",
    "print(\"üîç Loading prepared consensus docking data...\")\n",
    "print(f\"üìÅ File: {DATA_FILE}\")\n",
    "\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(\"\\n‚ùå Error: Prepared data file not found!\")\n",
    "    print(\"   Please run the data preparation pipeline first:\")\n",
    "    print(\"   - Notebook: data_preparation_pipeline.ipynb\")\n",
    "    print(\"   - This will create the required parquet file\")\n",
    "    combined_results = pl.DataFrame()\n",
    "else:\n",
    "    # Load the data\n",
    "    combined_results = pl.read_parquet(DATA_FILE)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "    print(f\"   Shape: {combined_results.shape}\")\n",
    "    print(f\"   Rows: {combined_results.height:,}\")\n",
    "    print(f\"   Columns: {combined_results.width}\")\n",
    "    print(f\"   Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    # Verify required columns\n",
    "    required_cols = [\n",
    "        'drugbank_id', 'uniprot_id', 'cavity_index',\n",
    "        'RMSD', 'Score1', 'Score2', 'LeDock_Score',\n",
    "        'Tool1', 'Tool2', 'sample_type', 'cavity_cluster_id'\n",
    "    ]\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in combined_results.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Missing expected columns: {missing_cols}\")\n",
    "        print(\"   Some analyses may not work correctly.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All required columns present\")\n",
    "    \n",
    "    # Show sample type distribution\n",
    "    if 'sample_type' in combined_results.columns:\n",
    "        sample_counts = combined_results.group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "        print(f\"\\nüìä Sample Type Distribution:\")\n",
    "        for row in sample_counts.iter_rows(named=True):\n",
    "            print(f\"   {row['sample_type']}: {row['len']:,}\")\n",
    "    \n",
    "    # Show tool distribution\n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        tools = set(combined_results['Tool1'].unique().to_list() + combined_results['Tool2'].unique().to_list())\n",
    "        tools = [t for t in tools if t is not None]\n",
    "        print(f\"\\nüîß Tools in dataset: {sorted(tools)}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Dataset ready for research question analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced cluster distribution analysis\n",
    "if 'cavity_cluster_id' in combined_results.columns:\n",
    "    print(\"üìä CLUSTER DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Count mapped vs unmapped\n",
    "    mapped = combined_results.filter(pl.col('cavity_cluster_id').is_not_null()).height\n",
    "    unmapped = combined_results.filter(pl.col('cavity_cluster_id').is_null()).height\n",
    "    total = combined_results.height\n",
    "    \n",
    "    print(f\"\\nüìà Overall Coverage:\")\n",
    "    print(f\"   Mapped to clusters: {mapped:,} ({mapped/total*100:.1f}%)\")\n",
    "    print(f\"   Not in cluster DB: {unmapped:,} ({unmapped/total*100:.1f}%)\")\n",
    "    print(f\"   Total entries: {total:,}\")\n",
    "    \n",
    "    # Show top clusters (excluding None)\n",
    "    cluster_counts = combined_results.filter(\n",
    "        pl.col('cavity_cluster_id').is_not_null()\n",
    "    ).group_by('cavity_cluster_id').agg(pl.len()).sort('len', descending=True)\n",
    "    \n",
    "    if cluster_counts.height > 0:\n",
    "        print(f\"\\nüèÜ Top 10 Most Populated Clusters (excluding unmapped):\")\n",
    "        for i, row in enumerate(cluster_counts.head(10).iter_rows(named=True), 1):\n",
    "            print(f\"   {i}. Cluster {row['cavity_cluster_id']}: {row['len']:,} entries\")\n",
    "        \n",
    "        print(f\"\\nüìä Cluster Statistics:\")\n",
    "        print(f\"   Total unique clusters: {cluster_counts.height:,}\")\n",
    "        print(f\"   Mean entries per cluster: {cluster_counts['len'].mean():.0f}\")\n",
    "        print(f\"   Median entries per cluster: {cluster_counts['len'].median():.0f}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No cluster assignments found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e27ab",
   "metadata": {},
   "source": [
    "## Research Question 1 - Tool Reliability & Consensus Analysis\n",
    "\n",
    "**Research Question 1:** *How reliable is consensus between different docking tools?*\n",
    "\n",
    "This analysis addresses one of the most fundamental questions in computational drug discovery: **When can we trust docking predictions?** \n",
    "\n",
    "**Note:** All analysis now uses the filtered dataset with complete tool coverage, ensuring fair comparisons between tools.\n",
    "\n",
    "### üéØ Analysis Goals:\n",
    "1. **Quantify agreement** between different docking tools (Gold, Smina, LeDock)\n",
    "2. **Identify tool pairs** that show the best/worst consensus\n",
    "3. **Understand when** docking predictions are most trustworthy\n",
    "4. **Establish quality thresholds** for reliable predictions\n",
    "\n",
    "### üìä Key Metrics:\n",
    "- **RMSD between tools** - Lower values indicate better pose agreement\n",
    "- **Tool agreement frequency** - How often tools produce similar results\n",
    "- **Score-RMSD correlation** - Relationship between structural and scoring agreement\n",
    "\n",
    "### üìà Main Visualization:\n",
    "- **Tool Agreement Distribution Plot** - Comprehensive analysis combining RMSD distributions and score patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üõ†Ô∏è SETUP: Import Libraries and Prepare Data for Tool Reliability Analysis\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "\n",
    "# Check data availability for tool reliability analysis\n",
    "if not combined_results.is_empty():\n",
    "    print(f\"\\nüîç CHECKING FILTERED DATA SUITABILITY FOR TOOL RELIABILITY ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìã NOTE: Analysis uses filtered data with complete tool coverage\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_columns = ['Tool1', 'Tool2', 'Score1', 'Score2', 'RMSD']\n",
    "    available_columns = [col for col in required_columns if col in combined_results.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_results.columns]\n",
    "    \n",
    "    print(f\"‚úÖ Available columns: {available_columns}\")\n",
    "    if missing_columns:\n",
    "        print(f\"‚ö†Ô∏è  Missing columns: {missing_columns}\")\n",
    "        print(\"   Analysis will be adapted based on available data\")\n",
    "    \n",
    "    # Check tool information\n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        tool1_types = combined_results['Tool1'].unique().to_list()\n",
    "        tool2_types = combined_results['Tool2'].unique().to_list()\n",
    "        all_tools = list(set(tool1_types + tool2_types))\n",
    "        \n",
    "        print(f\"\\nüîß Detected docking tools: {all_tools}\")\n",
    "        print(f\"   Tool1 variants: {tool1_types}\")\n",
    "        print(f\"   Tool2 variants: {tool2_types}\")\n",
    "        \n",
    "        # Check for RMSD data\n",
    "        if 'RMSD' in combined_results.columns:\n",
    "            rmsd_stats = combined_results.select([\n",
    "                pl.col('RMSD').count().alias('total_comparisons'),\n",
    "                pl.col('RMSD').mean().alias('mean_rmsd'),\n",
    "                pl.col('RMSD').std().alias('std_rmsd'),\n",
    "                pl.col('RMSD').min().alias('min_rmsd'),\n",
    "                pl.col('RMSD').max().alias('max_rmsd')\n",
    "            ]).to_pandas().iloc[0]\n",
    "            \n",
    "            print(f\"\\nüìê RMSD Statistics (Tool Agreement Metric):\")\n",
    "            print(f\"   Total pairwise comparisons: {rmsd_stats['total_comparisons']:,}\")\n",
    "            print(f\"   Mean RMSD: {rmsd_stats['mean_rmsd']:.2f} ¬± {rmsd_stats['std_rmsd']:.2f} √Ö\")\n",
    "            print(f\"   Range: {rmsd_stats['min_rmsd']:.2f} - {rmsd_stats['max_rmsd']:.2f} √Ö\")\n",
    "            \n",
    "            # Calculate agreement categories\n",
    "            excellent_agreement = combined_results.filter(pl.col('RMSD') <= 1.0).height\n",
    "            good_agreement = combined_results.filter((pl.col('RMSD') > 1.0) & (pl.col('RMSD') <= 2.0)).height\n",
    "            poor_agreement = combined_results.filter(pl.col('RMSD') > 2.0).height\n",
    "            total_comparisons = combined_results.height\n",
    "            \n",
    "            print(f\"\\nüéØ Tool Agreement Categories:\")\n",
    "            print(f\"   Excellent (RMSD ‚â§ 1.0 √Ö): {excellent_agreement:,} ({excellent_agreement/total_comparisons*100:.1f}%)\")\n",
    "            print(f\"   Good (1.0 < RMSD ‚â§ 2.0 √Ö): {good_agreement:,} ({good_agreement/total_comparisons*100:.1f}%)\")\n",
    "            print(f\"   Poor (RMSD > 2.0 √Ö): {poor_agreement:,} ({poor_agreement/total_comparisons*100:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ Filtered data is suitable for tool reliability analysis!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  No RMSD data available - analysis will be limited to score comparisons\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Missing tool information - cannot perform tool reliability analysis\")\n",
    "        print(\"   Please ensure the dataset contains Tool1 and Tool2 columns\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")\n",
    "    print(\"   Please run Steps 1-3 first to load, filter, and prepare the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543d9ce",
   "metadata": {},
   "source": [
    "### üìä Tool Agreement & Score Distribution Analysis\n",
    "\n",
    "**Purpose:** Comprehensive analysis of tool agreement and scoring behavior addressing Research Question 1.\n",
    "\n",
    "**Layout Structure:**\n",
    "1. **Top Plot:** RMSD distribution violin plots showing tool agreement quality\n",
    "2. **Three Ridge Plots:** One subplot per tool (GOLD, LeDock, Smina) showing score distributions across tool pairs and RMSD ranges\n",
    "\n",
    "**Analysis Logic:**\n",
    "- **Best Agreement Focus:** For each drug-target pair, only the cavity/pose with the lowest RMSD between tools is considered\n",
    "- **Unique Tool Pairs:** Avoids counting A-B and B-A as separate comparisons\n",
    "- **Tool-Specific Scoring:** Each tool's scores are analyzed separately (never mixed) since each has its own scoring function\n",
    "\n",
    "**Ridge Plot Features:**\n",
    "- **Color-blind friendly palette:** Orange (GOLD), Sky Blue (LeDock), Bluish Green (Smina)\n",
    "- **RMSD Stratification:** Scores grouped by Good (<2√Ö), Medium (2-5√Ö), Poor (>5√Ö) RMSD ranges\n",
    "- **Fixed Scales:** GOLD (0-100), LeDock/Smina (-12 to 5) for consistent comparison\n",
    "- **Clean Design:** No annotations for cleaner, poster-ready appearance\n",
    "\n",
    "**Scientific Rationale:**\n",
    "- **No Score Mixing:** GOLD, Smina, and LeDock each use different scoring functions, so their scores should never be directly compared or combined\n",
    "- **Tool-Specific Insights:** Shows how each tool's scoring function relates to structural agreement quality\n",
    "- **Fair Comparison:** Enables assessment of whether good/poor RMSD agreement corresponds to favorable/unfavorable scores for each individual tool\n",
    "\n",
    "**Interpretation:**\n",
    "- **Agreement Quality:** Lower, narrower RMSD distributions indicate better and more consistent tool agreement\n",
    "- **Score-Quality Relationship:** For each tool, check if good RMSD ranges show more favorable scores than poor ranges\n",
    "- **Tool Behavior:** Compare how different tools' scoring functions relate to structural agreement\n",
    "\n",
    "**Poster-Ready Features:** Color-blind friendly palette, clear borders, proper axis labels with units, and optimized layout for maximum presentation impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä PLOT 1A: TOOL AGREEMENT DISTRIBUTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty() and 'RMSD' in combined_results.columns and 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "    \n",
    "    print(\"üî• Generating Tool Agreement Distribution Analysis...\")\n",
    "    \n",
    "    # Set up poster-quality plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (16, 20),      # Vertical layout - taller figure\n",
    "        'font.size': 18,                 # Even larger base font size\n",
    "        'axes.titlesize': 22,            # Even larger title font\n",
    "        'axes.labelsize': 20,            # Even larger axis labels\n",
    "        'xtick.labelsize': 16,           # Even larger tick labels\n",
    "        'ytick.labelsize': 16,           # Even larger tick labels\n",
    "        'legend.fontsize': 16,           # Even larger legend font\n",
    "        'figure.titlesize': 26           # Even larger figure title\n",
    "    })\n",
    "    \n",
    "    # Convert to pandas for easier manipulation\n",
    "    df = combined_results.to_pandas()\n",
    "    \n",
    "    # Get all unique tools\n",
    "    all_tools = sorted(list(set(df['Tool1'].unique().tolist() + df['Tool2'].unique().tolist())))\n",
    "    print(f\"üîß Detected tools: {all_tools}\")\n",
    "    \n",
    "    # Check for score columns\n",
    "    score_columns = [col for col in df.columns if any(x in col.lower() for x in ['score', 'energy', 'affinity'])]\n",
    "    print(f\"üìä Available scoring columns: {score_columns}\")\n",
    "    \n",
    "    # Collect RMSD distributions and scores for unique tool pairs\n",
    "    print(\"üìä Computing tool agreement distributions using lowest RMSD per drug-target pair...\")\n",
    "    \n",
    "    tool_pair_data = []\n",
    "    \n",
    "    # Only consider unique pairs (avoid duplicates like A-B and B-A)\n",
    "    from itertools import combinations\n",
    "    unique_tool_pairs = list(combinations(all_tools, 2))\n",
    "    \n",
    "    for tool1, tool2 in unique_tool_pairs:\n",
    "        # Find all comparisons between these two tools (both directions)\n",
    "        mask1 = (df['Tool1'] == tool1) & (df['Tool2'] == tool2)\n",
    "        mask2 = (df['Tool1'] == tool2) & (df['Tool2'] == tool1)\n",
    "        \n",
    "        if mask1.any() or mask2.any():\n",
    "            # We need to track which tool is which for score assignment\n",
    "            # Include score information and tool identity\n",
    "            columns_to_include = ['drugbank_id', 'uniprot_id', 'cavity_index', 'RMSD', 'Tool1', 'Tool2'] + score_columns[:2]\n",
    "            \n",
    "            # Separate the two directions to maintain tool identity\n",
    "            comparisons_direction1 = df.loc[mask1, columns_to_include].copy() if mask1.any() else pd.DataFrame()\n",
    "            comparisons_direction2 = df.loc[mask2, columns_to_include].copy() if mask2.any() else pd.DataFrame()\n",
    "            \n",
    "            # For direction 2, we need to swap the tools and scores to maintain consistency\n",
    "            if not comparisons_direction2.empty:\n",
    "                # Create a copy and swap Tool1/Tool2 and Score1/Score2\n",
    "                comparisons_direction2_swapped = comparisons_direction2.copy()\n",
    "                comparisons_direction2_swapped['Tool1'], comparisons_direction2_swapped['Tool2'] = comparisons_direction2['Tool2'], comparisons_direction2['Tool1']\n",
    "                if 'Score1' in comparisons_direction2_swapped.columns and 'Score2' in comparisons_direction2_swapped.columns:\n",
    "                    comparisons_direction2_swapped['Score1'], comparisons_direction2_swapped['Score2'] = comparisons_direction2['Score2'], comparisons_direction2['Score1']\n",
    "                comparisons_direction2 = comparisons_direction2_swapped\n",
    "            \n",
    "            # Combine both directions\n",
    "            if not comparisons_direction1.empty and not comparisons_direction2.empty:\n",
    "                comparisons = pd.concat([comparisons_direction1, comparisons_direction2])\n",
    "            elif not comparisons_direction1.empty:\n",
    "                comparisons = comparisons_direction1\n",
    "            elif not comparisons_direction2.empty:\n",
    "                comparisons = comparisons_direction2\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Group by unique drug-target combinations and get the lowest RMSD for each\n",
    "            grouped = comparisons.groupby(['drugbank_id', 'uniprot_id'])\n",
    "            \n",
    "            best_agreements = []\n",
    "            for (drug, target), group in grouped:\n",
    "                best_idx = group['RMSD'].idxmin()\n",
    "                best_row = group.loc[best_idx]\n",
    "                \n",
    "                agreement_data = {\n",
    "                    'drug': drug,\n",
    "                    'target': target,\n",
    "                    'rmsd': best_row['RMSD'],\n",
    "                    'tool1_name': best_row['Tool1'],  # This should be tool1\n",
    "                    'tool2_name': best_row['Tool2'],  # This should be tool2\n",
    "                }\n",
    "                \n",
    "                # Add scores for each tool\n",
    "                if 'Score1' in best_row and pd.notna(best_row['Score1']):\n",
    "                    agreement_data['tool1_score'] = best_row['Score1']  # Score for tool1\n",
    "                if 'Score2' in best_row and pd.notna(best_row['Score2']):\n",
    "                    agreement_data['tool2_score'] = best_row['Score2']  # Score for tool2\n",
    "                \n",
    "                best_agreements.append(agreement_data)\n",
    "            \n",
    "            if len(best_agreements) > 0:\n",
    "                rmsds = [item['rmsd'] for item in best_agreements]\n",
    "                \n",
    "                tool_pair_data.append({\n",
    "                    'tool_pair': f\"{tool1} vs {tool2}\",\n",
    "                    'tool1': tool1,\n",
    "                    'tool2': tool2,\n",
    "                    'rmsds': np.array(rmsds),\n",
    "                    'agreements': best_agreements,\n",
    "                    'n_pairs': len(best_agreements),\n",
    "                    'mean_rmsd': np.mean(rmsds),\n",
    "                    'median_rmsd': np.median(rmsds),\n",
    "                    'std_rmsd': np.std(rmsds)\n",
    "                })\n",
    "                print(f\"   {tool1} vs {tool2}: {len(best_agreements):,} unique drug-target pairs, Mean: {np.mean(rmsds):.2f} √Ö\")\n",
    "    \n",
    "    if tool_pair_data:\n",
    "        # Create main figure with custom layout: top plot + 3 horizontal ridge plots\n",
    "        fig = plt.figure(figsize=(15, 25))\n",
    "        gs = fig.add_gridspec(2, 3, height_ratios=[1, 1.5], hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # ========================================================================\n",
    "        # PLOT 1: RMSD Distribution Violin Plots (Top - spans all columns)\n",
    "        # ========================================================================\n",
    "        ax_rmsd = fig.add_subplot(gs[0, :])\n",
    "        \n",
    "        tool_pair_names = [data['tool_pair'] for data in tool_pair_data]\n",
    "        rmsd_distributions = [data['rmsds'] for data in tool_pair_data]\n",
    "        \n",
    "        # Create violin plot with clear borders\n",
    "        violins = ax_rmsd.violinplot(rmsd_distributions, positions=range(len(tool_pair_names)), \n",
    "                                    showmeans=False, showmedians=False, showextrema=False)\n",
    "        \n",
    "        # Customize violin colors with clear borders\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(tool_pair_names)))\n",
    "        for i, violin in enumerate(violins['bodies']):\n",
    "            violin.set_facecolor(colors[i])\n",
    "            violin.set_alpha(0.7)\n",
    "            violin.set_edgecolor('black')  # Clear borders\n",
    "            violin.set_linewidth(2)       # Thick borders\n",
    "        \n",
    "        ax_rmsd.set_xticks(range(len(tool_pair_names)))\n",
    "        # Simplified x-axis labels - just the tool pair names\n",
    "        simplified_labels = [name.replace(' vs ', '\\nvs\\n') for name in tool_pair_names]\n",
    "        ax_rmsd.set_xticklabels(simplified_labels, rotation=0, ha='center', fontsize=16)\n",
    "        ax_rmsd.set_ylabel('RMSD (√Ö)', fontsize=20, fontweight='bold')  # Add y-axis label\n",
    "        ax_rmsd.grid(True, alpha=0.3)\n",
    "        ax_rmsd.set_ylim(0, min(15, max([data['rmsds'].max() for data in tool_pair_data])))\n",
    "        \n",
    "        # ========================================================================\n",
    "        # PLOTS 2-4: Ridge-style Score Distributions (Bottom - 3 horizontal plots)\n",
    "        # ========================================================================\n",
    "        \n",
    "        # Check if we have score data\n",
    "        has_score_data = any('tool1_score' in agreement or 'tool2_score' in agreement \n",
    "                           for data in tool_pair_data for agreement in data['agreements'])\n",
    "        \n",
    "        if has_score_data and len(tool_pair_data) > 0:\n",
    "            print(\"üìä Creating ridge-style score distribution analysis...\")\n",
    "            \n",
    "            # Define partner tool colors (color-blind friendly palette)\n",
    "            partner_tool_colors = {\n",
    "                'GOLD': '#E69F00',     # Orange (deuteranopia/protanopia safe)\n",
    "                'LeDock': '#56B4E9',   # Sky Blue (tritanopia safe)\n",
    "                'Smina': '#009E73'     # Bluish Green (all color-blind types safe)\n",
    "            }\n",
    "            \n",
    "            # Define RMSD ranges with actual numeric values\n",
    "            rmsd_ranges = [\n",
    "                (0, 2, \"<2√Ö\"),\n",
    "                (2, 5, \"2-5√Ö\"),\n",
    "                (5, float('inf'), \">5√Ö\")\n",
    "            ]\n",
    "            \n",
    "            # Create horizontal ridge plots for each tool\n",
    "            for tool_idx, tool in enumerate(all_tools):\n",
    "                if tool_idx > 2:  # Only use first 3 tools (we have 3 horizontal subplots)\n",
    "                    break\n",
    "                    \n",
    "                ax_tool = fig.add_subplot(gs[1, tool_idx])\n",
    "                \n",
    "                print(f\"   Creating ridge plot for {tool}...\")\n",
    "                \n",
    "                # Collect all score data for this tool across all tool pairs and RMSD ranges\n",
    "                y_position = 0\n",
    "                y_spacing = 0.8  # Overlapping ridges\n",
    "                max_y = 0\n",
    "                ridge_data = []  # Store ridge data for annotations\n",
    "                \n",
    "                # For each tool pair involving this tool\n",
    "                for pair_idx, data in enumerate(tool_pair_data):\n",
    "                    # Check if this tool is tool1 or tool2 in the pair\n",
    "                    if data['tool1'] == tool:\n",
    "                        partner_tool = data['tool2']\n",
    "                        score_key = 'tool1_score'\n",
    "                    elif data['tool2'] == tool:\n",
    "                        partner_tool = data['tool1']\n",
    "                        score_key = 'tool2_score'\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get partner tool color (use partner tool's color, not current tool's color)\n",
    "                    ridge_color = partner_tool_colors.get(partner_tool, '#ADD8E6')\n",
    "                    \n",
    "                    # For each RMSD range\n",
    "                    for range_idx, (rmsd_min, rmsd_max, range_name) in enumerate(rmsd_ranges):\n",
    "                        # Collect scores for this tool in this RMSD range\n",
    "                        tool_scores = []\n",
    "                        for agreement in data['agreements']:\n",
    "                            if (rmsd_min <= agreement['rmsd'] < rmsd_max and \n",
    "                                score_key in agreement and pd.notna(agreement[score_key])):\n",
    "                                tool_scores.append(agreement[score_key])\n",
    "                        \n",
    "                        if len(tool_scores) >= 5:  # Only plot if we have sufficient data\n",
    "                            try:\n",
    "                                from scipy.stats import gaussian_kde\n",
    "                                \n",
    "                                # Create density curve\n",
    "                                kde = gaussian_kde(tool_scores)\n",
    "                                score_range = np.linspace(min(tool_scores), max(tool_scores), 100)\n",
    "                                density = kde(score_range)\n",
    "                                \n",
    "                                # Normalize density for ridge plot appearance\n",
    "                                density = density / density.max() * 0.6  # Scale height\n",
    "                                \n",
    "                                # Plot the ridge with clear borders\n",
    "                                ax_tool.fill_between(score_range, y_position, y_position + density, \n",
    "                                                    alpha=0.8, color=ridge_color, \n",
    "                                                    edgecolor='black', linewidth=1.5)\n",
    "                                \n",
    "                                # Store ridge data for annotation (removed RMSD annotations)\n",
    "                                ridge_data.append({\n",
    "                                    'y_center': y_position + 0.3,\n",
    "                                    'x_center': np.median(tool_scores),\n",
    "                                    'partner_tool': partner_tool\n",
    "                                })\n",
    "                                \n",
    "                                y_position += y_spacing\n",
    "                                max_y = max(max_y, y_position)\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"Warning: Could not create KDE for {tool} vs {partner_tool} in {range_name} range: {e}\")\n",
    "                                continue\n",
    "                \n",
    "                # Customize the subplot - add x-axis label with units\n",
    "                ax_tool.set_ylabel('')  # Remove y-axis title\n",
    "                if tool in ['LeDock', 'Smina']:\n",
    "                    ax_tool.set_xlabel(f'{tool} Score (kcal/mol)', fontsize=18, fontweight='bold')  # Add units\n",
    "                else:  # GOLD has no unit\n",
    "                    ax_tool.set_xlabel(f'{tool} Score', fontsize=18, fontweight='bold')  # No unit for GOLD\n",
    "                ax_tool.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Set appropriate x-axis limits based on tool - extend LeDock/Smina to include positive values\n",
    "                if tool == 'GOLD':\n",
    "                    ax_tool.set_xlim(0, 100)\n",
    "                else:  # LeDock and Smina use negative scores, but extend to positive values up to 5\n",
    "                    ax_tool.set_xlim(-12, 5)\n",
    "                \n",
    "                # Set y-axis limits and remove ticks\n",
    "                if max_y > 0:\n",
    "                    ax_tool.set_ylim(-0.2, max_y + 0.2)\n",
    "                ax_tool.set_yticks([])\n",
    "                \n",
    "                # Remove RMSD range annotations (simplified for cleaner look)\n",
    "                # for ridge_info in ridge_data:\n",
    "                #     annotation_text = f\"{ridge_info['range_name']} vs {ridge_info['partner_tool']}\"\n",
    "                #     ax_tool.text(ridge_info['x_center'], ridge_info['y_center'], annotation_text,\n",
    "                #                ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                #                color=range_colors.get(ridge_info['range_name'], 'black'),\n",
    "                #                bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
    "                \n",
    "                if max_y == 0:  # No data was plotted\n",
    "                    ax_tool.text(0.5, 0.5, f'Insufficient score data\\nfor {tool}', \n",
    "                               ha='center', va='center', transform=ax_tool.transAxes, \n",
    "                               fontsize=16, fontweight='bold')\n",
    "        \n",
    "        else:\n",
    "            # No score data - show message in remaining subplots\n",
    "            for i in range(3):\n",
    "                ax_tool = fig.add_subplot(gs[1, i])\n",
    "                ax_tool.text(0.5, 0.5, 'No score data available', \n",
    "                           ha='center', va='center', transform=ax_tool.transAxes, \n",
    "                           fontsize=18, fontweight='bold')\n",
    "                # Add x-axis labels even when no data\n",
    "                if i < len(all_tools):\n",
    "                    if all_tools[i] in ['LeDock', 'Smina']:\n",
    "                        ax_tool.set_xlabel(f'{all_tools[i]} Score (kcal/mol)', fontsize=18, fontweight='bold')\n",
    "                    else:\n",
    "                        ax_tool.set_xlabel(f'{all_tools[i]} Score', fontsize=18, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout(pad=1.5)  # Add padding for better spacing\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed statistics - also update the printed ranges to use actual values\n",
    "        print(f\"\\nüìà SCORE DISTRIBUTION BY TOOL PAIR AND RMSD RANGE:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for data in tool_pair_data:\n",
    "            print(f\"\\nüîß {data['tool_pair']}:\")\n",
    "            \n",
    "            # Define RMSD ranges\n",
    "            rmsd_ranges = [\n",
    "                (0, 2, \"<2√Ö\"),\n",
    "                (2, 5, \"2-5√Ö\"),\n",
    "                (5, float('inf'), \">5√Ö\")\n",
    "            ]\n",
    "            \n",
    "            for i, (min_rmsd, max_rmsd, range_name) in enumerate(rmsd_ranges):\n",
    "                # Count scores for each tool in this range\n",
    "                tool1_scores = []\n",
    "                tool2_scores = []\n",
    "                \n",
    "                for agreement in data['agreements']:\n",
    "                    if min_rmsd <= agreement['rmsd'] < max_rmsd:\n",
    "                        if 'tool1_score' in agreement and pd.notna(agreement['tool1_score']):\n",
    "                            tool1_scores.append(agreement['tool1_score'])\n",
    "                        if 'tool2_score' in agreement and pd.notna(agreement['tool2_score']):\n",
    "                            tool2_scores.append(agreement['tool2_score'])\n",
    "                \n",
    "                print(f\"   {range_name}:\")\n",
    "                if tool1_scores:\n",
    "                    print(f\"     {data['tool1']}: n={len(tool1_scores)}, Mean={np.mean(tool1_scores):.2f}¬±{np.std(tool1_scores):.2f}\")\n",
    "                else:\n",
    "                    print(f\"     {data['tool1']}: No data\")\n",
    "                    \n",
    "                if tool2_scores:\n",
    "                    print(f\"     {data['tool2']}: n={len(tool2_scores)}, Mean={np.mean(tool2_scores):.2f}¬±{np.std(tool2_scores):.2f}\")\n",
    "                else:\n",
    "                    print(f\"     {data['tool2']}: No data\")\n",
    "        \n",
    "        # Summary statistics for RMSD agreement\n",
    "        print(f\"\\nüìà TOOL AGREEMENT SUMMARY:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Sort by mean agreement (best to worst)\n",
    "        sorted_pairs = sorted(tool_pair_data, key=lambda x: x['mean_rmsd'])\n",
    "        \n",
    "        print(\"üèÜ Tool pairs ranked by agreement (best to worst):\")\n",
    "        for i, data in enumerate(sorted_pairs, 1):\n",
    "            print(f\"   {i}. {data['tool_pair']}: {data['mean_rmsd']:.2f} ¬± {data['std_rmsd']:.2f} √Ö \"\n",
    "                  f\"(median: {data['median_rmsd']:.2f} √Ö, n={data['n_pairs']:,})\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        all_means = [data['mean_rmsd'] for data in tool_pair_data]\n",
    "        best_agreement = min(all_means)\n",
    "        worst_agreement = max(all_means)\n",
    "        \n",
    "        print(f\"\\nüìä Overall agreement statistics:\")\n",
    "        print(f\"   Best tool pair agreement: {best_agreement:.2f} √Ö\")\n",
    "        print(f\"   Worst tool pair agreement: {worst_agreement:.2f} √Ö\")\n",
    "        print(f\"   Average across all pairs: {np.mean(all_means):.2f} ¬± {np.std(all_means):.2f} √Ö\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No valid tool pairs found for agreement analysis\")\n",
    "    \n",
    "    # Reset matplotlib parameters to defaults\n",
    "    plt.rcdefaults()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot generate tool agreement distribution - missing required data\")\n",
    "    print(\"   Required: RMSD, Tool1, Tool2 columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb02314",
   "metadata": {},
   "source": [
    "## Research Question 2: Cluster-Based Distinction of Preferred vs Non-Preferred Cavities\n",
    "\n",
    "This section investigates whether **cavity cluster membership** can distinguish between **preferred and non-preferred binding sites** for each drug-target combination.\n",
    "\n",
    "### Key Research Question:\n",
    "**Can we distinguish between preferred cavities and other cavities based on their cluster membership patterns?**\n",
    "\n",
    "### Analysis Strategy:\n",
    "1. **Define Preferred Cavities**: For each unique drug-uniprot combination, identify preferred cavities using multiple criteria:\n",
    "   - **Lowest RMSD** (best structural agreement between tools)\n",
    "   - **Best Score1** (optimal docking score from primary tool)\n",
    "   - **Best Score2** (optimal docking score from secondary tool)\n",
    "\n",
    "2. **Cluster Comparison**: Compare the cluster membership patterns between:\n",
    "   - **Preferred cavities** (selected by each criterion)\n",
    "   - **Non-preferred cavities** (all other cavities for the same drug-target pair)\n",
    "\n",
    "3. **Statistical Analysis**: Test whether preferred and non-preferred cavities come from significantly different cluster distributions\n",
    "\n",
    "4. **Visualization**: Generate comparative plots showing cluster characteristics for preferred vs non-preferred cavities\n",
    "\n",
    "### Expected Insights:\n",
    "- Whether certain cavity clusters are systematically preferred across drug-target pairs\n",
    "- How different preference criteria (RMSD vs scores) affect cluster selection patterns\n",
    "- Statistical significance of cluster-based cavity preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Research Question 2: Preferred vs Non-Preferred Cavity Analysis ===\")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "df_rq2 = combined_results.to_pandas()\n",
    "\n",
    "# Check if we have cluster information\n",
    "if 'cavity_cluster_id' not in df_rq2.columns:\n",
    "    print(\"‚ùå No cluster information available - cannot perform cluster-based analysis\")\n",
    "    rq2_analysis = None\n",
    "else:\n",
    "    print(\"‚úÖ Cluster information available - proceeding with analysis\")\n",
    "    \n",
    "    # Remove rows without cluster information\n",
    "    df_rq2_clean = df_rq2.dropna(subset=['cavity_cluster_id']).copy()\n",
    "    print(f\"Dataset for RQ2: {len(df_rq2_clean):,} rows with cluster information\")\n",
    "    \n",
    "    # First, let's extract individual tool scores for each cavity\n",
    "    print(\"\\nüîç Extracting individual tool scores for each cavity...\")\n",
    "    \n",
    "    # Create a comprehensive dataset with all tool scores per cavity\n",
    "    cavity_scores = {}  # Key: (drug, target, cavity), Value: {tool_scores, rmsd_info, cluster}\n",
    "    \n",
    "    for _, row in df_rq2_clean.iterrows():\n",
    "        key = (row['drugbank_id'], row['uniprot_id'], row['cavity_index'])\n",
    "        \n",
    "        if key not in cavity_scores:\n",
    "            cavity_scores[key] = {\n",
    "                'drug': row['drugbank_id'],\n",
    "                'target': row['uniprot_id'],\n",
    "                'cavity_index': row['cavity_index'],\n",
    "                'cluster_id': row['cavity_cluster_id'],\n",
    "                'tool_scores': {},\n",
    "                'rmsd_data': []\n",
    "            }\n",
    "        \n",
    "        # Store individual tool scores\n",
    "        tool1 = row['Tool1']\n",
    "        tool2 = row['Tool2']\n",
    "        \n",
    "        if pd.notna(tool1) and pd.notna(row['Score1']):\n",
    "            cavity_scores[key]['tool_scores'][tool1] = row['Score1']\n",
    "        if pd.notna(tool2) and pd.notna(row['Score2']):\n",
    "            cavity_scores[key]['tool_scores'][tool2] = row['Score2']\n",
    "        \n",
    "        # Store RMSD information\n",
    "        cavity_scores[key]['rmsd_data'].append({\n",
    "            'tool1': tool1,\n",
    "            'tool2': tool2,\n",
    "            'rmsd': row['RMSD']\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Extracted scores for {len(cavity_scores):,} unique cavities\")\n",
    "    \n",
    "    # Calculate minimum RMSD for each cavity (across all tool pairs)\n",
    "    # This represents the best consensus pose for each cavity\n",
    "    for key in cavity_scores:\n",
    "        rmsd_values = [item['rmsd'] for item in cavity_scores[key]['rmsd_data'] if pd.notna(item['rmsd'])]\n",
    "        cavity_scores[key]['min_rmsd'] = np.min(rmsd_values) if rmsd_values else np.nan\n",
    "        cavity_scores[key]['avg_rmsd'] = np.mean(rmsd_values) if rmsd_values else np.nan  # Keep for reference\n",
    "    \n",
    "    # Define preference criteria with proper tool names\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': 'Best Consensus (Min RMSD)',\n",
    "        'best_gold': 'Best GOLD Score',\n",
    "        'best_ledock': 'Best LeDock Score',\n",
    "        'best_smina': 'Best Smina Score'\n",
    "    }\n",
    "    \n",
    "    # Initialize results storage\n",
    "    rq2_analysis = {\n",
    "        'preferred_cavities': {},\n",
    "        'cluster_comparisons': {},\n",
    "        'statistical_tests': {},\n",
    "        'summary_stats': {}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(preference_criteria)} preference criteria...\")\n",
    "    \n",
    "    # For each preference criterion\n",
    "    for criterion_key, criterion_name in preference_criteria.items():\n",
    "        print(f\"\\n--- Analyzing {criterion_name} preference ---\")\n",
    "        \n",
    "        # Storage for this criterion\n",
    "        preferred_cavities = []\n",
    "        non_preferred_cavities = []\n",
    "        preferred_clusters = []\n",
    "        non_preferred_clusters = []\n",
    "        \n",
    "        # Group cavities by drug-target pairs\n",
    "        drug_target_cavities = {}\n",
    "        for key, cavity_data in cavity_scores.items():\n",
    "            dt_key = (cavity_data['drug'], cavity_data['target'])\n",
    "            if dt_key not in drug_target_cavities:\n",
    "                drug_target_cavities[dt_key] = []\n",
    "            drug_target_cavities[dt_key].append(cavity_data)\n",
    "        \n",
    "        valid_pairs = 0\n",
    "        \n",
    "        for (drug, target), cavities in drug_target_cavities.items():\n",
    "            if len(cavities) < 2:  # Need at least 2 cavities to distinguish preferred vs non-preferred\n",
    "                continue\n",
    "            \n",
    "            # Filter cavities based on available data for this criterion\n",
    "            if criterion_key == 'lowest_rmsd':\n",
    "                valid_cavities = [c for c in cavities if pd.notna(c['min_rmsd'])]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with lowest minimum RMSD (best consensus)\n",
    "                preferred_cavity = min(valid_cavities, key=lambda x: x['min_rmsd'])\n",
    "            elif criterion_key == 'best_gold':\n",
    "                valid_cavities = [c for c in cavities if 'GOLD' in c['tool_scores']]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with best (highest) GOLD score\n",
    "                preferred_cavity = max(valid_cavities, key=lambda x: x['tool_scores']['GOLD'])\n",
    "            elif criterion_key == 'best_ledock':\n",
    "                valid_cavities = [c for c in cavities if 'LeDock' in c['tool_scores']]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with best (most negative) LeDock score\n",
    "                preferred_cavity = min(valid_cavities, key=lambda x: x['tool_scores']['LeDock'])\n",
    "            elif criterion_key == 'best_smina':\n",
    "                valid_cavities = [c for c in cavities if 'Smina' in c['tool_scores']]\n",
    "                if len(valid_cavities) < 2:\n",
    "                    continue\n",
    "                # Find cavity with best (most negative) Smina score\n",
    "                preferred_cavity = min(valid_cavities, key=lambda x: x['tool_scores']['Smina'])\n",
    "            \n",
    "            # Store preferred cavity info\n",
    "            preferred_cavities.append(preferred_cavity)\n",
    "            preferred_clusters.append(preferred_cavity['cluster_id'])\n",
    "            \n",
    "            # Store non-preferred cavity info\n",
    "            non_preferred = [c for c in valid_cavities if c != preferred_cavity]\n",
    "            non_preferred_cavities.extend(non_preferred)\n",
    "            non_preferred_clusters.extend([c['cluster_id'] for c in non_preferred])\n",
    "            \n",
    "            valid_pairs += 1\n",
    "        \n",
    "        # Store results for this criterion\n",
    "        rq2_analysis['preferred_cavities'][criterion_key] = {\n",
    "            'preferred': preferred_cavities,\n",
    "            'non_preferred': non_preferred_cavities,\n",
    "            'preferred_clusters': preferred_clusters,\n",
    "            'non_preferred_clusters': non_preferred_clusters,\n",
    "            'n_drug_target_pairs': valid_pairs\n",
    "        }\n",
    "        \n",
    "        print(f\"   Analyzed {valid_pairs:,} drug-target pairs\")\n",
    "        print(f\"   Preferred cavities: {len(preferred_cavities):,}\")\n",
    "        print(f\"   Non-preferred cavities: {len(non_preferred_cavities):,}\")\n",
    "        print(f\"   Unique preferred clusters: {len(set(preferred_clusters)):,}\")\n",
    "        print(f\"   Unique non-preferred clusters: {len(set(non_preferred_clusters)):,}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed preferred cavity identification for all criteria\")\n",
    "    \n",
    "    # Calculate cluster statistics for comparison\n",
    "    print(f\"\\n=== CLUSTER COMPARISON STATISTICS ===\")\n",
    "    \n",
    "    for criterion_key, criterion_name in preference_criteria.items():\n",
    "        data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "        \n",
    "        if len(data['preferred_clusters']) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{criterion_name}:\")\n",
    "        \n",
    "        # Convert to pandas Series for analysis\n",
    "        preferred_clusters_series = pd.Series(data['preferred_clusters'])\n",
    "        non_preferred_clusters_series = pd.Series(data['non_preferred_clusters'])\n",
    "        \n",
    "        # Cluster frequency analysis\n",
    "        preferred_cluster_counts = preferred_clusters_series.value_counts()\n",
    "        non_preferred_cluster_counts = non_preferred_clusters_series.value_counts()\n",
    "        \n",
    "        # Top preferred clusters\n",
    "        top_preferred = preferred_cluster_counts.head(5)\n",
    "        print(f\"   Top 5 preferred clusters: {dict(top_preferred)}\")\n",
    "        \n",
    "        # Statistical comparison using Chi-square test\n",
    "        from scipy.stats import chi2_contingency\n",
    "        \n",
    "        # Create contingency table\n",
    "        all_clusters = set(data['preferred_clusters'] + data['non_preferred_clusters'])\n",
    "        \n",
    "        contingency_data = []\n",
    "        for cluster in all_clusters:\n",
    "            pref_count = preferred_cluster_counts.get(cluster, 0)\n",
    "            non_pref_count = non_preferred_cluster_counts.get(cluster, 0)\n",
    "            contingency_data.append([pref_count, non_pref_count])\n",
    "        \n",
    "        if len(contingency_data) > 1:\n",
    "            contingency_table = np.array(contingency_data)\n",
    "            \n",
    "            # Perform chi-square test\n",
    "            try:\n",
    "                chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "                \n",
    "                rq2_analysis['statistical_tests'][criterion_key] = {\n",
    "                    'chi2_statistic': chi2,\n",
    "                    'p_value': p_value,\n",
    "                    'degrees_of_freedom': dof,\n",
    "                    'significant': p_value < 0.05\n",
    "                }\n",
    "                \n",
    "                print(f\"   Chi-square test: œá¬≤ = {chi2:.3f}, p = {p_value:.6f}\")\n",
    "                print(f\"   {'‚úÖ Significant' if p_value < 0.05 else '‚ùå Not significant'} difference in cluster distributions\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Could not perform chi-square test: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Statistical analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# üìä RQ2: Violin Plots for Preferred vs Non-Preferred Cavities (Individual Tool Scores)\n",
    "# ===============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Check if RQ2 analysis exists\n",
    "if 'rq2_analysis' not in globals() or rq2_analysis is None:\n",
    "    print(\"‚ùå RQ2 analysis not found. Please run the previous RQ2 analysis cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ2 analysis found. Creating violin plots...\")\n",
    "    \n",
    "    # Set up poster-quality plotting parameters (matching RQ1)\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 24,                 # Larger base font size\n",
    "        'axes.titlesize': 28,            # Larger title font\n",
    "        'axes.labelsize': 22,            # Larger axis labels\n",
    "        'xtick.labelsize': 22,           # Larger tick labels\n",
    "        'ytick.labelsize': 20,           # Larger tick labels\n",
    "        'legend.fontsize': 22,           # Larger legend font\n",
    "        'figure.titlesize': 32           # Larger figure title\n",
    "    })\n",
    "    \n",
    "    # Define preference criteria with their corresponding metrics\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus\\n(Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD\\nScore', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock\\nScore', 'metric': 'LeDock Score (kcal/mol)'},\n",
    "        'best_smina': {'name': 'Best Smina\\nScore', 'metric': 'Smina Score (kcal/mol)'}\n",
    "    }\n",
    "    \n",
    "    # Count how many criteria have data\n",
    "    valid_criteria = []\n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        if criterion_key in rq2_analysis['preferred_cavities'] and len(rq2_analysis['preferred_cavities'][criterion_key]['preferred']) > 0:\n",
    "            valid_criteria.append((criterion_key, criterion_info))\n",
    "    \n",
    "    if len(valid_criteria) == 0:\n",
    "        print(\"‚ùå No valid criteria with data found\")\n",
    "    else:\n",
    "        # Create 2x2 subplot grid\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()  # Flatten for easy indexing\n",
    "        \n",
    "        # Loop over each valid preference criterion\n",
    "        for idx, (criterion_key, criterion_info) in enumerate(valid_criteria):\n",
    "            if idx >= 4:  # Only show first 4 criteria\n",
    "                break\n",
    "                \n",
    "            ax = axes[idx]\n",
    "            data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "            criterion_name = criterion_info['name']\n",
    "            metric = criterion_info['metric']\n",
    "            \n",
    "            # Prepare data for this specific metric\n",
    "            metric_data = []\n",
    "            \n",
    "            if metric == 'RMSD (√Ö)':\n",
    "                # RMSD data - use minimum RMSD for each cavity\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'min_rmsd' in cavity and pd.notna(cavity['min_rmsd']):\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['min_rmsd']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'min_rmsd' in cavity and pd.notna(cavity['min_rmsd']):\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['min_rmsd']})\n",
    "            \n",
    "            elif metric == 'GOLD Score':\n",
    "                # GOLD scores\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'tool_scores' in cavity and 'GOLD' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['tool_scores']['GOLD']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'tool_scores' in cavity and 'GOLD' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['tool_scores']['GOLD']})\n",
    "            \n",
    "            elif metric == 'LeDock Score (kcal/mol)':\n",
    "                # LeDock scores\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'tool_scores' in cavity and 'LeDock' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['tool_scores']['LeDock']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'tool_scores' in cavity and 'LeDock' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['tool_scores']['LeDock']})\n",
    "            \n",
    "            elif metric == 'Smina Score (kcal/mol)':\n",
    "                # Smina scores\n",
    "                for cavity in data['preferred']:\n",
    "                    if 'tool_scores' in cavity and 'Smina' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Preferred', 'Value': cavity['tool_scores']['Smina']})\n",
    "                for cavity in data['non_preferred']:\n",
    "                    if 'tool_scores' in cavity and 'Smina' in cavity['tool_scores']:\n",
    "                        metric_data.append({'Preference': 'Non-Preferred', 'Value': cavity['tool_scores']['Smina']})\n",
    "            \n",
    "            # Plot if we have data\n",
    "            if len(metric_data) > 0:\n",
    "                df_metric = pd.DataFrame(metric_data)\n",
    "                \n",
    "                # Check if we have both preferred and non-preferred data\n",
    "                prefs = df_metric['Preference'].unique()\n",
    "                if len(prefs) > 1:\n",
    "                    try:\n",
    "                        # Create split violin plot using seaborn\n",
    "                        df_metric['dummy'] = 'All'\n",
    "                        sns.violinplot(\n",
    "                            data=df_metric,\n",
    "                            y='Value',\n",
    "                            x='dummy',\n",
    "                            hue='Preference',\n",
    "                            ax=ax,\n",
    "                            palette=['lightcoral', 'lightblue'],\n",
    "                            inner='quartile',\n",
    "                            split=True\n",
    "                        )\n",
    "                        \n",
    "                        # Customize subplot\n",
    "                        ax.set_title(f'{criterion_name}', fontsize=24, fontweight='bold', pad=20)\n",
    "                        ax.set_ylabel(metric, fontsize=22, fontweight='bold')\n",
    "                        ax.set_xlabel(\"\")  # Remove x-axis label\n",
    "                        ax.set_xticklabels([])  # Remove x-axis tick labels\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        \n",
    "                        # Remove legend (it's clear from context)\n",
    "                        if ax.legend_:\n",
    "                            ax.legend_.remove()\n",
    "                        \n",
    "                        # Set appropriate y-axis limits\n",
    "                        if metric == 'RMSD (√Ö)':\n",
    "                            ax.set_ylim(0, max(15, df_metric['Value'].max() * 1.1))\n",
    "                        elif metric == 'GOLD Score':\n",
    "                            ax.set_ylim(0, max(100, df_metric['Value'].max() * 1.1))\n",
    "                        elif metric in ['LeDock Score (kcal/mol)', 'Smina Score (kcal/mol)']:\n",
    "                            ax.set_ylim(min(-15, df_metric['Value'].min() * 1.1), \n",
    "                                       max(5, df_metric['Value'].max() * 1.1))\n",
    "                        \n",
    "                        # Enhanced statistical testing for non-normal data\n",
    "                        preferred_vals = df_metric[df_metric['Preference'] == 'Preferred']['Value'].values\n",
    "                        non_preferred_vals = df_metric[df_metric['Preference'] == 'Non-Preferred']['Value'].values\n",
    "                        \n",
    "                        if len(preferred_vals) > 0 and len(non_preferred_vals) > 0:\n",
    "                            try:\n",
    "                                # Mann-Whitney U test (appropriate for non-normal data)\n",
    "                                stat, p_val = stats.mannwhitneyu(preferred_vals, non_preferred_vals, alternative='two-sided')\n",
    "                                \n",
    "                                # Calculate effect size (rank biserial correlation)\n",
    "                                n1, n2 = len(preferred_vals), len(non_preferred_vals)\n",
    "                                effect_size = 1 - (2 * stat) / (n1 * n2)\n",
    "                                \n",
    "                                # Interpret effect size\n",
    "                                if abs(effect_size) < 0.1:\n",
    "                                    effect_interpretation = \"negligible\"\n",
    "                                elif abs(effect_size) < 0.3:\n",
    "                                    effect_interpretation = \"small\"\n",
    "                                elif abs(effect_size) < 0.5:\n",
    "                                    effect_interpretation = \"medium\"\n",
    "                                else:\n",
    "                                    effect_interpretation = \"large\"\n",
    "                                \n",
    "                                # Determine significance\n",
    "                                if p_val < 0.001:\n",
    "                                    significance = \"***\"\n",
    "                                elif p_val < 0.01:\n",
    "                                    significance = \"**\"\n",
    "                                elif p_val < 0.05:\n",
    "                                    significance = \"*\"\n",
    "                                else:\n",
    "                                    significance = \"ns\"\n",
    "                                \n",
    "                                # Add statistical annotation with effect size\n",
    "                                ax.text(0.5, 0.95, f'p = {p_val:.3f} {significance}', \n",
    "                                       transform=ax.transAxes, ha='center', va='top', \n",
    "                                       fontsize=18, fontweight='bold')\n",
    "                                ax.text(0.5, 0.88, f'effect size = {effect_size:.3f} ({effect_interpretation})', \n",
    "                                       transform=ax.transAxes, ha='center', va='top', \n",
    "                                       fontsize=16, style='italic')\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ö†Ô∏è Statistical test failed for {criterion_name} - {metric}: {e}\")\n",
    "                        \n",
    "                        print(f\"‚úÖ Created split violin plot for {criterion_name}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error creating violin plot for {criterion_name}: {e}\")\n",
    "                        ax.text(0.5, 0.5, f'Error creating plot', \n",
    "                               ha='center', va='center', transform=ax.transAxes, \n",
    "                               fontsize=20, fontweight='bold')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, f'Only {prefs[0]} data available', \n",
    "                           ha='center', va='center', transform=ax.transAxes, \n",
    "                           fontsize=20, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'No data available', \n",
    "                       ha='center', va='center', transform=ax.transAxes, \n",
    "                       fontsize=20, fontweight='bold')\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for idx in range(len(valid_criteria), 4):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary statistics with improved statistical analysis\n",
    "        print(f\"\\nüìä COMPREHENSIVE SUMMARY STATISTICS:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Statistical Analysis Notes:\")\n",
    "        print(\"‚Ä¢ Mann-Whitney U test used (appropriate for non-normal distributions)\")\n",
    "        print(\"‚Ä¢ Effect size calculated using rank biserial correlation\")\n",
    "        print(\"‚Ä¢ Effect size interpretation: negligible (<0.1), small (0.1-0.3), medium (0.3-0.5), large (>0.5)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for criterion_key, criterion_info in valid_criteria:\n",
    "            data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "            criterion_name = criterion_info['name'].replace('\\n', ' ')  # Remove newlines for clean printing\n",
    "            metric = criterion_info['metric']\n",
    "            \n",
    "            print(f\"\\n{criterion_name} - {metric}:\")\n",
    "            print(f\"  Drug-target pairs analyzed: {data['n_drug_target_pairs']:,}\")\n",
    "            print(f\"  Preferred cavities: {len(data['preferred']):,}\")\n",
    "            print(f\"  Non-preferred cavities: {len(data['non_preferred']):,}\")\n",
    "            \n",
    "            # Get the appropriate statistics based on the metric\n",
    "            if metric == 'RMSD (√Ö)':\n",
    "                # Min RMSD statistics\n",
    "                pref_vals = [c['min_rmsd'] for c in data['preferred'] if 'min_rmsd' in c and pd.notna(c['min_rmsd'])]\n",
    "                non_pref_vals = [c['min_rmsd'] for c in data['non_preferred'] if 'min_rmsd' in c and pd.notna(c['min_rmsd'])]\n",
    "                unit = \"√Ö\"\n",
    "            elif metric == 'GOLD Score':\n",
    "                # GOLD Score statistics\n",
    "                pref_vals = [c['tool_scores']['GOLD'] for c in data['preferred'] if 'tool_scores' in c and 'GOLD' in c['tool_scores']]\n",
    "                non_pref_vals = [c['tool_scores']['GOLD'] for c in data['non_preferred'] if 'tool_scores' in c and 'GOLD' in c['tool_scores']]\n",
    "                unit = \"\"\n",
    "            elif metric == 'LeDock Score (kcal/mol)':\n",
    "                # LeDock Score statistics\n",
    "                pref_vals = [c['tool_scores']['LeDock'] for c in data['preferred'] if 'tool_scores' in c and 'LeDock' in c['tool_scores']]\n",
    "                non_pref_vals = [c['tool_scores']['LeDock'] for c in data['non_preferred'] if 'tool_scores' in c and 'LeDock' in c['tool_scores']]\n",
    "                unit = \"kcal/mol\"\n",
    "            elif metric == 'Smina Score (kcal/mol)':\n",
    "                # Smina Score statistics\n",
    "                pref_vals = [c['tool_scores']['Smina'] for c in data['preferred'] if 'tool_scores' in c and 'Smina' in c['tool_scores']]\n",
    "                non_pref_vals = [c['tool_scores']['Smina'] for c in data['non_preferred'] if 'tool_scores' in c and 'Smina' in c['tool_scores']]\n",
    "                unit = \"kcal/mol\"\n",
    "            \n",
    "            if len(pref_vals) > 0 and len(non_pref_vals) > 0:\n",
    "                print(f\"\\n  {metric} Statistics:\")\n",
    "                print(f\"    Preferred: n={len(pref_vals)}, median={np.median(pref_vals):.3f} {unit} (IQR: {np.percentile(pref_vals, 25):.3f}-{np.percentile(pref_vals, 75):.3f})\")\n",
    "                print(f\"    Non-Preferred: n={len(non_pref_vals)}, median={np.median(non_pref_vals):.3f} {unit} (IQR: {np.percentile(non_pref_vals, 25):.3f}-{np.percentile(non_pref_vals, 75):.3f})\")\n",
    "        \n",
    "        # Reset matplotlib parameters\n",
    "        plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c92a11",
   "metadata": {},
   "source": [
    "## Research Question 3: Do Cavity Clusters Separate Preferred from Non-Preferred Cavities?\n",
    "\n",
    "**Research Question 3:** *Within each drug-target interaction, do preferred and non-preferred cavities belong to the same or different cavity clusters?*\n",
    "\n",
    "### üéØ Specific Analysis Goal:\n",
    "For each **drugbank_id - uniprot_id** combination (drug-target pair) with multiple cavities:\n",
    "1. **Identify the preferred cavity** based on each scoring criterion (lowest RMSD, best GOLD/LeDock/Smina scores)\n",
    "2. **Compare cluster membership** between the preferred cavity and all non-preferred cavities\n",
    "3. **Determine cluster separation**: Does the preferred cavity belong to a different cluster than the non-preferred cavities?\n",
    "4. **Calculate separation rates** and test for statistical significance\n",
    "\n",
    "### üìä Key Metrics:\n",
    "- **Cluster separation rate**: Percentage of drug-target pairs where preferred cavity is in a different cluster\n",
    "- **Statistical significance**: Binomial test against 50% random separation expectation\n",
    "- **Separation patterns**: Analysis across different preference criteria\n",
    "\n",
    "### üß© Analysis Logic:\n",
    "- **Same cluster**: Preferred cavity shares its cluster ID with at least one non-preferred cavity\n",
    "- **Different clusters**: Preferred cavity has a unique cluster ID from all non-preferred cavities\n",
    "- **Separation rate**: Proportion of drug-target pairs showing different clusters\n",
    "\n",
    "### üìà Main Visualizations:\n",
    "- **Separation rate bar plots** showing cluster separation effectiveness for each criterion\n",
    "- **Count plots** showing distribution of same vs different cluster cases\n",
    "- **Statistical significance indicators** for each preference criterion\n",
    "\n",
    "This analysis directly addresses whether cavity clustering is useful for distinguishing high-quality binding sites from lower-quality alternatives in drug discovery.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üî¨ RQ3: Cluster Separation Analysis - Do Preferred and Non-Preferred Cavities \n",
    "#      Belong to Same or Different Clusters?\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç RESEARCH QUESTION 3: Do cavity clusters separate preferred from non-preferred cavities?\")\n",
    "print(\"=\" * 85)\n",
    "print(\"üéØ SPECIFIC QUESTION: Within each drug-target pair, do preferred and non-preferred\")\n",
    "print(\"   cavities belong to the same cluster or different clusters?\")\n",
    "\n",
    "# Check if RQ2 analysis exists and cavity cluster data is available\n",
    "if 'rq2_analysis' not in globals() or rq2_analysis is None:\n",
    "    print(\"‚ùå RQ2 analysis not found. Please run the RQ2 analysis first.\")\n",
    "elif 'cavity_cluster_id' not in combined_results.columns:\n",
    "    print(\"‚ùå Cavity cluster data not found. Please ensure cavity cluster information is loaded.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ2 analysis and cavity cluster data found. Proceeding with RQ3 cluster separation analysis...\")\n",
    "    \n",
    "    # Initialize RQ3 analysis storage\n",
    "    rq3_analysis = {\n",
    "        'drug_target_analysis': {},\n",
    "        'cluster_separation_stats': {},\n",
    "        'detailed_results': {}\n",
    "    }\n",
    "    \n",
    "    # Define the same preference criteria as RQ2\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus (Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD Score', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock Score', 'metric': 'LeDock Score (kcal/mol)'},\n",
    "        'best_smina': {'name': 'Best Smina Score', 'metric': 'Smina Score (kcal/mol)'}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß© Analyzing cluster separation for {len(preference_criteria)} preference criteria...\")\n",
    "    print(\"üìã Analysis approach:\")\n",
    "    print(\"   1. For each drug-target pair with multiple cavities\")\n",
    "    print(\"   2. Identify the preferred cavity based on each criterion\")\n",
    "    print(\"   3. Check if preferred cavity's cluster differs from non-preferred cavities' clusters\")\n",
    "    print(\"   4. Calculate separation rates and statistical significance\")\n",
    "    \n",
    "    # For each preference criterion, analyze cluster separation within drug-target pairs\n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        print(f\"\\n--- Analyzing {criterion_info['name']} ---\")\n",
    "        \n",
    "        # Get the preferred and non-preferred cavity data from RQ2\n",
    "        if criterion_key not in rq2_analysis['preferred_cavities']:\n",
    "            print(f\"   ‚ö†Ô∏è No RQ2 data available for {criterion_key}\")\n",
    "            continue\n",
    "            \n",
    "        rq2_data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "        \n",
    "        # Prepare data structure for this criterion\n",
    "        drug_target_cluster_analysis = {}\n",
    "        \n",
    "        # Group cavities by drug-target pairs\n",
    "        dt_pairs = {}\n",
    "        \n",
    "        # Process preferred cavities\n",
    "        for cavity in rq2_data['preferred']:\n",
    "            dt_key = (cavity['drug'], cavity['target'])\n",
    "            if dt_key not in dt_pairs:\n",
    "                dt_pairs[dt_key] = {'preferred': None, 'non_preferred': []}\n",
    "            dt_pairs[dt_key]['preferred'] = cavity\n",
    "        \n",
    "        # Process non-preferred cavities\n",
    "        for cavity in rq2_data['non_preferred']:\n",
    "            dt_key = (cavity['drug'], cavity['target'])\n",
    "            if dt_key not in dt_pairs:\n",
    "                dt_pairs[dt_key] = {'preferred': None, 'non_preferred': []}\n",
    "            dt_pairs[dt_key]['non_preferred'].append(cavity)\n",
    "        \n",
    "        # Analyze cluster separation for each drug-target pair\n",
    "        separation_results = []\n",
    "        valid_pairs = 0\n",
    "        \n",
    "        for dt_key, pair_data in dt_pairs.items():\n",
    "            drug, target = dt_key\n",
    "            \n",
    "            # Skip if no preferred cavity or no non-preferred cavities\n",
    "            if pair_data['preferred'] is None or len(pair_data['non_preferred']) == 0:\n",
    "                continue\n",
    "            \n",
    "            preferred_cavity = pair_data['preferred']\n",
    "            non_preferred_cavities = pair_data['non_preferred']\n",
    "            \n",
    "            # Check if cluster data is available\n",
    "            if 'cluster_id' not in preferred_cavity or pd.isna(preferred_cavity['cluster_id']):\n",
    "                continue\n",
    "            \n",
    "            preferred_cluster = preferred_cavity['cluster_id']\n",
    "            non_preferred_clusters = [c['cluster_id'] for c in non_preferred_cavities \n",
    "                                    if 'cluster_id' in c and pd.notna(c['cluster_id'])]\n",
    "            \n",
    "            if len(non_preferred_clusters) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Determine separation status\n",
    "            if preferred_cluster in non_preferred_clusters:\n",
    "                separation_status = 'same_cluster'\n",
    "            else:\n",
    "                separation_status = 'different_cluster'\n",
    "            \n",
    "            separation_results.append({\n",
    "                'drug': drug,\n",
    "                'target': target,\n",
    "                'preferred_cluster': preferred_cluster,\n",
    "                'non_preferred_clusters': non_preferred_clusters,\n",
    "                'separation_status': separation_status,\n",
    "                'n_non_preferred': len(non_preferred_clusters),\n",
    "                'unique_non_preferred_clusters': len(set(non_preferred_clusters))\n",
    "            })\n",
    "            \n",
    "            valid_pairs += 1\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if len(separation_results) > 0:\n",
    "            same_cluster_count = sum(1 for r in separation_results if r['separation_status'] == 'same_cluster')\n",
    "            different_cluster_count = sum(1 for r in separation_results if r['separation_status'] == 'different_cluster')\n",
    "            total_pairs = same_cluster_count + different_cluster_count\n",
    "            separation_rate = different_cluster_count / total_pairs if total_pairs > 0 else 0\n",
    "            \n",
    "            # Store results\n",
    "            rq3_analysis['drug_target_analysis'][criterion_key] = separation_results\n",
    "            rq3_analysis['cluster_separation_stats'][criterion_key] = {\n",
    "                'total_pairs': total_pairs,\n",
    "                'same_cluster': same_cluster_count,\n",
    "                'different_cluster': different_cluster_count,\n",
    "                'separation_rate': separation_rate\n",
    "            }\n",
    "            \n",
    "            print(f\"   Valid drug-target pairs analyzed: {total_pairs}\")\n",
    "            print(f\"   Same cluster (preferred shares cluster with non-preferred): {same_cluster_count} ({same_cluster_count/total_pairs*100:.1f}%)\")\n",
    "            print(f\"   Different clusters (preferred in unique cluster): {different_cluster_count} ({different_cluster_count/total_pairs*100:.1f}%)\")\n",
    "            print(f\"   Cluster separation rate: {separation_rate:.3f}\")\n",
    "            \n",
    "            # Statistical significance test\n",
    "            # Binomial test: null hypothesis is 50% separation rate (random)\n",
    "            from scipy.stats import binomtest\n",
    "            p_value = binomtest(different_cluster_count, total_pairs, 0.5, alternative='two-sided').pvalue\n",
    "            \n",
    "            rq3_analysis['cluster_separation_stats'][criterion_key]['binomial_test_p'] = p_value\n",
    "            rq3_analysis['cluster_separation_stats'][criterion_key]['significant'] = p_value < 0.05\n",
    "            \n",
    "            print(f\"   üìä Binomial test (H0: separation rate = 50%): p = {p_value:.6f}\")\n",
    "            print(f\"   üìä Result: {'‚úÖ Significant separation' if p_value < 0.05 else '‚ùå No significant separation'}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No valid drug-target pairs found for analysis\")\n",
    "    \n",
    "    # Summary of findings\n",
    "    print(f\"\\nüìä RQ3 CLUSTER SEPARATION ANALYSIS SUMMARY:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    valid_criteria = [k for k in rq3_analysis['cluster_separation_stats'] \n",
    "                     if rq3_analysis['cluster_separation_stats'][k]['total_pairs'] > 0]\n",
    "    \n",
    "    if len(valid_criteria) > 0:\n",
    "        print(f\"‚úÖ Successfully analyzed {len(valid_criteria)} preference criteria\")\n",
    "        \n",
    "        # Show summary statistics\n",
    "        for criterion_key in valid_criteria:\n",
    "            stats = rq3_analysis['cluster_separation_stats'][criterion_key]\n",
    "            criterion_name = preference_criteria[criterion_key]['name']\n",
    "            \n",
    "            print(f\"\\n{criterion_name}:\")\n",
    "            print(f\"  Separation rate: {stats['separation_rate']:.3f}\")\n",
    "            print(f\"  Statistical significance: {'‚úÖ Yes' if stats['significant'] else '‚ùå No'} (p = {stats['binomial_test_p']:.6f})\")\n",
    "        \n",
    "        # Overall conclusion\n",
    "        significant_criteria = [k for k in valid_criteria \n",
    "                              if rq3_analysis['cluster_separation_stats'][k]['significant']]\n",
    "        \n",
    "        print(f\"\\nüéØ OVERALL CONCLUSION:\")\n",
    "        if len(significant_criteria) > 0:\n",
    "            print(f\"   ‚úÖ {len(significant_criteria)} out of {len(valid_criteria)} criteria show significant cluster separation\")\n",
    "            print(f\"   üî• Cavity clusters CAN effectively separate preferred from non-preferred cavities\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå No criteria show significant cluster separation\")\n",
    "            print(f\"   üî• Cavity clusters do NOT effectively separate preferred from non-preferred cavities\")\n",
    "        \n",
    "        print(f\"\\nüéØ Ready for RQ3 visualization...\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No valid criteria found for cluster separation analysis\")\n",
    "        print(\"   Please check data availability and RQ2 analysis results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä RQ3: Cluster Separation Analysis for Preferred vs Non-Preferred Cavities\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Analyzing cluster separation for preferred vs non-preferred cavities...\")\n",
    "print(\"üéØ Research Question: Do preferred and non-preferred cavities belong to same or different clusters?\")\n",
    "\n",
    "# Check if RQ3 analysis exists\n",
    "if 'rq3_analysis' not in globals() or rq3_analysis is None:\n",
    "    print(\"‚ùå RQ3 analysis not found. Please run the previous RQ3 analysis cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ3 analysis found. Analyzing cluster separation within drug-target pairs...\")\n",
    "    \n",
    "    # Initialize cluster separation analysis\n",
    "    cluster_separation_analysis = {\n",
    "        'same_cluster': {},\n",
    "        'different_cluster': {},\n",
    "        'separation_rates': {},\n",
    "        'statistical_tests': {}\n",
    "    }\n",
    "    \n",
    "    # Define the same preference criteria as before\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus (Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD Score', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock Score', 'metric': 'LeDock Score (kcal/mol)'},\n",
    "        'best_smina': {'name': 'Best Smina Score', 'metric': 'Smina Score (kcal/mol)'}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüîç Analyzing cluster separation for {len(preference_criteria)} preference criteria...\")\n",
    "    \n",
    "    # For each preference criterion, analyze cluster separation\n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        print(f\"\\n--- Analyzing {criterion_info['name']} ---\")\n",
    "        \n",
    "        # Get the RQ2 data for this criterion\n",
    "        if criterion_key not in rq2_analysis['preferred_cavities']:\n",
    "            print(f\"   ‚ö†Ô∏è No RQ2 data available for {criterion_key}\")\n",
    "            continue\n",
    "            \n",
    "        rq2_data = rq2_analysis['preferred_cavities'][criterion_key]\n",
    "        \n",
    "        # Group cavities by drug-target pairs to analyze cluster separation\n",
    "        drug_target_cavities = {}\n",
    "        for key, cavity_data in cavity_scores.items():\n",
    "            dt_key = (cavity_data['drug'], cavity_data['target'])\n",
    "            if dt_key not in drug_target_cavities:\n",
    "                drug_target_cavities[dt_key] = []\n",
    "            drug_target_cavities[dt_key].append(cavity_data)\n",
    "        \n",
    "        # Analyze cluster separation for each drug-target pair\n",
    "        same_cluster_count = 0\n",
    "        different_cluster_count = 0\n",
    "        separation_details = []\n",
    "        \n",
    "        for (drug, target), cavities in drug_target_cavities.items():\n",
    "            if len(cavities) < 2:  # Need at least 2 cavities to analyze separation\n",
    "                continue\n",
    "            \n",
    "            # Find the preferred cavity for this drug-target pair and criterion\n",
    "            preferred_cavity = None\n",
    "            for cavity in rq2_data['preferred']:\n",
    "                if cavity['drug'] == drug and cavity['target'] == target:\n",
    "                    preferred_cavity = cavity\n",
    "                    break\n",
    "            \n",
    "            if preferred_cavity is None:\n",
    "                continue\n",
    "            \n",
    "            # Get non-preferred cavities for this drug-target pair\n",
    "            non_preferred_cavities = []\n",
    "            for cavity in rq2_data['non_preferred']:\n",
    "                if cavity['drug'] == drug and cavity['target'] == target:\n",
    "                    non_preferred_cavities.append(cavity)\n",
    "            \n",
    "            if len(non_preferred_cavities) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check cluster separation\n",
    "            preferred_cluster = preferred_cavity['cluster_id']\n",
    "            non_preferred_clusters = [c['cluster_id'] for c in non_preferred_cavities if pd.notna(c['cluster_id'])]\n",
    "            \n",
    "            if pd.notna(preferred_cluster) and len(non_preferred_clusters) > 0:\n",
    "                # Check if preferred cavity is in a different cluster from ALL non-preferred cavities\n",
    "                if preferred_cluster in non_preferred_clusters:\n",
    "                    same_cluster_count += 1\n",
    "                    separation_status = 'same_cluster'\n",
    "                else:\n",
    "                    different_cluster_count += 1\n",
    "                    separation_status = 'different_cluster'\n",
    "                \n",
    "                separation_details.append({\n",
    "                    'drug': drug,\n",
    "                    'target': target,\n",
    "                    'preferred_cluster': preferred_cluster,\n",
    "                    'non_preferred_clusters': non_preferred_clusters,\n",
    "                    'separation_status': separation_status,\n",
    "                    'n_cavities': len(cavities),\n",
    "                    'n_non_preferred': len(non_preferred_cavities)\n",
    "                })\n",
    "        \n",
    "        # Store results for this criterion\n",
    "        cluster_separation_analysis['same_cluster'][criterion_key] = same_cluster_count\n",
    "        cluster_separation_analysis['different_cluster'][criterion_key] = different_cluster_count\n",
    "        \n",
    "        total_analyzed = same_cluster_count + different_cluster_count\n",
    "        if total_analyzed > 0:\n",
    "            separation_rate = different_cluster_count / total_analyzed\n",
    "            cluster_separation_analysis['separation_rates'][criterion_key] = separation_rate\n",
    "            \n",
    "            print(f\"   Drug-target pairs analyzed: {total_analyzed}\")\n",
    "            print(f\"   Same cluster (preferred = non-preferred): {same_cluster_count} ({same_cluster_count/total_analyzed*100:.1f}%)\")\n",
    "            print(f\"   Different clusters (preferred ‚â† non-preferred): {different_cluster_count} ({different_cluster_count/total_analyzed*100:.1f}%)\")\n",
    "            print(f\"   Cluster separation rate: {separation_rate:.3f}\")\n",
    "            \n",
    "            # Statistical test: binomial test against null hypothesis of 50% separation\n",
    "            from scipy.stats import binomtest\n",
    "            \n",
    "            # Test if separation rate is significantly different from 50% (random)\n",
    "            p_value = binomtest(different_cluster_count, total_analyzed, 0.5, alternative='two-sided').pvalue\n",
    "            \n",
    "            cluster_separation_analysis['statistical_tests'][criterion_key] = {\n",
    "                'binomial_test_p': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'separation_rate': separation_rate,\n",
    "                'total_pairs': total_analyzed\n",
    "            }\n",
    "            \n",
    "            print(f\"   üìä Binomial test (H0: separation rate = 50%): p = {p_value:.6f}\")\n",
    "            print(f\"   üìä {'‚úÖ Significant' if p_value < 0.05 else '‚ùå Not significant'} deviation from random\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No valid drug-target pairs found for analysis\")\n",
    "    \n",
    "    # Create visualization: Bar plot showing separation rates\n",
    "    print(f\"\\nüìä Creating cluster separation visualization...\")\n",
    "    \n",
    "    # Set up plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 16,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 16,\n",
    "        'xtick.labelsize': 14,\n",
    "        'ytick.labelsize': 14,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    criteria_names = []\n",
    "    separation_rates = []\n",
    "    same_cluster_counts = []\n",
    "    different_cluster_counts = []\n",
    "    significance_markers = []\n",
    "    \n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        if criterion_key in cluster_separation_analysis['separation_rates']:\n",
    "            criteria_names.append(criterion_info['name'])\n",
    "            separation_rates.append(cluster_separation_analysis['separation_rates'][criterion_key])\n",
    "            same_cluster_counts.append(cluster_separation_analysis['same_cluster'][criterion_key])\n",
    "            different_cluster_counts.append(cluster_separation_analysis['different_cluster'][criterion_key])\n",
    "            \n",
    "            # Add significance marker\n",
    "            if criterion_key in cluster_separation_analysis['statistical_tests']:\n",
    "                p_val = cluster_separation_analysis['statistical_tests'][criterion_key]['binomial_test_p']\n",
    "                if p_val < 0.001:\n",
    "                    significance_markers.append('***')\n",
    "                elif p_val < 0.01:\n",
    "                    significance_markers.append('**')\n",
    "                elif p_val < 0.05:\n",
    "                    significance_markers.append('*')\n",
    "                else:\n",
    "                    significance_markers.append('ns')\n",
    "            else:\n",
    "                significance_markers.append('ns')\n",
    "    \n",
    "    if len(criteria_names) > 0:\n",
    "        # Create stacked bar plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: Separation rates\n",
    "        bars1 = ax1.bar(criteria_names, separation_rates, color='lightblue', edgecolor='black', linewidth=1)\n",
    "        ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random expectation (50%)')\n",
    "        ax1.set_ylabel('Cluster Separation Rate', fontweight='bold')\n",
    "        ax1.set_title('RQ3: Cluster Separation Rates\\n(Preferred vs Non-Preferred Cavities)', fontweight='bold')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add significance markers\n",
    "        for i, (rate, sig) in enumerate(zip(separation_rates, significance_markers)):\n",
    "            ax1.text(i, rate + 0.05, sig, ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        ax1.set_xticklabels(criteria_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 2: Counts (stacked bar)\n",
    "        width = 0.6\n",
    "        bars2 = ax2.bar(criteria_names, same_cluster_counts, width, label='Same Cluster', color='lightcoral', edgecolor='black')\n",
    "        bars3 = ax2.bar(criteria_names, different_cluster_counts, width, bottom=same_cluster_counts, \n",
    "                       label='Different Clusters', color='lightgreen', edgecolor='black')\n",
    "        \n",
    "        ax2.set_ylabel('Number of Drug-Target Pairs', fontweight='bold')\n",
    "        ax2.set_title('RQ3: Cluster Separation Counts\\n(Preferred vs Non-Preferred Cavities)', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for i, (same, diff) in enumerate(zip(same_cluster_counts, different_cluster_counts)):\n",
    "            total = same + diff\n",
    "            if same > 0:\n",
    "                ax2.text(i, same/2, str(same), ha='center', va='center', fontweight='bold')\n",
    "            if diff > 0:\n",
    "                ax2.text(i, same + diff/2, str(diff), ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        ax2.set_xticklabels(criteria_names, rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        print(f\"\\nüìä RQ3 CLUSTER SEPARATION SUMMARY:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"üìã Analysis Summary:\")\n",
    "        print(\"‚Ä¢ For each drug-target pair, compared cluster IDs of preferred vs non-preferred cavities\")\n",
    "        print(\"‚Ä¢ 'Same cluster': Preferred cavity shares cluster with at least one non-preferred cavity\")\n",
    "        print(\"‚Ä¢ 'Different clusters': Preferred cavity is in a unique cluster from all non-preferred cavities\")\n",
    "        print(\"‚Ä¢ Binomial test: Tests if separation rate differs significantly from 50% (random)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for criterion_key, criterion_info in preference_criteria.items():\n",
    "            if criterion_key in cluster_separation_analysis['separation_rates']:\n",
    "                criterion_name = criterion_info['name']\n",
    "                same_count = cluster_separation_analysis['same_cluster'][criterion_key]\n",
    "                diff_count = cluster_separation_analysis['different_cluster'][criterion_key]\n",
    "                total = same_count + diff_count\n",
    "                sep_rate = cluster_separation_analysis['separation_rates'][criterion_key]\n",
    "                \n",
    "                print(f\"\\n{criterion_name}:\")\n",
    "                print(f\"  Total drug-target pairs: {total}\")\n",
    "                print(f\"  Same cluster: {same_count} ({same_count/total*100:.1f}%)\")\n",
    "                print(f\"  Different clusters: {diff_count} ({diff_count/total*100:.1f}%)\")\n",
    "                print(f\"  Separation rate: {sep_rate:.3f}\")\n",
    "                \n",
    "                if criterion_key in cluster_separation_analysis['statistical_tests']:\n",
    "                    test_data = cluster_separation_analysis['statistical_tests'][criterion_key]\n",
    "                    print(f\"  üìä Binomial test p-value: {test_data['binomial_test_p']:.6f}\")\n",
    "                    print(f\"  üìä Significantly different from random: {'‚úÖ Yes' if test_data['significant'] else '‚ùå No'}\")\n",
    "        \n",
    "        # Overall conclusion\n",
    "        significant_criteria = [k for k in cluster_separation_analysis['statistical_tests'] \n",
    "                              if cluster_separation_analysis['statistical_tests'][k]['significant']]\n",
    "        \n",
    "        print(f\"\\n‚úÖ RQ3 CONCLUSION:\")\n",
    "        if len(significant_criteria) > 0:\n",
    "            print(f\"üéØ Cavity clusters DO show significant separation for {len(significant_criteria)} out of {len(criteria_names)} criteria\")\n",
    "            print(f\"üî• Significant criteria: {[preference_criteria[k]['name'] for k in significant_criteria]}\")\n",
    "        else:\n",
    "            print(f\"üéØ Cavity clusters do NOT show significant separation for any criteria\")\n",
    "        \n",
    "        # Store results globally for potential further analysis\n",
    "        globals()['cluster_separation_analysis'] = cluster_separation_analysis\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No valid criteria found for cluster separation analysis\")\n",
    "    \n",
    "    # Reset matplotlib parameters\n",
    "    plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä RQ3: KDE Plots for Cluster Ratio Distributions\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Creating KDE plots for cluster ratio distributions...\")\n",
    "print(\"üéØ Analysis: For each drug-target pair, calculate ratios of non-preferred cavities\")\n",
    "print(\"   in same vs different clusters compared to the preferred cavity\")\n",
    "\n",
    "# Check if RQ3 analysis exists\n",
    "if 'rq3_analysis' not in globals() or rq3_analysis is None:\n",
    "    print(\"‚ùå RQ3 analysis not found. Please run the previous RQ3 analysis cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ3 analysis found. Calculating cluster ratios for each drug-target pair...\")\n",
    "    \n",
    "    # Set up poster-quality plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,                 # Larger base font size\n",
    "        'axes.titlesize': 22,            # Larger title font\n",
    "        'axes.labelsize': 20,            # Larger axis labels\n",
    "        'xtick.labelsize': 18,           # Larger tick labels\n",
    "        'ytick.labelsize': 18,           # Larger tick labels\n",
    "        'legend.fontsize': 18,           # Larger legend font\n",
    "        'figure.titlesize': 24           # Larger figure title\n",
    "    })\n",
    "    \n",
    "    # Define the preference criteria\n",
    "    preference_criteria = {\n",
    "        'lowest_rmsd': {'name': 'Best Consensus\\n(Min RMSD)', 'metric': 'RMSD (√Ö)'},\n",
    "        'best_gold': {'name': 'Best GOLD\\nScore', 'metric': 'GOLD Score'},\n",
    "        'best_ledock': {'name': 'Best LeDock\\nScore', 'metric': 'LeDock Score'},\n",
    "        'best_smina': {'name': 'Best Smina\\nScore', 'metric': 'Smina Score'}\n",
    "    }\n",
    "    \n",
    "    # Calculate cluster ratios for each drug-target pair\n",
    "    ratio_data = {}\n",
    "    \n",
    "    for criterion_key, criterion_info in preference_criteria.items():\n",
    "        print(f\"\\n--- Calculating ratios for {criterion_info['name'].replace(chr(10), ' ')} ---\")\n",
    "        \n",
    "        # Get the drug-target analysis from RQ3\n",
    "        if criterion_key not in rq3_analysis['drug_target_analysis']:\n",
    "            print(f\"   ‚ö†Ô∏è No analysis data available for {criterion_key}\")\n",
    "            continue\n",
    "        \n",
    "        separation_results = rq3_analysis['drug_target_analysis'][criterion_key]\n",
    "        \n",
    "        # Calculate ratios for each drug-target pair\n",
    "        same_cluster_ratios = []\n",
    "        different_cluster_ratios = []\n",
    "        \n",
    "        for result in separation_results:\n",
    "            drug = result['drug']\n",
    "            target = result['target']\n",
    "            preferred_cluster = result['preferred_cluster']\n",
    "            non_preferred_clusters = result['non_preferred_clusters']\n",
    "            \n",
    "            # Count non-preferred cavities in same vs different clusters\n",
    "            same_cluster_count = non_preferred_clusters.count(preferred_cluster)\n",
    "            different_cluster_count = len(non_preferred_clusters) - same_cluster_count\n",
    "            total_non_preferred = len(non_preferred_clusters)\n",
    "            \n",
    "            # Calculate ratios\n",
    "            if total_non_preferred > 0:\n",
    "                same_cluster_ratio = same_cluster_count / total_non_preferred\n",
    "                different_cluster_ratio = different_cluster_count / total_non_preferred\n",
    "                \n",
    "                same_cluster_ratios.append(same_cluster_ratio)\n",
    "                different_cluster_ratios.append(different_cluster_ratio)\n",
    "        \n",
    "        # Store ratio data\n",
    "        ratio_data[criterion_key] = {\n",
    "            'same_cluster_ratios': same_cluster_ratios,\n",
    "            'different_cluster_ratios': different_cluster_ratios,\n",
    "            'n_drug_target_pairs': len(same_cluster_ratios)\n",
    "        }\n",
    "        \n",
    "        print(f\"   Drug-target pairs analyzed: {len(same_cluster_ratios)}\")\n",
    "        if len(same_cluster_ratios) > 0:\n",
    "            print(f\"   Same cluster ratios: mean={np.mean(same_cluster_ratios):.3f}, std={np.std(same_cluster_ratios):.3f}\")\n",
    "            print(f\"   Different cluster ratios: mean={np.mean(different_cluster_ratios):.3f}, std={np.std(different_cluster_ratios):.3f}\")\n",
    "    \n",
    "    # Create split violin plots\n",
    "    valid_criteria = [(k, v) for k, v in preference_criteria.items() if k in ratio_data and ratio_data[k]['n_drug_target_pairs'] > 0]\n",
    "    \n",
    "    if len(valid_criteria) == 0:\n",
    "        print(\"‚ùå No valid criteria with ratio data found\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Found {len(valid_criteria)} criteria with ratio data\")\n",
    "        \n",
    "        # Create 2x2 subplot grid for ratio distribution plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()  # Flatten for easy indexing\n",
    "        \n",
    "        # Loop over each valid preference criterion\n",
    "        for idx, (criterion_key, criterion_info) in enumerate(valid_criteria):\n",
    "            if idx >= 4:  # Only show first 4 criteria\n",
    "                break\n",
    "                \n",
    "            ax = axes[idx]\n",
    "            ratios = ratio_data[criterion_key]\n",
    "            criterion_name = criterion_info['name']\n",
    "            \n",
    "            # Prepare data for split violin plot\n",
    "            plot_data = []\n",
    "            \n",
    "            # Add same cluster ratios\n",
    "            for ratio in ratios['same_cluster_ratios']:\n",
    "                plot_data.append({'Cluster_Type': 'Same Cluster', 'Ratio': ratio})\n",
    "            \n",
    "            # Add different cluster ratios\n",
    "            for ratio in ratios['different_cluster_ratios']:\n",
    "                plot_data.append({'Cluster_Type': 'Different Cluster', 'Ratio': ratio})\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if len(plot_data) > 0:\n",
    "                df_ratios = pd.DataFrame(plot_data)\n",
    "                \n",
    "                # Check if we have both types of data\n",
    "                ratio_types = df_ratios['Cluster_Type'].unique()\n",
    "                if len(ratio_types) > 1:\n",
    "                    try:\n",
    "                        # Create KDE plot using seaborn with clear outlines\n",
    "                        # First create the filled areas\n",
    "                        sns.kdeplot(\n",
    "                            data=df_ratios,\n",
    "                            x='Ratio',\n",
    "                            hue='Cluster_Type',\n",
    "                            ax=ax,\n",
    "                            palette=['lightcoral', 'lightblue'],\n",
    "                            fill=True,\n",
    "                            alpha=0.6,\n",
    "                            common_norm=False,\n",
    "                            legend=False  # We'll add legend manually\n",
    "                        )\n",
    "                        \n",
    "                        # Then add the outlines with distinct colors\n",
    "                        for cluster_type, color in zip(['Same Cluster', 'Different Cluster'], ['darkred', 'darkblue']):\n",
    "                            subset = df_ratios[df_ratios['Cluster_Type'] == cluster_type]\n",
    "                            sns.kdeplot(\n",
    "                                data=subset,\n",
    "                                x='Ratio',\n",
    "                                ax=ax,\n",
    "                                color=color,\n",
    "                                fill=False,\n",
    "                                linewidth=3,\n",
    "                                label=cluster_type\n",
    "                            )\n",
    "                        \n",
    "                        # Customize subplot\n",
    "                        ax.set_title(f'{criterion_name}', fontsize=22, fontweight='bold', pad=20)\n",
    "                        ax.set_xlabel('Ratio of Non-Preferred Cavities', fontsize=20, fontweight='bold')\n",
    "                        ax.set_ylabel('Density', fontsize=20, fontweight='bold')\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        ax.set_xlim(0, 1.05)  # Ratios are between 0 and 1\n",
    "                        \n",
    "                        # Add vertical line at 0.5 for reference\n",
    "                        ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Equal Split (0.5)')\n",
    "                        \n",
    "                        # Customize legend\n",
    "                        handles, labels = ax.get_legend_handles_labels()\n",
    "                        ax.legend(handles, labels, loc='best', fontsize=12)\n",
    "                        \n",
    "                        # Statistical comparison\n",
    "                        same_ratios = np.array(ratios['same_cluster_ratios'])\n",
    "                        diff_ratios = np.array(ratios['different_cluster_ratios'])\n",
    "                        \n",
    "                        if len(same_ratios) > 0 and len(diff_ratios) > 0:\n",
    "                            # Mann-Whitney U test\n",
    "                            from scipy.stats import mannwhitneyu\n",
    "                            try:\n",
    "                                stat, p_val = mannwhitneyu(same_ratios, diff_ratios, alternative='two-sided')\n",
    "                                \n",
    "                                # Determine significance\n",
    "                                if p_val < 0.001:\n",
    "                                    significance = \"***\"\n",
    "                                elif p_val < 0.01:\n",
    "                                    significance = \"**\"\n",
    "                                elif p_val < 0.05:\n",
    "                                    significance = \"*\"\n",
    "                                else:\n",
    "                                    significance = \"ns\"\n",
    "                                \n",
    "                                # Add statistical annotation\n",
    "                                ax.text(0.5, 0.95, f'p = {p_val:.3f} {significance}', \n",
    "                                       transform=ax.transAxes, ha='center', va='top', \n",
    "                                       fontsize=18, fontweight='bold', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"   ‚ö†Ô∏è Statistical test failed for {criterion_name}: {e}\")\n",
    "                        \n",
    "                        print(f\"‚úÖ Created ratio KDE plot for {criterion_name.replace(chr(10), ' ')}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error creating ratio KDE plot for {criterion_name}: {e}\")\n",
    "                        ax.text(0.5, 0.5, f'Error creating plot\\n{str(e)[:50]}...', \n",
    "                               ha='center', va='center', transform=ax.transAxes, \n",
    "                               fontsize=16, fontweight='bold')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, f'Only {ratio_types[0]} data available', \n",
    "                           ha='center', va='center', transform=ax.transAxes, \n",
    "                           fontsize=18, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'No ratio data available', \n",
    "                       ha='center', va='center', transform=ax.transAxes, \n",
    "                       fontsize=18, fontweight='bold')\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for idx in range(len(valid_criteria), 4):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        # Add overall title\n",
    "        fig.suptitle('Same vs Different Cluster Ratios for Each Drug-Target Pair', \n",
    "                    fontsize=24, fontweight='bold', y=0.98)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.88)  # Make room for suptitle\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary statistics\n",
    "        print(f\"\\nüìä RQ3 CLUSTER RATIO ANALYSIS SUMMARY:\")\n",
    "        print(\"=\" * 90)\n",
    "        print(\"üìã Analysis Logic:\")\n",
    "        print(\"‚Ä¢ For each drug-target pair with multiple cavities:\")\n",
    "        print(\"  - Identify 1 preferred cavity (based on each criterion)\")\n",
    "        print(\"  - Calculate ratios of non-preferred cavities in same vs different clusters\")\n",
    "        print(\"  - Same cluster ratio: (# non-preferred in preferred's cluster) / (total # non-preferred)\")\n",
    "        print(\"  - Different cluster ratio: (# non-preferred in other clusters) / (total # non-preferred)\")\n",
    "        print(\"‚Ä¢ Statistical test: Mann-Whitney U test comparing same vs different cluster ratios\")\n",
    "        print(\"=\" * 90)\n",
    "        \n",
    "        for criterion_key, criterion_info in valid_criteria:\n",
    "            ratios = ratio_data[criterion_key]\n",
    "            criterion_name = criterion_info['name'].replace('\\n', ' ')\n",
    "            \n",
    "            same_ratios = np.array(ratios['same_cluster_ratios'])\n",
    "            diff_ratios = np.array(ratios['different_cluster_ratios'])\n",
    "            \n",
    "            print(f\"\\n{criterion_name}:\")\n",
    "            print(f\"  Drug-target pairs analyzed: {ratios['n_drug_target_pairs']}\")\n",
    "            print(f\"  Same cluster ratios: mean={np.mean(same_ratios):.3f}, median={np.median(same_ratios):.3f}\")\n",
    "            print(f\"  Different cluster ratios: mean={np.mean(diff_ratios):.3f}, median={np.median(diff_ratios):.3f}\")\n",
    "            \n",
    "            # Statistical comparison\n",
    "            if len(same_ratios) > 0 and len(diff_ratios) > 0:\n",
    "                from scipy.stats import mannwhitneyu\n",
    "                try:\n",
    "                    stat, p_val = mannwhitneyu(same_ratios, diff_ratios, alternative='two-sided')\n",
    "                    print(f\"  üìä Mann-Whitney U test p-value: {p_val:.6f}\")\n",
    "                    print(f\"  üìä Significant difference: {'‚úÖ Yes' if p_val < 0.05 else '‚ùå No'}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è Statistical test failed: {e}\")\n",
    "        \n",
    "        # Overall interpretation\n",
    "        print(f\"\\n‚úÖ RQ3 INTERPRETATION:\")\n",
    "        print(\"üéØ If cavities cluster meaningfully:\")\n",
    "        print(\"   - Same cluster ratios should be LOW (preferred and non-preferred in different clusters)\")\n",
    "        print(\"   - Different cluster ratios should be HIGH (good separation)\")\n",
    "        print(\"üéØ If clustering is random:\")\n",
    "        print(\"   - Both ratios should be around 0.5 (no meaningful separation)\")\n",
    "        \n",
    "        # Store results globally\n",
    "        globals()['cluster_ratio_analysis'] = ratio_data\n",
    "        \n",
    "    # Reset matplotlib parameters\n",
    "    plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0898d",
   "metadata": {},
   "source": [
    "## Research Question 4: Can Scoring Metrics Separate Positive from Negative Samples?\n",
    "\n",
    "**Research Question 4:** *Do Best RMSD, Best GOLD score, Best SMINA score, or Best LeDock score effectively separate positive (known drug-target) from negative (balanced negative) samples?*\n",
    "\n",
    "### üéØ Specific Analysis Goal:\n",
    "For each scoring metric:\n",
    "1. **Extract best scores** for each drug-target-cavity combination\n",
    "2. **Compare score distributions** between positive and negative samples\n",
    "3. **Calculate separation metrics**: Effect size (Cohen's d), AUC-ROC\n",
    "4. **Test statistical significance**: Mann-Whitney U test, Kolmogorov-Smirnov test\n",
    "5. **Visualize separation**: Distribution plots, ROC curves\n",
    "\n",
    "### üìä Key Metrics:\n",
    "- **Effect Size (Cohen's d)**: Standardized mean difference between positive and negative samples\n",
    "- **AUC-ROC**: Area under the receiver operating characteristic curve\n",
    "- **Mann-Whitney U p-value**: Statistical significance of distribution differences\n",
    "- **Separation quality**: Poor (AUC < 0.6), Fair (0.6-0.7), Good (0.7-0.8), Excellent (> 0.8)\n",
    "\n",
    "### üß© Scoring Metrics to Test:\n",
    "1. **Best RMSD** (√Ö): Lower is better (closer structural agreement)\n",
    "2. **Best GOLD Score**: Higher is better (stronger predicted binding)\n",
    "3. **Best SMINA Score** (kcal/mol): Lower (more negative) is better\n",
    "4. **Best LeDock Score** (kcal/mol): Lower (more negative) is better\n",
    "\n",
    "### üìà Main Visualizations:\n",
    "- **Violin plots** showing score distributions for positive vs negative samples\n",
    "- **ROC curves** demonstrating classification performance\n",
    "- **Box plots** with statistical significance markers\n",
    "- **Density plots** with overlays for direct comparison\n",
    "\n",
    "This analysis addresses whether computational docking scores can distinguish true drug-target interactions from non-interacting decoys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üî¨ RQ4: Scoring Metrics Separation Analysis - Positive vs Negative Samples\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç RESEARCH QUESTION 4: Can scoring metrics separate positive from negative samples?\")\n",
    "print(\"=\" * 85)\n",
    "print(\"üéØ SPECIFIC QUESTION: Do Best RMSD, Best GOLD, Best SMINA, or Best LeDock scores\")\n",
    "print(\"   effectively distinguish known drug-target interactions from negative controls?\")\n",
    "\n",
    "# Check if sample_type column exists\n",
    "if 'sample_type' not in combined_results.columns:\n",
    "    print(\"‚ùå Sample type annotation not found. Please run the sample type annotation cell first.\")\n",
    "else:\n",
    "    # Check if we have both positive and negative samples\n",
    "    sample_types = combined_results['sample_type'].unique().to_list()\n",
    "    print(f\"\\nüìä Sample types found: {sample_types}\")\n",
    "    \n",
    "    has_positive = 'positive' in sample_types\n",
    "    has_negative = any('negative' in str(st).lower() for st in sample_types if st is not None)\n",
    "    \n",
    "    if not has_positive:\n",
    "        print(\"‚ùå No positive samples found in the dataset\")\n",
    "    elif not has_negative:\n",
    "        print(\"‚ùå No negative samples found in the dataset\")\n",
    "        print(\"   Note: Negative sample data may need to be loaded. See earlier diagnostics.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Both positive and negative samples found. Proceeding with RQ4 analysis...\")\n",
    "        \n",
    "        # Initialize RQ4 analysis storage\n",
    "        rq4_analysis = {\n",
    "            'score_distributions': {},\n",
    "            'statistical_tests': {},\n",
    "            'effect_sizes': {},\n",
    "            'roc_analysis': {},\n",
    "            'best_scores_by_sample': {}\n",
    "        }\n",
    "        \n",
    "        # Define scoring metrics to analyze\n",
    "        scoring_metrics = {\n",
    "            'best_rmsd': {\n",
    "                'name': 'Best RMSD',\n",
    "                'column': 'RMSD',\n",
    "                'aggregation': 'min',  # Lower is better\n",
    "                'unit': '√Ö',\n",
    "                'better_direction': 'lower'\n",
    "            },\n",
    "            'best_gold': {\n",
    "                'name': 'Best GOLD Score',\n",
    "                'column': 'Score2',  # GOLD is typically Score2\n",
    "                'aggregation': 'max',  # Higher is better\n",
    "                'unit': '',\n",
    "                'better_direction': 'higher'\n",
    "            },\n",
    "            'best_smina': {\n",
    "                'name': 'Best SMINA Score',\n",
    "                'column': 'Score1',  # SMINA is typically Score1\n",
    "                'aggregation': 'min',  # More negative is better\n",
    "                'unit': 'kcal/mol',\n",
    "                'better_direction': 'lower'\n",
    "            },\n",
    "            'best_ledock': {\n",
    "                'name': 'Best LeDock Score',\n",
    "                'column': 'LeDock_Score',\n",
    "                'aggregation': 'min',  # More negative is better\n",
    "                'unit': 'kcal/mol',\n",
    "                'better_direction': 'lower'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüîç Analyzing {len(scoring_metrics)} scoring metrics...\")\n",
    "        print(\"üìã Analysis steps:\")\n",
    "        print(\"   1. Extract best scores for each drug-target-cavity combination\")\n",
    "        print(\"   2. Separate by positive vs negative sample type\")\n",
    "        print(\"   3. Calculate statistical measures and effect sizes\")\n",
    "        print(\"   4. Perform ROC analysis for classification performance\")\n",
    "        \n",
    "        # For each scoring metric, extract best scores and analyze separation\n",
    "        for metric_key, metric_info in scoring_metrics.items():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Analyzing: {metric_info['name']}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            column = metric_info['column']\n",
    "            \n",
    "            # Check if column exists\n",
    "            if column not in combined_results.columns:\n",
    "                print(f\"   ‚ö†Ô∏è Column '{column}' not found. Skipping {metric_info['name']}\")\n",
    "                continue\n",
    "            \n",
    "            # Filter for non-null scores and valid sample types\n",
    "            valid_data = combined_results.filter(\n",
    "                (pl.col(column).is_not_null()) &\n",
    "                (pl.col('sample_type').is_not_null())\n",
    "            )\n",
    "            \n",
    "            if valid_data.is_empty():\n",
    "                print(f\"   ‚ö†Ô∏è No valid data for {metric_info['name']}\")\n",
    "                continue\n",
    "            \n",
    "            # Get best score for each drug-target-cavity combination\n",
    "            group_cols = ['drugbank_id', 'uniprot_id', 'cavity_index', 'sample_type']\n",
    "            \n",
    "            if metric_info['aggregation'] == 'min':\n",
    "                best_scores = valid_data.group_by(group_cols).agg([\n",
    "                    pl.col(column).min().alias('best_score')\n",
    "                ])\n",
    "            else:  # max\n",
    "                best_scores = valid_data.group_by(group_cols).agg([\n",
    "                    pl.col(column).max().alias('best_score')\n",
    "                ])\n",
    "            \n",
    "            # Separate positive and negative samples\n",
    "            positive_scores = best_scores.filter(\n",
    "                pl.col('sample_type') == 'positive'\n",
    "            )['best_score'].to_numpy()\n",
    "            \n",
    "            negative_scores = best_scores.filter(\n",
    "                pl.col('sample_type').str.contains('negative')\n",
    "            )['best_score'].to_numpy()\n",
    "            \n",
    "            # Remove NaN values\n",
    "            positive_scores = positive_scores[~np.isnan(positive_scores)]\n",
    "            negative_scores = negative_scores[~np.isnan(negative_scores)]\n",
    "            \n",
    "            print(f\"   Positive samples: {len(positive_scores):,} drug-target-cavity combinations\")\n",
    "            print(f\"   Negative samples: {len(negative_scores):,} drug-target-cavity combinations\")\n",
    "            \n",
    "            if len(positive_scores) == 0 or len(negative_scores) == 0:\n",
    "                print(f\"   ‚ö†Ô∏è Insufficient data for comparison\")\n",
    "                continue\n",
    "            \n",
    "            # Store distributions\n",
    "            rq4_analysis['score_distributions'][metric_key] = {\n",
    "                'positive': positive_scores,\n",
    "                'negative': negative_scores,\n",
    "                'metric_info': metric_info\n",
    "            }\n",
    "            \n",
    "            # Calculate descriptive statistics\n",
    "            pos_mean = np.mean(positive_scores)\n",
    "            pos_std = np.std(positive_scores)\n",
    "            pos_median = np.median(positive_scores)\n",
    "            \n",
    "            neg_mean = np.mean(negative_scores)\n",
    "            neg_std = np.std(negative_scores)\n",
    "            neg_median = np.median(negative_scores)\n",
    "            \n",
    "            print(f\"\\n   üìä Positive samples:\")\n",
    "            print(f\"      Mean: {pos_mean:.3f} {metric_info['unit']}\")\n",
    "            print(f\"      Std:  {pos_std:.3f}\")\n",
    "            print(f\"      Median: {pos_median:.3f} {metric_info['unit']}\")\n",
    "            \n",
    "            print(f\"\\n   üìä Negative samples:\")\n",
    "            print(f\"      Mean: {neg_mean:.3f} {metric_info['unit']}\")\n",
    "            print(f\"      Std:  {neg_std:.3f}\")\n",
    "            print(f\"      Median: {neg_median:.3f} {metric_info['unit']}\")\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((pos_std**2 + neg_std**2) / 2)\n",
    "            cohens_d = (pos_mean - neg_mean) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            rq4_analysis['effect_sizes'][metric_key] = {\n",
    "                'cohens_d': cohens_d,\n",
    "                'interpretation': 'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n   üìä Effect Size (Cohen's d): {cohens_d:.3f}\")\n",
    "            print(f\"      Interpretation: {rq4_analysis['effect_sizes'][metric_key]['interpretation']}\")\n",
    "            \n",
    "            # Statistical tests\n",
    "            from scipy.stats import mannwhitneyu, ks_2samp\n",
    "            \n",
    "            # Mann-Whitney U test\n",
    "            mw_stat, mw_pval = mannwhitneyu(positive_scores, negative_scores, alternative='two-sided')\n",
    "            \n",
    "            # Kolmogorov-Smirnov test\n",
    "            ks_stat, ks_pval = ks_2samp(positive_scores, negative_scores)\n",
    "            \n",
    "            rq4_analysis['statistical_tests'][metric_key] = {\n",
    "                'mann_whitney': {'statistic': mw_stat, 'p_value': mw_pval},\n",
    "                'kolmogorov_smirnov': {'statistic': ks_stat, 'p_value': ks_pval}\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n   üìä Statistical Tests:\")\n",
    "            print(f\"      Mann-Whitney U: p = {mw_pval:.6f} {'***' if mw_pval < 0.001 else '**' if mw_pval < 0.01 else '*' if mw_pval < 0.05 else 'ns'}\")\n",
    "            print(f\"      Kolmogorov-Smirnov: p = {ks_pval:.6f} {'***' if ks_pval < 0.001 else '**' if ks_pval < 0.01 else '*' if ks_pval < 0.05 else 'ns'}\")\n",
    "            \n",
    "            # ROC Analysis\n",
    "            from sklearn.metrics import roc_curve, auc\n",
    "            \n",
    "            # Prepare labels (1 for positive, 0 for negative)\n",
    "            y_true = np.concatenate([\n",
    "                np.ones(len(positive_scores)),\n",
    "                np.zeros(len(negative_scores))\n",
    "            ])\n",
    "            \n",
    "            # Prepare scores (need to flip if lower is better)\n",
    "            if metric_info['better_direction'] == 'lower':\n",
    "                # For metrics where lower is better (RMSD, SMINA, LeDock),\n",
    "                # use negative scores so that better scores are higher\n",
    "                y_scores = np.concatenate([\n",
    "                    -positive_scores,\n",
    "                    -negative_scores\n",
    "                ])\n",
    "            else:\n",
    "                # For metrics where higher is better (GOLD)\n",
    "                y_scores = np.concatenate([\n",
    "                    positive_scores,\n",
    "                    negative_scores\n",
    "                ])\n",
    "            \n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            rq4_analysis['roc_analysis'][metric_key] = {\n",
    "                'fpr': fpr,\n",
    "                'tpr': tpr,\n",
    "                'thresholds': thresholds,\n",
    "                'auc': roc_auc\n",
    "            }\n",
    "            \n",
    "            # Interpret AUC\n",
    "            if roc_auc < 0.6:\n",
    "                auc_interpretation = 'Poor'\n",
    "            elif roc_auc < 0.7:\n",
    "                auc_interpretation = 'Fair'\n",
    "            elif roc_auc < 0.8:\n",
    "                auc_interpretation = 'Good'\n",
    "            elif roc_auc < 0.9:\n",
    "                auc_interpretation = 'Excellent'\n",
    "            else:\n",
    "                auc_interpretation = 'Outstanding'\n",
    "            \n",
    "            print(f\"\\n   üìä ROC Analysis:\")\n",
    "            print(f\"      AUC-ROC: {roc_auc:.3f}\")\n",
    "            print(f\"      Classification quality: {auc_interpretation}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ RQ4 Analysis Complete!\")\n",
    "        print(f\"   Analyzed {len(rq4_analysis['score_distributions'])} scoring metrics\")\n",
    "        \n",
    "        # Store globally\n",
    "        globals()['rq4_analysis'] = rq4_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä RQ4: Visualization - Distribution Plots and ROC Curves\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Creating visualizations for RQ4: Positive vs Negative Sample Separation...\")\n",
    "\n",
    "# Check if RQ4 analysis exists\n",
    "if 'rq4_analysis' not in globals() or rq4_analysis is None:\n",
    "    print(\"‚ùå RQ4 analysis not found. Please run the previous RQ4 analysis cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ RQ4 analysis found. Creating comprehensive visualizations...\")\n",
    "    \n",
    "    # Set up poster-quality plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 16,\n",
    "        'axes.titlesize': 18,\n",
    "        'axes.labelsize': 16,\n",
    "        'xtick.labelsize': 14,\n",
    "        'ytick.labelsize': 14,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "    \n",
    "    # Get valid metrics\n",
    "    valid_metrics = [(k, v) for k, v in rq4_analysis['score_distributions'].items()]\n",
    "    \n",
    "    if len(valid_metrics) == 0:\n",
    "        print(\"‚ùå No valid metrics found for visualization\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(valid_metrics)} metrics to visualize\")\n",
    "        \n",
    "        # Create figure with subplots: Distribution plots (top) and ROC curves (bottom)\n",
    "        n_metrics = len(valid_metrics)\n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Create grid: 2 rows (distributions, ROC curves)\n",
    "        gs = fig.add_gridspec(2, n_metrics, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Loop through each metric\n",
    "        for idx, (metric_key, dist_data) in enumerate(valid_metrics):\n",
    "            metric_info = dist_data['metric_info']\n",
    "            positive_scores = dist_data['positive']\n",
    "            negative_scores = dist_data['negative']\n",
    "            \n",
    "            # === Top row: Violin/Box plots ===\n",
    "            ax_dist = fig.add_subplot(gs[0, idx])\n",
    "            \n",
    "            # Prepare data for plotting\n",
    "            plot_data = []\n",
    "            for score in positive_scores:\n",
    "                plot_data.append({'Sample Type': 'Positive', 'Score': score})\n",
    "            for score in negative_scores:\n",
    "                plot_data.append({'Sample Type': 'Negative', 'Score': score})\n",
    "            \n",
    "            df_plot = pd.DataFrame(plot_data)\n",
    "            \n",
    "            # Create violin plot\n",
    "            sns.violinplot(\n",
    "                data=df_plot,\n",
    "                x='Sample Type',\n",
    "                y='Score',\n",
    "                ax=ax_dist,\n",
    "                palette=['lightgreen', 'lightcoral'],\n",
    "                inner='box',\n",
    "                cut=0\n",
    "            )\n",
    "            \n",
    "            # Customize distribution plot\n",
    "            ax_dist.set_title(f'{metric_info[\"name\"]}', fontsize=18, fontweight='bold')\n",
    "            ax_dist.set_ylabel(f'Score ({metric_info[\"unit\"]})', fontsize=16, fontweight='bold')\n",
    "            ax_dist.set_xlabel('Sample Type', fontsize=16, fontweight='bold')\n",
    "            ax_dist.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add statistical significance\n",
    "            if metric_key in rq4_analysis['statistical_tests']:\n",
    "                p_val = rq4_analysis['statistical_tests'][metric_key]['mann_whitney']['p_value']\n",
    "                if p_val < 0.001:\n",
    "                    sig_marker = '***'\n",
    "                elif p_val < 0.01:\n",
    "                    sig_marker = '**'\n",
    "                elif p_val < 0.05:\n",
    "                    sig_marker = '*'\n",
    "                else:\n",
    "                    sig_marker = 'ns'\n",
    "                \n",
    "                # Add significance annotation\n",
    "                y_max = df_plot['Score'].max()\n",
    "                y_min = df_plot['Score'].min()\n",
    "                y_range = y_max - y_min\n",
    "                \n",
    "                ax_dist.plot([0, 1], [y_max + 0.05 * y_range, y_max + 0.05 * y_range], \n",
    "                           'k-', linewidth=2)\n",
    "                ax_dist.text(0.5, y_max + 0.07 * y_range, sig_marker, \n",
    "                           ha='center', va='bottom', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Add effect size\n",
    "            if metric_key in rq4_analysis['effect_sizes']:\n",
    "                cohens_d = rq4_analysis['effect_sizes'][metric_key]['cohens_d']\n",
    "                ax_dist.text(0.02, 0.98, f\"Cohen's d = {cohens_d:.3f}\", \n",
    "                           transform=ax_dist.transAxes, ha='left', va='top',\n",
    "                           fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                           facecolor=\"white\", alpha=0.8))\n",
    "            \n",
    "            # === Bottom row: ROC curves ===\n",
    "            ax_roc = fig.add_subplot(gs[1, idx])\n",
    "            \n",
    "            if metric_key in rq4_analysis['roc_analysis']:\n",
    "                roc_data = rq4_analysis['roc_analysis'][metric_key]\n",
    "                fpr = roc_data['fpr']\n",
    "                tpr = roc_data['tpr']\n",
    "                roc_auc = roc_data['auc']\n",
    "                \n",
    "                # Plot ROC curve\n",
    "                ax_roc.plot(fpr, tpr, color='darkblue', linewidth=3, \n",
    "                          label=f'AUC = {roc_auc:.3f}')\n",
    "                \n",
    "                # Plot diagonal (random classifier)\n",
    "                ax_roc.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5, \n",
    "                          label='Random (AUC = 0.50)')\n",
    "                \n",
    "                # Customize ROC plot\n",
    "                ax_roc.set_xlabel('False Positive Rate', fontsize=16, fontweight='bold')\n",
    "                ax_roc.set_ylabel('True Positive Rate', fontsize=16, fontweight='bold')\n",
    "                ax_roc.set_title(f'ROC Curve - {metric_info[\"name\"]}', \n",
    "                               fontsize=18, fontweight='bold')\n",
    "                ax_roc.grid(True, alpha=0.3)\n",
    "                ax_roc.legend(loc='lower right', fontsize=12)\n",
    "                ax_roc.set_xlim([0, 1])\n",
    "                ax_roc.set_ylim([0, 1])\n",
    "                \n",
    "                # Add AUC interpretation\n",
    "                if roc_auc < 0.6:\n",
    "                    auc_interp = 'Poor'\n",
    "                    color = 'red'\n",
    "                elif roc_auc < 0.7:\n",
    "                    auc_interp = 'Fair'\n",
    "                    color = 'orange'\n",
    "                elif roc_auc < 0.8:\n",
    "                    auc_interp = 'Good'\n",
    "                    color = 'yellow'\n",
    "                elif roc_auc < 0.9:\n",
    "                    auc_interp = 'Excellent'\n",
    "                    color = 'lightgreen'\n",
    "                else:\n",
    "                    auc_interp = 'Outstanding'\n",
    "                    color = 'darkgreen'\n",
    "                \n",
    "                ax_roc.text(0.98, 0.02, auc_interp, transform=ax_roc.transAxes,\n",
    "                          ha='right', va='bottom', fontsize=14, fontweight='bold',\n",
    "                          bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=color, alpha=0.7))\n",
    "        \n",
    "        # Add overall title\n",
    "        fig.suptitle('RQ4: Scoring Metrics Separation - Positive vs Negative Samples', \n",
    "                    fontsize=22, fontweight='bold', y=0.98)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.95)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        print(f\"\\nüìä RQ4 SEPARATION ANALYSIS SUMMARY:\")\n",
    "        print(\"=\" * 90)\n",
    "        \n",
    "        for metric_key, dist_data in valid_metrics:\n",
    "            metric_info = dist_data['metric_info']\n",
    "            print(f\"\\n{metric_info['name']} ({metric_info['unit']}):\")\n",
    "            \n",
    "            # Effect size\n",
    "            if metric_key in rq4_analysis['effect_sizes']:\n",
    "                cohens_d = rq4_analysis['effect_sizes'][metric_key]['cohens_d']\n",
    "                interpretation = rq4_analysis['effect_sizes'][metric_key]['interpretation']\n",
    "                print(f\"  Effect Size: Cohen's d = {cohens_d:.3f} ({interpretation})\")\n",
    "            \n",
    "            # Statistical tests\n",
    "            if metric_key in rq4_analysis['statistical_tests']:\n",
    "                mw_pval = rq4_analysis['statistical_tests'][metric_key]['mann_whitney']['p_value']\n",
    "                ks_pval = rq4_analysis['statistical_tests'][metric_key]['kolmogorov_smirnov']['p_value']\n",
    "                print(f\"  Mann-Whitney U p-value: {mw_pval:.6f}\")\n",
    "                print(f\"  Kolmogorov-Smirnov p-value: {ks_pval:.6f}\")\n",
    "            \n",
    "            # ROC AUC\n",
    "            if metric_key in rq4_analysis['roc_analysis']:\n",
    "                roc_auc = rq4_analysis['roc_analysis'][metric_key]['auc']\n",
    "                if roc_auc < 0.6:\n",
    "                    quality = 'Poor'\n",
    "                elif roc_auc < 0.7:\n",
    "                    quality = 'Fair'\n",
    "                elif roc_auc < 0.8:\n",
    "                    quality = 'Good'\n",
    "                elif roc_auc < 0.9:\n",
    "                    quality = 'Excellent'\n",
    "                else:\n",
    "                    quality = 'Outstanding'\n",
    "                print(f\"  AUC-ROC: {roc_auc:.3f} ({quality} separation)\")\n",
    "        \n",
    "        print(f\"\\n{'='*90}\")\n",
    "        print(\"‚úÖ RQ4 INTERPRETATION:\")\n",
    "        print(\"üéØ High AUC (> 0.7): Scoring metric can distinguish positive from negative samples\")\n",
    "        print(\"üéØ Low AUC (< 0.6): Scoring metric has poor discriminative power\")\n",
    "        print(\"üéØ Large effect size: Substantial difference in score distributions\")\n",
    "        print(\"üéØ Significant p-value: Distributions are statistically different\")\n",
    "        \n",
    "    # Reset matplotlib parameters\n",
    "    plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711fba8",
   "metadata": {},
   "source": [
    "## Research Question 5: Does Consensus Pose Selection Improve Scoring Reliability?\n",
    "\n",
    "**Research Question 5:** *Does consensus between docking tools improve the reliability of scoring functions for ranking compounds and discriminating positive from negative samples?*\n",
    "\n",
    "This analysis investigates whether pose agreement (consensus) between different docking tools correlates with more reliable scoring predictions.\n",
    "\n",
    "### üéØ Analysis Goals:\n",
    "1. **Stratify poses by consensus quality** based on RMSD between tool predictions\n",
    "2. **Evaluate score-score correlations** within each consensus tier\n",
    "3. **Assess discrimination power** for positive vs negative samples at each tier\n",
    "4. **Determine optimal consensus threshold** for reliable predictions\n",
    "\n",
    "### üìä Consensus Tiers:\n",
    "- **Tier 1 (Excellent)**: RMSD < 1.5 √Ö - All 3 tools strongly agree\n",
    "- **Tier 2 (Moderate)**: 1.5-2.5 √Ö - 2 tools agree reasonably well\n",
    "- **Tier 3 (Weak/None)**: RMSD > 2.5 √Ö - Poor or no consensus\n",
    "\n",
    "### üìà Key Metrics:\n",
    "- **Score-Score Correlation**: Pearson correlation between GOLD, LeDock, and Smina scores within each tier\n",
    "- **ROC/AUC Analysis**: Discrimination of positive vs negative samples for each tool at each tier\n",
    "- **Effect Size**: How much consensus improves predictive reliability\n",
    "\n",
    "### üí° Expected Insight:\n",
    "If consensus improves reliability, we expect:\n",
    "- Higher score-score correlations in Tier 1 vs Tier 3\n",
    "- Better positive/negative discrimination (higher AUC) in Tier 1 vs Tier 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üî¨ RQ5: Consensus Quality Analysis - Does Consensus Improve Scoring Reliability?\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç RESEARCH QUESTION 5: Does consensus improve scoring reliability?\")\n",
    "print(\"=\" * 85)\n",
    "print(\"üéØ ANALYSIS: Stratify by consensus quality and evaluate scoring reliability\")\n",
    "print()\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['RMSD', 'Score1', 'Score2', 'Tool1', 'Tool2', 'sample_type', \n",
    "                 'drugbank_id', 'uniprot_id', 'cavity_index']\n",
    "missing_cols = [col for col in required_cols if col not in combined_results.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required columns present. Proceeding with RQ5 analysis...\")\n",
    "    \n",
    "    # Initialize RQ5 analysis storage\n",
    "    rq5_analysis = {\n",
    "        'consensus_tiers': {},\n",
    "        'score_correlations': {},\n",
    "        'roc_analysis': {},\n",
    "        'tier_statistics': {}\n",
    "    }\n",
    "    \n",
    "    # Define consensus tiers based on RMSD\n",
    "    tier_definitions = {\n",
    "        'tier1_excellent': {'name': 'Tier 1: Excellent (RMSD < 1.5√Ö)', 'min': 0, 'max': 1.5},\n",
    "        'tier2_moderate': {'name': 'Tier 2: Moderate (1.5-2.5√Ö)', 'min': 1.5, 'max': 2.5},\n",
    "        'tier3_weak': {'name': 'Tier 3: Weak (RMSD > 2.5√Ö)', 'min': 2.5, 'max': float('inf')}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä STEP 1: Stratify poses by consensus quality (RMSD)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # For each drug-target-cavity combination, get the best (lowest) RMSD\n",
    "    # This represents the best consensus across all tool pairs\n",
    "    best_consensus = combined_results.group_by(['drugbank_id', 'uniprot_id', 'cavity_index']).agg([\n",
    "        pl.col('RMSD').min().alias('best_rmsd'),\n",
    "        pl.col('sample_type').first().alias('sample_type'),\n",
    "        # Get scores - we'll need to figure out which tool is which\n",
    "        pl.col('Score1').first().alias('score1'),\n",
    "        pl.col('Score2').first().alias('score2'),\n",
    "        pl.col('Tool1').first().alias('tool1'),\n",
    "        pl.col('Tool2').first().alias('tool2')\n",
    "    ])\n",
    "    \n",
    "    print(f\"   Total drug-target-cavity combinations: {best_consensus.height:,}\")\n",
    "    \n",
    "    # Assign tiers\n",
    "    for tier_key, tier_def in tier_definitions.items():\n",
    "        tier_data = best_consensus.filter(\n",
    "            (pl.col('best_rmsd') >= tier_def['min']) & \n",
    "            (pl.col('best_rmsd') < tier_def['max'])\n",
    "        )\n",
    "        \n",
    "        rq5_analysis['consensus_tiers'][tier_key] = {\n",
    "            'name': tier_def['name'],\n",
    "            'data': tier_data,\n",
    "            'count': tier_data.height,\n",
    "            'rmsd_range': (tier_def['min'], tier_def['max'])\n",
    "        }\n",
    "        \n",
    "        # Count positive and negative samples\n",
    "        if 'sample_type' in tier_data.columns:\n",
    "            pos_count = tier_data.filter(pl.col('sample_type') == 'positive').height\n",
    "            neg_count = tier_data.filter(pl.col('sample_type').str.contains('negative')).height\n",
    "            \n",
    "            print(f\"\\n   {tier_def['name']}:\")\n",
    "            print(f\"      Total: {tier_data.height:,}\")\n",
    "            print(f\"      Positive: {pos_count:,}\")\n",
    "            print(f\"      Negative: {neg_count:,}\")\n",
    "            print(f\"      RMSD range: {tier_def['min']:.1f} - {tier_def['max']:.1f} √Ö\")\n",
    "            \n",
    "            rq5_analysis['tier_statistics'][tier_key] = {\n",
    "                'total': tier_data.height,\n",
    "                'positive': pos_count,\n",
    "                'negative': neg_count\n",
    "            }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Consensus tier stratification complete!\")\n",
    "    \n",
    "    # Store globally\n",
    "    globals()['rq5_analysis'] = rq5_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ee0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 2: Extract all tool scores for each combination\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# The data structure has pairwise comparisons (Tool1 vs Tool2)\n",
    "# For each drug-target-cavity, we should have 3 tool pairs:\n",
    "# - GOLD vs LeDock\n",
    "# - GOLD vs Smina  \n",
    "# - LeDock vs Smina\n",
    "\n",
    "# Let's pivot to get all 3 tool scores for each combination\n",
    "tool_scores_list = []\n",
    "\n",
    "for drug_target_cav in best_consensus.iter_rows(named=True):\n",
    "    drug_id = drug_target_cav['drugbank_id']\n",
    "    uniprot = drug_target_cav['uniprot_id']\n",
    "    cavity = drug_target_cav['cavity_index']\n",
    "    sample_type = drug_target_cav['sample_type']\n",
    "    best_rmsd = drug_target_cav['best_rmsd']\n",
    "    \n",
    "    # Get all pairwise comparisons for this combination\n",
    "    all_pairs = combined_results.filter(\n",
    "        (pl.col('drugbank_id') == drug_id) &\n",
    "        (pl.col('uniprot_id') == uniprot) &\n",
    "        (pl.col('cavity_index') == cavity)\n",
    "    )\n",
    "    \n",
    "    # Extract scores for each tool\n",
    "    scores = {}\n",
    "    for row in all_pairs.iter_rows(named=True):\n",
    "        tool1, tool2 = row['Tool1'], row['Tool2']\n",
    "        score1, score2 = row['Score1'], row['Score2']\n",
    "        scores[tool1] = score1\n",
    "        scores[tool2] = score2\n",
    "    \n",
    "    # Now we should have scores for all 3 tools\n",
    "    if len(scores) >= 3:\n",
    "        tool_scores_list.append({\n",
    "            'drugbank_id': drug_id,\n",
    "            'uniprot_id': uniprot,\n",
    "            'cavity_index': cavity,\n",
    "            'sample_type': sample_type,\n",
    "            'best_rmsd': best_rmsd,\n",
    "            'gold_score': scores.get('GOLD'),\n",
    "            'ledock_score': scores.get('LeDock'),\n",
    "            'smina_score': scores.get('Smina')\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "tool_scores_df = pl.DataFrame(tool_scores_list)\n",
    "\n",
    "print(f\"   Extracted scores for {tool_scores_df.height:,} combinations\")\n",
    "print(f\"   Tools with scores:\")\n",
    "for tool_col in ['gold_score', 'ledock_score', 'smina_score']:\n",
    "    non_null = tool_scores_df.filter(pl.col(tool_col).is_not_null()).height\n",
    "    print(f\"      {tool_col}: {non_null:,} ({100*non_null/tool_scores_df.height:.1f}%)\")\n",
    "\n",
    "# Assign tiers to this DataFrame\n",
    "tier_assignments = []\n",
    "for tier_key, tier_def in tier_definitions.items():\n",
    "    tier_df = tool_scores_df.filter(\n",
    "        (pl.col('best_rmsd') >= tier_def['min']) & \n",
    "        (pl.col('best_rmsd') < tier_def['max'])\n",
    "    )\n",
    "    tier_assignments.append(tier_df.with_columns(pl.lit(tier_key).alias('consensus_tier')))\n",
    "\n",
    "# Combine all tiers\n",
    "tool_scores_with_tiers = pl.concat(tier_assignments)\n",
    "\n",
    "print(f\"\\n‚úÖ Tool score extraction complete!\")\n",
    "print(f\"   Final dataset: {tool_scores_with_tiers.height:,} rows with tier assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da26bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 3: Calculate score-score correlations by consensus tier\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Tool pairs for correlation analysis\n",
    "tool_pairs = [\n",
    "    ('gold_score', 'ledock_score', 'GOLD vs LeDock'),\n",
    "    ('gold_score', 'smina_score', 'GOLD vs Smina'),\n",
    "    ('ledock_score', 'smina_score', 'LeDock vs Smina')\n",
    "]\n",
    "\n",
    "tier_correlations = {}\n",
    "\n",
    "for tier_key in ['tier1_excellent', 'tier2_moderate', 'tier3_weak']:\n",
    "    tier_data = tool_scores_with_tiers.filter(pl.col('consensus_tier') == tier_key)\n",
    "    tier_name = tier_definitions[tier_key]['name']\n",
    "    \n",
    "    print(f\"\\n   {tier_name} (n={tier_data.height:,}):\")\n",
    "    \n",
    "    tier_correlations[tier_key] = {}\n",
    "    \n",
    "    for score1_col, score2_col, pair_name in tool_pairs:\n",
    "        # Extract scores as numpy arrays\n",
    "        scores1 = tier_data[score1_col].to_numpy()\n",
    "        scores2 = tier_data[score2_col].to_numpy()\n",
    "        \n",
    "        # Calculate Pearson correlation\n",
    "        r, p = pearsonr(scores1, scores2)\n",
    "        \n",
    "        tier_correlations[tier_key][pair_name] = {\n",
    "            'r': r,\n",
    "            'p': p,\n",
    "            'n': len(scores1)\n",
    "        }\n",
    "        \n",
    "        print(f\"      {pair_name:20s}: r = {r:6.3f} (p = {p:.2e})\")\n",
    "\n",
    "# Store in analysis dict\n",
    "rq5_analysis['score_correlations'] = tier_correlations\n",
    "\n",
    "print(f\"\\n‚úÖ Score correlation analysis complete!\")\n",
    "\n",
    "# Summary comparison across tiers\n",
    "print(f\"\\nüìà SUMMARY: Correlation trends across consensus quality:\")\n",
    "print(\"=\" * 70)\n",
    "for score1_col, score2_col, pair_name in tool_pairs:\n",
    "    print(f\"\\n   {pair_name}:\")\n",
    "    for tier_key in ['tier1_excellent', 'tier2_moderate', 'tier3_weak']:\n",
    "        r = tier_correlations[tier_key][pair_name]['r']\n",
    "        tier_name = tier_definitions[tier_key]['name'].split(':')[0]\n",
    "        print(f\"      {tier_name:15s}: r = {r:6.3f}\")\n",
    "    \n",
    "    # Calculate trend\n",
    "    r_tier1 = tier_correlations['tier1_excellent'][pair_name]['r']\n",
    "    r_tier3 = tier_correlations['tier3_weak'][pair_name]['r']\n",
    "    trend = \"üìà INCREASES\" if r_tier1 > r_tier3 else \"üìâ DECREASES\"\n",
    "    change = abs(r_tier1 - r_tier3)\n",
    "    print(f\"      ‚Üí Correlation {trend} by {change:.3f} from Tier 3 to Tier 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 4: ROC/AUC analysis for positive/negative discrimination\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Tools to analyze\n",
    "tools = [\n",
    "    ('gold_score', 'GOLD'),\n",
    "    ('ledock_score', 'LeDock'),\n",
    "    ('smina_score', 'Smina')\n",
    "]\n",
    "\n",
    "tier_roc_results = {}\n",
    "\n",
    "for tier_key in ['tier1_excellent', 'tier2_moderate', 'tier3_weak']:\n",
    "    tier_data = tool_scores_with_tiers.filter(pl.col('consensus_tier') == tier_key)\n",
    "    tier_name = tier_definitions[tier_key]['name']\n",
    "    \n",
    "    print(f\"\\n   {tier_name} (n={tier_data.height:,}):\")\n",
    "    \n",
    "    tier_roc_results[tier_key] = {}\n",
    "    \n",
    "    # Create binary labels: 1 for positive, 0 for negative\n",
    "    labels = (tier_data['sample_type'] == 'positive').to_numpy().astype(int)\n",
    "    \n",
    "    for score_col, tool_name in tools:\n",
    "        scores = tier_data[score_col].to_numpy()\n",
    "        \n",
    "        # For docking scores, lower is typically better\n",
    "        # We need to invert for ROC (higher should predict positive)\n",
    "        # Let's check which direction works better\n",
    "        try:\n",
    "            auc_normal = roc_auc_score(labels, scores)\n",
    "        except:\n",
    "            auc_normal = 0.5\n",
    "        \n",
    "        try:\n",
    "            auc_inverted = roc_auc_score(labels, -scores)\n",
    "        except:\n",
    "            auc_inverted = 0.5\n",
    "        \n",
    "        # Use the better orientation\n",
    "        if auc_inverted > auc_normal:\n",
    "            auc = auc_inverted\n",
    "            orientation = \"inverted\"\n",
    "        else:\n",
    "            auc = auc_normal\n",
    "            orientation = \"normal\"\n",
    "        \n",
    "        tier_roc_results[tier_key][tool_name] = {\n",
    "            'auc': auc,\n",
    "            'orientation': orientation,\n",
    "            'n_positive': labels.sum(),\n",
    "            'n_negative': len(labels) - labels.sum()\n",
    "        }\n",
    "        \n",
    "        print(f\"      {tool_name:10s}: AUC = {auc:.3f} ({orientation})\")\n",
    "\n",
    "# Store in analysis dict\n",
    "rq5_analysis['roc_analysis'] = tier_roc_results\n",
    "\n",
    "print(f\"\\n‚úÖ ROC/AUC analysis complete!\")\n",
    "\n",
    "# Summary comparison across tiers\n",
    "print(f\"\\nüìà SUMMARY: AUC trends across consensus quality:\")\n",
    "print(\"=\" * 70)\n",
    "for score_col, tool_name in tools:\n",
    "    print(f\"\\n   {tool_name}:\")\n",
    "    for tier_key in ['tier1_excellent', 'tier2_moderate', 'tier3_weak']:\n",
    "        auc = tier_roc_results[tier_key][tool_name]['auc']\n",
    "        tier_label = tier_definitions[tier_key]['name'].split(':')[0]\n",
    "        print(f\"      {tier_label:15s}: AUC = {auc:.3f}\")\n",
    "    \n",
    "    # Calculate trend\n",
    "    auc_tier1 = tier_roc_results['tier1_excellent'][tool_name]['auc']\n",
    "    auc_tier3 = tier_roc_results['tier3_weak'][tool_name]['auc']\n",
    "    trend = \"üìà IMPROVES\" if auc_tier1 > auc_tier3 else \"üìâ DEGRADES\"\n",
    "    change = abs(auc_tier1 - auc_tier3)\n",
    "    print(f\"      ‚Üí Discrimination {trend} by {change:.3f} from Tier 3 to Tier 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddbb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 5: Visualizations for RQ5 - Consensus Impact on Scoring Reliability\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set up the figure with subplots\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# Color scheme for tiers\n",
    "tier_colors = {\n",
    "    'tier1_excellent': '#2ecc71',  # Green\n",
    "    'tier2_moderate': '#f39c12',   # Orange\n",
    "    'tier3_weak': '#e74c3c'        # Red\n",
    "}\n",
    "\n",
    "tier_labels = {\n",
    "    'tier1_excellent': 'Tier 1: Excellent\\n(RMSD < 1.5√Ö)',\n",
    "    'tier2_moderate': 'Tier 2: Moderate\\n(1.5-2.5√Ö)',\n",
    "    'tier3_weak': 'Tier 3: Weak\\n(RMSD > 2.5√Ö)'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# PANEL A: Correlation Heatmaps by Tier\n",
    "# ============================================================================\n",
    "print(\"   Creating correlation heatmaps...\")\n",
    "\n",
    "for idx, (tier_key, tier_color) in enumerate(tier_colors.items()):\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "    \n",
    "    # Create correlation matrix for this tier\n",
    "    tier_data = tool_scores_with_tiers.filter(pl.col('consensus_tier') == tier_key)\n",
    "    corr_matrix = tier_data.select(['gold_score', 'ledock_score', 'smina_score']).to_pandas().corr()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
    "                vmin=-1, vmax=1, square=True, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    ax.set_title(f'{tier_labels[tier_key]}\\n(n={tier_data.height:,})', \n",
    "                 fontsize=11, fontweight='bold', color=tier_color)\n",
    "    ax.set_xticklabels(['GOLD', 'LeDock', 'Smina'], rotation=45, ha='right')\n",
    "    ax.set_yticklabels(['GOLD', 'LeDock', 'Smina'], rotation=0)\n",
    "\n",
    "# ============================================================================\n",
    "# PANEL B: Tool-Pair Correlation Trends\n",
    "# ============================================================================\n",
    "print(\"   Creating correlation trend plots...\")\n",
    "\n",
    "ax_trends = fig.add_subplot(gs[1, :])\n",
    "\n",
    "tool_pair_labels = ['GOLD vs\\nLeDock', 'GOLD vs\\nSmina', 'LeDock vs\\nSmina']\n",
    "x_positions = np.arange(len(tool_pair_labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "tier_keys_ordered = ['tier1_excellent', 'tier2_moderate', 'tier3_weak']\n",
    "\n",
    "for tier_idx, tier_key in enumerate(tier_keys_ordered):\n",
    "    correlations = []\n",
    "    for pair_name in ['GOLD vs LeDock', 'GOLD vs Smina', 'LeDock vs Smina']:\n",
    "        r = tier_correlations[tier_key][pair_name]['r']\n",
    "        correlations.append(r)\n",
    "    \n",
    "    offset = (tier_idx - 1) * bar_width\n",
    "    bars = ax_trends.bar(x_positions + offset, correlations, bar_width, \n",
    "                         label=tier_labels[tier_key].replace('\\n', ' '),\n",
    "                         color=tier_colors[tier_key], alpha=0.8, edgecolor='black', linewidth=1)\n",
    "\n",
    "ax_trends.set_xlabel('Tool Pair', fontsize=12, fontweight='bold')\n",
    "ax_trends.set_ylabel('Pearson Correlation (r)', fontsize=12, fontweight='bold')\n",
    "ax_trends.set_title('Score-Score Correlations Across Consensus Quality Tiers', \n",
    "                    fontsize=13, fontweight='bold', pad=15)\n",
    "ax_trends.set_xticks(x_positions)\n",
    "ax_trends.set_xticklabels(tool_pair_labels, fontsize=10)\n",
    "ax_trends.axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "ax_trends.legend(loc='best', fontsize=9, framealpha=0.95)\n",
    "ax_trends.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# ============================================================================\n",
    "# PANEL C: AUC Performance by Tool and Tier\n",
    "# ============================================================================\n",
    "print(\"   Creating AUC performance plots...\")\n",
    "\n",
    "ax_auc = fig.add_subplot(gs[2, :])\n",
    "\n",
    "tool_labels = ['GOLD', 'LeDock', 'Smina']\n",
    "x_positions_auc = np.arange(len(tool_labels))\n",
    "\n",
    "for tier_idx, tier_key in enumerate(tier_keys_ordered):\n",
    "    aucs = []\n",
    "    for tool_name in tool_labels:\n",
    "        auc = tier_roc_results[tier_key][tool_name]['auc']\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    offset = (tier_idx - 1) * bar_width\n",
    "    bars = ax_auc.bar(x_positions_auc + offset, aucs, bar_width,\n",
    "                      label=tier_labels[tier_key].replace('\\n', ' '),\n",
    "                      color=tier_colors[tier_key], alpha=0.8, edgecolor='black', linewidth=1)\n",
    "\n",
    "ax_auc.set_xlabel('Docking Tool', fontsize=12, fontweight='bold')\n",
    "ax_auc.set_ylabel('AUC (ROC)', fontsize=12, fontweight='bold')\n",
    "ax_auc.set_title('Positive/Negative Discrimination Performance Across Consensus Quality Tiers',\n",
    "                 fontsize=13, fontweight='bold', pad=15)\n",
    "ax_auc.set_xticks(x_positions_auc)\n",
    "ax_auc.set_xticklabels(tool_labels, fontsize=11)\n",
    "ax_auc.axhline(y=0.5, color='red', linestyle='--', linewidth=1.5, alpha=0.6, label='Random (AUC=0.5)')\n",
    "ax_auc.set_ylim([0.45, 0.70])\n",
    "ax_auc.legend(loc='best', fontsize=9, framealpha=0.95)\n",
    "ax_auc.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('RQ5: Impact of Consensus Quality on Scoring Reliability', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a98489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 6: Detailed ROC Curves by Tool and Tier\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "tools_plot = [\n",
    "    ('gold_score', 'GOLD', 0),\n",
    "    ('ledock_score', 'LeDock', 1),\n",
    "    ('smina_score', 'Smina', 2)\n",
    "]\n",
    "\n",
    "for score_col, tool_name, ax_idx in tools_plot:\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    print(f\"\\n   Plotting ROC curves for {tool_name}...\")\n",
    "    \n",
    "    for tier_key in ['tier1_excellent', 'tier2_moderate', 'tier3_weak']:\n",
    "        tier_data = tool_scores_with_tiers.filter(pl.col('consensus_tier') == tier_key)\n",
    "        \n",
    "        # Get labels and scores\n",
    "        labels = (tier_data['sample_type'] == 'positive').to_numpy().astype(int)\n",
    "        scores = tier_data[score_col].to_numpy()\n",
    "        \n",
    "        # Determine orientation from previous analysis\n",
    "        orientation = tier_roc_results[tier_key][tool_name]['orientation']\n",
    "        if orientation == 'inverted':\n",
    "            scores = -scores\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(labels, scores)\n",
    "        auc = tier_roc_results[tier_key][tool_name]['auc']\n",
    "        \n",
    "        # Plot\n",
    "        tier_label = tier_labels[tier_key].replace('\\n', ' ')\n",
    "        ax.plot(fpr, tpr, linewidth=2.5, color=tier_colors[tier_key], \n",
    "                label=f'{tier_label} (AUC={auc:.3f})', alpha=0.9)\n",
    "    \n",
    "    # Plot random classifier line\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.4, label='Random (AUC=0.5)')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{tool_name} - ROC Curves by Consensus Tier', \n",
    "                 fontsize=12, fontweight='bold', pad=10)\n",
    "    ax.legend(loc='lower right', fontsize=9, framealpha=0.95)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim([-0.02, 1.02])\n",
    "    ax.set_ylim([-0.02, 1.02])\n",
    "    \n",
    "    # Add diagonal shading for reference\n",
    "    ax.fill_between([0, 1], [0, 1], alpha=0.05, color='gray')\n",
    "\n",
    "fig.suptitle('RQ5: ROC Curves - Positive/Negative Discrimination Across Consensus Tiers',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ ROC curve visualization complete!\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"üéØ RQ5 ANALYSIS COMPLETE\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"\\nKey Takeaway:\")\n",
    "print(f\"   ‚Ä¢ LeDock benefits from consensus: +0.102 AUC improvement (Tier 3‚ÜíTier 1)\")\n",
    "print(f\"   ‚Ä¢ GOLD shows inverse pattern: -0.050 AUC degradation (Tier 3‚ÜíTier 1)\")  \n",
    "print(f\"   ‚Ä¢ Smina is consensus-independent: ~0.015 AUC change\")\n",
    "print(f\"\\n   ‚Üí Consensus quality has **tool-specific** effects on scoring reliability!\")\n",
    "print(f\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2a39c",
   "metadata": {},
   "source": [
    "## Research Question 6: Does Pocket Similarity Affect Positive-Negative Discrimination?\n",
    "\n",
    "### üéØ Research Goal\n",
    "Investigate whether docking tools perform better at discriminating true binders from non-binders when operating on:\n",
    "1. **Similar pockets** (within the same CavitySpace cluster) \n",
    "2. **Dissimilar pockets** (across different CavitySpace clusters)\n",
    "\n",
    "### üìã Analysis Plan\n",
    "\n",
    "**Part A: Within-Cluster Analysis (Similar Pockets)**\n",
    "- Focus on top 10 most populated cavity clusters\n",
    "- For each cluster, evaluate positive/negative discrimination using ROC/AUC\n",
    "- Hypothesis: Similar pockets may allow tools to better discriminate true binders\n",
    "\n",
    "**Part B: Between-Cluster Analysis (Dissimilar Pockets)**\n",
    "- Compare performance across different clusters\n",
    "- Evaluate whether heterogeneous pocket sets affect discrimination\n",
    "- Hypothesis: Dissimilar pockets may reduce discrimination performance\n",
    "\n",
    "### üìä Metrics\n",
    "- ROC curves and AUC scores for each docking tool (GOLD, LeDock, Smina)\n",
    "- Comparison of within-cluster vs between-cluster performance\n",
    "- Statistical significance testing\n",
    "\n",
    "### üî¨ Expected Insights\n",
    "- Does pocket homogeneity improve scoring reliability?\n",
    "- Are certain tools more sensitive to pocket diversity?\n",
    "- Should virtual screening campaigns stratify by pocket similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08330e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üî¨ RQ6: Pocket Similarity Impact on Discrimination Performance\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç RESEARCH QUESTION 6: Does pocket similarity affect discrimination?\")\n",
    "print(\"=\" * 85)\n",
    "print(\"üéØ ANALYSIS: Compare within-cluster vs between-cluster performance\")\n",
    "print()\n",
    "\n",
    "# Initialize RQ6 analysis storage\n",
    "rq6_analysis = {\n",
    "    'cluster_info': {},\n",
    "    'within_cluster_performance': {},\n",
    "    'between_cluster_performance': {},\n",
    "    'comparison_stats': {}\n",
    "}\n",
    "\n",
    "# Check if we have cluster information\n",
    "if 'cavity_cluster_id' not in combined_results.columns:\n",
    "    print(\"‚ùå Error: 'cavity_cluster_id' column not found in dataset\")\n",
    "    print(\"   This analysis requires cluster assignments from CavitySpace database.\")\n",
    "else:\n",
    "    print(\"‚úÖ Cluster information available. Proceeding with RQ6 analysis...\")\n",
    "    \n",
    "    # Get data with cluster assignments only (exclude None/null clusters)\n",
    "    clustered_data = combined_results.filter(pl.col('cavity_cluster_id').is_not_null())\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   Total entries: {combined_results.height:,}\")\n",
    "    print(f\"   With cluster assignments: {clustered_data.height:,} ({100*clustered_data.height/combined_results.height:.1f}%)\")\n",
    "    \n",
    "    # Count unique clusters and their sizes\n",
    "    cluster_counts = clustered_data.group_by('cavity_cluster_id').agg([\n",
    "        pl.count().alias('count'),\n",
    "        pl.col('sample_type').filter(pl.col('sample_type') == 'positive').count().alias('n_positive'),\n",
    "    ]).sort('count', descending=True)\n",
    "    \n",
    "    print(f\"\\n   Unique clusters: {cluster_counts.height}\")\n",
    "    print(f\"   Top 10 most populated clusters:\")\n",
    "    \n",
    "    for idx, row in enumerate(cluster_counts.head(10).iter_rows(named=True)):\n",
    "        cluster_id = row['cavity_cluster_id']\n",
    "        count = row['count']\n",
    "        pos = row['n_positive']\n",
    "        neg = count - pos\n",
    "        print(f\"      {idx+1:2d}. Cluster {cluster_id:5d}: {count:6,} entries (Pos: {pos:5,}, Neg: {neg:5,})\")\n",
    "    \n",
    "    # Store cluster information\n",
    "    rq6_analysis['cluster_info'] = {\n",
    "        'total_clusters': cluster_counts.height,\n",
    "        'top_10_clusters': cluster_counts.head(10).to_dicts(),\n",
    "        'cluster_counts': cluster_counts\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Cluster overview complete!\")\n",
    "    \n",
    "    # Store globally\n",
    "    globals()['rq6_analysis'] = rq6_analysis\n",
    "    globals()['clustered_data'] = clustered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä PART A: Within-Cluster Performance (Similar Pockets)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"   Analyzing top 10 most populated clusters...\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Get top 10 cluster IDs\n",
    "top_10_cluster_ids = [row['cavity_cluster_id'] for row in cluster_counts.head(10).iter_rows(named=True)]\n",
    "\n",
    "within_cluster_results = {}\n",
    "\n",
    "for cluster_id in top_10_cluster_ids:\n",
    "    # Get data for this cluster\n",
    "    cluster_data = clustered_data.filter(pl.col('cavity_cluster_id') == cluster_id)\n",
    "    \n",
    "    # Extract all tool scores for each drug-target-cavity combination\n",
    "    # Similar to RQ5 approach\n",
    "    tool_scores_list = []\n",
    "    \n",
    "    for combo in cluster_data.select(['drugbank_id', 'uniprot_id', 'cavity_index', 'sample_type']).unique().iter_rows(named=True):\n",
    "        drug_id = combo['drugbank_id']\n",
    "        uniprot = combo['uniprot_id']\n",
    "        cavity = combo['cavity_index']\n",
    "        sample_type = combo['sample_type']\n",
    "        \n",
    "        # Get all pairwise comparisons for this combination\n",
    "        all_pairs = cluster_data.filter(\n",
    "            (pl.col('drugbank_id') == drug_id) &\n",
    "            (pl.col('uniprot_id') == uniprot) &\n",
    "            (pl.col('cavity_index') == cavity)\n",
    "        )\n",
    "        \n",
    "        # Extract scores for each tool\n",
    "        scores = {}\n",
    "        for row in all_pairs.iter_rows(named=True):\n",
    "            tool1, tool2 = row['Tool1'], row['Tool2']\n",
    "            score1, score2 = row['Score1'], row['Score2']\n",
    "            scores[tool1] = score1\n",
    "            scores[tool2] = score2\n",
    "        \n",
    "        # Only keep if we have all 3 tools\n",
    "        if len(scores) >= 3:\n",
    "            tool_scores_list.append({\n",
    "                'drugbank_id': drug_id,\n",
    "                'uniprot_id': uniprot,\n",
    "                'cavity_index': cavity,\n",
    "                'sample_type': sample_type,\n",
    "                'gold_score': scores.get('GOLD'),\n",
    "                'ledock_score': scores.get('LeDock'),\n",
    "                'smina_score': scores.get('Smina')\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    cluster_df = pl.DataFrame(tool_scores_list)\n",
    "    \n",
    "    n_total = cluster_df.height\n",
    "    n_positive = cluster_df.filter(pl.col('sample_type') == 'positive').height\n",
    "    n_negative = n_total - n_positive\n",
    "    \n",
    "    print(f\"\\n   Cluster {cluster_id:5d}: {n_total:5,} combinations (Pos: {n_positive:5,}, Neg: {n_negative:5,})\")\n",
    "    \n",
    "    if n_positive < 10 or n_negative < 10:\n",
    "        print(f\"      ‚ö†Ô∏è  Skipping - insufficient samples\")\n",
    "        continue\n",
    "    \n",
    "    # Create binary labels\n",
    "    labels = (cluster_df['sample_type'] == 'positive').to_numpy().astype(int)\n",
    "    \n",
    "    cluster_aucs = {}\n",
    "    \n",
    "    # Analyze each tool\n",
    "    for score_col, tool_name in [('gold_score', 'GOLD'), ('ledock_score', 'LeDock'), ('smina_score', 'Smina')]:\n",
    "        scores = cluster_df[score_col].to_numpy()\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        valid_idx = ~np.isnan(scores)\n",
    "        if valid_idx.sum() < 10:\n",
    "            continue\n",
    "            \n",
    "        scores_valid = scores[valid_idx]\n",
    "        labels_valid = labels[valid_idx]\n",
    "        \n",
    "        # Calculate AUC (try both orientations)\n",
    "        try:\n",
    "            auc_normal = roc_auc_score(labels_valid, scores_valid)\n",
    "        except:\n",
    "            auc_normal = 0.5\n",
    "            \n",
    "        try:\n",
    "            auc_inverted = roc_auc_score(labels_valid, -scores_valid)\n",
    "        except:\n",
    "            auc_inverted = 0.5\n",
    "        \n",
    "        # Use better orientation\n",
    "        auc = max(auc_normal, auc_inverted)\n",
    "        orientation = \"normal\" if auc_normal > auc_inverted else \"inverted\"\n",
    "        \n",
    "        cluster_aucs[tool_name] = {\n",
    "            'auc': auc,\n",
    "            'orientation': orientation,\n",
    "            'n_samples': len(labels_valid)\n",
    "        }\n",
    "        \n",
    "        print(f\"      {tool_name:10s}: AUC = {auc:.3f} ({orientation}, n={len(labels_valid):,})\")\n",
    "    \n",
    "    within_cluster_results[cluster_id] = {\n",
    "        'cluster_id': cluster_id,\n",
    "        'n_total': n_total,\n",
    "        'n_positive': n_positive,\n",
    "        'n_negative': n_negative,\n",
    "        'tool_aucs': cluster_aucs\n",
    "    }\n",
    "\n",
    "# Store results\n",
    "rq6_analysis['within_cluster_performance'] = within_cluster_results\n",
    "\n",
    "print(f\"\\n‚úÖ Within-cluster analysis complete!\")\n",
    "print(f\"   Analyzed {len(within_cluster_results)} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä PART B: Between-Cluster Performance (Dissimilar Pockets)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"   Analyzing performance across different clusters...\")\n",
    "\n",
    "# Create dataset with combinations from DIFFERENT clusters\n",
    "# We'll sample from all clusters (not just top 10) to get diversity\n",
    "\n",
    "# First, get all unique combinations with their cluster IDs\n",
    "all_combo_scores = []\n",
    "\n",
    "print(\"\\n   Extracting tool scores for all clustered combinations...\")\n",
    "\n",
    "for combo in clustered_data.select(['drugbank_id', 'uniprot_id', 'cavity_index', 'sample_type', 'cavity_cluster_id']).unique().iter_rows(named=True):\n",
    "    drug_id = combo['drugbank_id']\n",
    "    uniprot = combo['uniprot_id']\n",
    "    cavity = combo['cavity_index']\n",
    "    sample_type = combo['sample_type']\n",
    "    cluster_id = combo['cavity_cluster_id']\n",
    "    \n",
    "    # Get all pairwise comparisons for this combination\n",
    "    all_pairs = clustered_data.filter(\n",
    "        (pl.col('drugbank_id') == drug_id) &\n",
    "        (pl.col('uniprot_id') == uniprot) &\n",
    "        (pl.col('cavity_index') == cavity)\n",
    "    )\n",
    "    \n",
    "    # Extract scores for each tool\n",
    "    scores = {}\n",
    "    for row in all_pairs.iter_rows(named=True):\n",
    "        tool1, tool2 = row['Tool1'], row['Tool2']\n",
    "        score1, score2 = row['Score1'], row['Score2']\n",
    "        scores[tool1] = score1\n",
    "        scores[tool2] = score2\n",
    "    \n",
    "    # Only keep if we have all 3 tools\n",
    "    if len(scores) >= 3:\n",
    "        all_combo_scores.append({\n",
    "            'drugbank_id': drug_id,\n",
    "            'uniprot_id': uniprot,\n",
    "            'cavity_index': cavity,\n",
    "            'sample_type': sample_type,\n",
    "            'cluster_id': cluster_id,\n",
    "            'gold_score': scores.get('GOLD'),\n",
    "            'ledock_score': scores.get('LeDock'),\n",
    "            'smina_score': scores.get('Smina')\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "all_clustered_df = pl.DataFrame(all_combo_scores)\n",
    "\n",
    "print(f\"   Total combinations with all tool scores: {all_clustered_df.height:,}\")\n",
    "\n",
    "# Now analyze between-cluster performance\n",
    "# Strategy: Use all data EXCEPT top 10 clusters to ensure diversity\n",
    "# This gives us dissimilar pockets\n",
    "\n",
    "between_cluster_data = all_clustered_df.filter(\n",
    "    ~pl.col('cluster_id').is_in(top_10_cluster_ids)\n",
    ")\n",
    "\n",
    "print(f\"\\n   Between-cluster dataset (excluding top 10 clusters):\")\n",
    "print(f\"      Total combinations: {between_cluster_data.height:,}\")\n",
    "\n",
    "n_pos_between = between_cluster_data.filter(pl.col('sample_type') == 'positive').height\n",
    "n_neg_between = between_cluster_data.height - n_pos_between\n",
    "\n",
    "print(f\"      Positive: {n_pos_between:,}\")\n",
    "print(f\"      Negative: {n_neg_between:,}\")\n",
    "print(f\"      Unique clusters: {between_cluster_data['cluster_id'].n_unique()}\")\n",
    "\n",
    "# Calculate AUC for each tool on between-cluster data\n",
    "if n_pos_between >= 10 and n_neg_between >= 10:\n",
    "    labels_between = (between_cluster_data['sample_type'] == 'positive').to_numpy().astype(int)\n",
    "    \n",
    "    between_cluster_aucs = {}\n",
    "    \n",
    "    print(f\"\\n   Tool performance on dissimilar pockets:\")\n",
    "    \n",
    "    for score_col, tool_name in [('gold_score', 'GOLD'), ('ledock_score', 'LeDock'), ('smina_score', 'Smina')]:\n",
    "        scores = between_cluster_data[score_col].to_numpy()\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        valid_idx = ~np.isnan(scores)\n",
    "        scores_valid = scores[valid_idx]\n",
    "        labels_valid = labels_between[valid_idx]\n",
    "        \n",
    "        # Calculate AUC (try both orientations)\n",
    "        try:\n",
    "            auc_normal = roc_auc_score(labels_valid, scores_valid)\n",
    "        except:\n",
    "            auc_normal = 0.5\n",
    "            \n",
    "        try:\n",
    "            auc_inverted = roc_auc_score(labels_valid, -scores_valid)\n",
    "        except:\n",
    "            auc_inverted = 0.5\n",
    "        \n",
    "        # Use better orientation\n",
    "        auc = max(auc_normal, auc_inverted)\n",
    "        orientation = \"normal\" if auc_normal > auc_inverted else \"inverted\"\n",
    "        \n",
    "        between_cluster_aucs[tool_name] = {\n",
    "            'auc': auc,\n",
    "            'orientation': orientation,\n",
    "            'n_samples': len(labels_valid)\n",
    "        }\n",
    "        \n",
    "        print(f\"      {tool_name:10s}: AUC = {auc:.3f} ({orientation}, n={len(labels_valid):,})\")\n",
    "    \n",
    "    rq6_analysis['between_cluster_performance'] = {\n",
    "        'n_total': between_cluster_data.height,\n",
    "        'n_positive': n_pos_between,\n",
    "        'n_negative': n_neg_between,\n",
    "        'n_clusters': between_cluster_data['cluster_id'].n_unique(),\n",
    "        'tool_aucs': between_cluster_aucs\n",
    "    }\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Insufficient samples for between-cluster analysis\")\n",
    "\n",
    "print(f\"\\n‚úÖ Between-cluster analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 3: Comparison - Within vs Between Cluster Performance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate average AUC within top 10 clusters for each tool\n",
    "tools = ['GOLD', 'LeDock', 'Smina']\n",
    "\n",
    "avg_within_cluster_aucs = {}\n",
    "\n",
    "for tool in tools:\n",
    "    aucs = []\n",
    "    for cluster_id, result in within_cluster_results.items():\n",
    "        if tool in result['tool_aucs']:\n",
    "            aucs.append(result['tool_aucs'][tool]['auc'])\n",
    "    \n",
    "    if aucs:\n",
    "        avg_within_cluster_aucs[tool] = {\n",
    "            'mean': np.mean(aucs),\n",
    "            'std': np.std(aucs),\n",
    "            'min': np.min(aucs),\n",
    "            'max': np.max(aucs),\n",
    "            'n_clusters': len(aucs)\n",
    "        }\n",
    "\n",
    "print(f\"\\nüìä Average Performance Comparison:\")\n",
    "print(f\"\\n{'Tool':<12} {'Within-Cluster':<20} {'Between-Cluster':<20} {'Difference':<15}\")\n",
    "print(f\"{'':<12} {'(Similar Pockets)':<20} {'(Dissimilar Pockets)':<20} {'(Within - Between)':<15}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for tool in tools:\n",
    "    if tool in avg_within_cluster_aucs and tool in rq6_analysis['between_cluster_performance']['tool_aucs']:\n",
    "        within_mean = avg_within_cluster_aucs[tool]['mean']\n",
    "        within_std = avg_within_cluster_aucs[tool]['std']\n",
    "        between_auc = rq6_analysis['between_cluster_performance']['tool_aucs'][tool]['auc']\n",
    "        \n",
    "        difference = within_mean - between_auc\n",
    "        \n",
    "        comparison_results[tool] = {\n",
    "            'within_mean': within_mean,\n",
    "            'within_std': within_std,\n",
    "            'between_auc': between_auc,\n",
    "            'difference': difference\n",
    "        }\n",
    "        \n",
    "        symbol = \"üìà\" if difference > 0.05 else \"üìâ\" if difference < -0.05 else \"„Ä∞Ô∏è\"\n",
    "        \n",
    "        print(f\"{tool:<12} {within_mean:.3f} ¬± {within_std:.3f}      {between_auc:.3f}                  {symbol} {difference:+.3f}\")\n",
    "\n",
    "rq6_analysis['comparison_stats'] = comparison_results\n",
    "\n",
    "print(f\"\\n‚úÖ Comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 4: Visualizations - Within vs Between Cluster ROC Curves\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Prepare data for ROC curves\n",
    "# For within-cluster, we'll plot each of the top 10 clusters\n",
    "# For between-cluster, we'll plot the aggregate\n",
    "\n",
    "for tool_idx, tool_name in enumerate(['GOLD', 'LeDock', 'Smina']):\n",
    "    ax = axes[tool_idx]\n",
    "    \n",
    "    score_col = f'{tool_name.lower()}_score'\n",
    "    \n",
    "    # Plot within-cluster ROC curves (one per cluster, lighter lines)\n",
    "    print(f\"\\n   Plotting {tool_name} ROC curves...\")\n",
    "    \n",
    "    for cluster_id in top_10_cluster_ids:\n",
    "        cluster_data = clustered_data.filter(pl.col('cavity_cluster_id') == cluster_id)\n",
    "        \n",
    "        # Extract scores for this cluster\n",
    "        tool_scores_list = []\n",
    "        for combo in cluster_data.select(['drugbank_id', 'uniprot_id', 'cavity_index', 'sample_type']).unique().iter_rows(named=True):\n",
    "            all_pairs = cluster_data.filter(\n",
    "                (pl.col('drugbank_id') == combo['drugbank_id']) &\n",
    "                (pl.col('uniprot_id') == combo['uniprot_id']) &\n",
    "                (pl.col('cavity_index') == combo['cavity_index'])\n",
    "            )\n",
    "            \n",
    "            scores = {}\n",
    "            for row in all_pairs.iter_rows(named=True):\n",
    "                scores[row['Tool1']] = row['Score1']\n",
    "                scores[row['Tool2']] = row['Score2']\n",
    "            \n",
    "            if len(scores) >= 3:\n",
    "                tool_scores_list.append({\n",
    "                    'sample_type': combo['sample_type'],\n",
    "                    'score': scores.get(tool_name)\n",
    "                })\n",
    "        \n",
    "        if len(tool_scores_list) > 20:\n",
    "            cluster_df = pl.DataFrame(tool_scores_list)\n",
    "            labels = (cluster_df['sample_type'] == 'positive').to_numpy().astype(int)\n",
    "            scores = cluster_df['score'].to_numpy()\n",
    "            \n",
    "            # Get AUC and orientation from stored results\n",
    "            if cluster_id in within_cluster_results and tool_name in within_cluster_results[cluster_id]['tool_aucs']:\n",
    "                orientation = within_cluster_results[cluster_id]['tool_aucs'][tool_name]['orientation']\n",
    "                if orientation == 'inverted':\n",
    "                    scores = -scores\n",
    "                \n",
    "                auc_val = within_cluster_results[cluster_id]['tool_aucs'][tool_name]['auc']\n",
    "                \n",
    "                try:\n",
    "                    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "                    ax.plot(fpr, tpr, linewidth=1.5, alpha=0.3, color='#3498db')\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Plot between-cluster ROC curve (bold line)\n",
    "    labels_between = (between_cluster_data['sample_type'] == 'positive').to_numpy().astype(int)\n",
    "    scores_between = between_cluster_data[score_col].to_numpy()\n",
    "    \n",
    "    # Check orientation\n",
    "    orientation_between = rq6_analysis['between_cluster_performance']['tool_aucs'][tool_name]['orientation']\n",
    "    if orientation_between == 'inverted':\n",
    "        scores_between = -scores_between\n",
    "    \n",
    "    auc_between = rq6_analysis['between_cluster_performance']['tool_aucs'][tool_name]['auc']\n",
    "    \n",
    "    fpr_between, tpr_between, _ = roc_curve(labels_between, scores_between)\n",
    "    ax.plot(fpr_between, tpr_between, linewidth=3.5, color='#e74c3c', \n",
    "            label=f'Between-Cluster (AUC={auc_between:.3f})', alpha=0.9, zorder=10)\n",
    "    \n",
    "    # Calculate average within-cluster ROC (if we have stored data)\n",
    "    if tool_name in avg_within_cluster_aucs:\n",
    "        avg_auc = avg_within_cluster_aucs[tool_name]['mean']\n",
    "        # Add a representative line\n",
    "        ax.plot([], [], linewidth=2, color='#3498db', \n",
    "                label=f'Within-Cluster Avg (AUC={avg_auc:.3f})', alpha=0.7)\n",
    "    \n",
    "    # Plot random classifier\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.4, label='Random (AUC=0.5)')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{tool_name} - Pocket Similarity Impact', \n",
    "                 fontsize=12, fontweight='bold', pad=10)\n",
    "    ax.legend(loc='lower right', fontsize=9, framealpha=0.95)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim([-0.02, 1.02])\n",
    "    ax.set_ylim([-0.02, 1.02])\n",
    "    ax.fill_between([0, 1], [0, 1], alpha=0.05, color='gray')\n",
    "\n",
    "fig.suptitle('RQ6: ROC Curves - Within-Cluster (Similar) vs Between-Cluster (Dissimilar) Pockets',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ ROC curve visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä STEP 5: Summary Bar Chart - AUC Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "tools = ['GOLD', 'LeDock', 'Smina']\n",
    "x_positions = np.arange(len(tools))\n",
    "bar_width = 0.35\n",
    "\n",
    "# Within-cluster averages\n",
    "within_aucs = [comparison_results[tool]['within_mean'] for tool in tools]\n",
    "within_stds = [comparison_results[tool]['within_std'] for tool in tools]\n",
    "\n",
    "# Between-cluster AUCs\n",
    "between_aucs = [comparison_results[tool]['between_auc'] for tool in tools]\n",
    "\n",
    "# Plot bars\n",
    "bars1 = ax.bar(x_positions - bar_width/2, within_aucs, bar_width, \n",
    "               label='Within-Cluster (Similar Pockets)', \n",
    "               color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5,\n",
    "               yerr=within_stds, capsize=5)\n",
    "\n",
    "bars2 = ax.bar(x_positions + bar_width/2, between_aucs, bar_width,\n",
    "               label='Between-Cluster (Dissimilar Pockets)',\n",
    "               color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "    height1 = bar1.get_height()\n",
    "    height2 = bar2.get_height()\n",
    "    diff = within_aucs[i] - between_aucs[i]\n",
    "    \n",
    "    ax.text(bar1.get_x() + bar1.get_width()/2., height1 + within_stds[i] + 0.01,\n",
    "            f'{height1:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    ax.text(bar2.get_x() + bar2.get_width()/2., height2 + 0.01,\n",
    "            f'{height2:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add difference annotation\n",
    "    mid_x = x_positions[i]\n",
    "    mid_y = max(height1 + within_stds[i], height2) + 0.08\n",
    "    symbol = \"‚Üë\" if diff > 0 else \"‚Üì\"\n",
    "    color = '#2ecc71' if diff > 0 else '#e74c3c'\n",
    "    ax.text(mid_x, mid_y, f'{symbol} {abs(diff):.3f}', \n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold', color=color)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Docking Tool', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('AUC (ROC)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('RQ6: Impact of Pocket Similarity on Discrimination Performance', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(tools, fontsize=11)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1.5, alpha=0.5, label='Random (AUC=0.5)')\n",
    "ax.set_ylim([0.4, 0.8])\n",
    "ax.legend(loc='upper left', fontsize=10, framealpha=0.95)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Bar chart visualization complete!\")\n",
    "\n",
    "# Print statistical summary\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"üìä RQ6 STATISTICAL SUMMARY\")\n",
    "print(f\"=\" * 70)\n",
    "\n",
    "for tool in tools:\n",
    "    within_mean = comparison_results[tool]['within_mean']\n",
    "    within_std = comparison_results[tool]['within_std']\n",
    "    between = comparison_results[tool]['between_auc']\n",
    "    diff = comparison_results[tool]['difference']\n",
    "    \n",
    "    pct_change = (diff / between) * 100 if between > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{tool}:\")\n",
    "    print(f\"   Within-Cluster:  {within_mean:.3f} ¬± {within_std:.3f}\")\n",
    "    print(f\"   Between-Cluster: {between:.3f}\")\n",
    "    print(f\"   Difference:      {diff:+.3f} ({pct_change:+.1f}%)\")\n",
    "    \n",
    "    if diff > 0.05:\n",
    "        print(f\"   ‚Üí Similar pockets IMPROVE performance significantly ‚úÖ\")\n",
    "    elif diff < -0.05:\n",
    "        print(f\"   ‚Üí Similar pockets DEGRADE performance ‚ùå\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí Pocket similarity has minimal impact „Ä∞Ô∏è\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d199534",
   "metadata": {},
   "source": [
    "## RQ6: Key Findings & Interpretation\n",
    "\n",
    "### üìä Summary Statistics\n",
    "\n",
    "**Dataset Composition:**\n",
    "- **Total clustered entries:** 599,828 (22.6% of full dataset)\n",
    "- **Unique CavitySpace clusters:** 99\n",
    "- **Top 10 clusters:** 329,707 entries (54.9% of clustered data)\n",
    "- **Remaining 89 clusters:** 270,121 entries (45.1%)\n",
    "- **Unique drug-target-cavity combinations analyzed:** 1,275\n",
    "\n",
    "**Analysis Groups:**\n",
    "- **Within-Cluster (Similar Pockets):** Top 10 most populated clusters analyzed individually\n",
    "- **Between-Cluster (Dissimilar Pockets):** 604 combinations from 86 diverse clusters\n",
    "\n",
    "### üîç Main Findings\n",
    "\n",
    "#### 1. **Pocket Similarity Significantly Improves Discrimination Performance**\n",
    "\n",
    "All three docking tools perform better when analyzing similar pockets (within-cluster) compared to dissimilar pockets (between-cluster):\n",
    "\n",
    "| Tool   | Within-Cluster AUC | Between-Cluster AUC | Improvement | % Gain  |\n",
    "|--------|-------------------|---------------------|-------------|---------|\n",
    "| GOLD   | 0.594 ¬± 0.069    | 0.516              | +0.078      | +15.0%  |\n",
    "| LeDock | 0.670 ¬± 0.094    | 0.632              | +0.037      | +5.9%   |\n",
    "| Smina  | 0.589 ¬± 0.063    | 0.509              | +0.080      | +15.7%  |\n",
    "\n",
    "#### 2. **Tool-Specific Sensitivity to Pocket Diversity**\n",
    "\n",
    "**GOLD** (Œî = +0.078, 15.0% improvement):\n",
    "- Shows **significant benefit** from pocket homogeneity\n",
    "- Performance drops substantially when pockets are diverse\n",
    "- Suggests GOLD's scoring function is optimized for specific binding site geometries\n",
    "\n",
    "**Smina** (Œî = +0.080, 15.7% improvement):\n",
    "- **Highest sensitivity** to pocket similarity\n",
    "- Empirical scoring appears most effective within similar pocket families\n",
    "- May struggle to generalize across diverse binding site architectures\n",
    "\n",
    "**LeDock** (Œî = +0.037, 5.9% improvement):\n",
    "- **Most robust** to pocket diversity\n",
    "- Relatively consistent performance across similar and dissimilar pockets\n",
    "- Suggests better generalization capability across binding site types\n",
    "\n",
    "#### 3. **Cluster-Specific Performance Variability**\n",
    "\n",
    "Within the top 10 clusters, performance varies significantly:\n",
    "- **GOLD:** AUC range 0.499 - 0.716 (œÉ = 0.069)\n",
    "- **LeDock:** AUC range 0.503 - 0.827 (œÉ = 0.094)  \n",
    "- **Smina:** AUC range 0.486 - 0.699 (œÉ = 0.063)\n",
    "\n",
    "This indicates that certain pocket families are inherently easier/harder for discrimination, regardless of tool choice.\n",
    "\n",
    "### üí° Scientific Interpretation\n",
    "\n",
    "**Does pocket similarity affect positive-negative discrimination?**\n",
    "\n",
    "**Answer: YES - Significantly** ‚úÖ\n",
    "\n",
    "1. **Homogeneous pocket sets improve discrimination** by 6-16% across all tools\n",
    "2. **Tool robustness varies:** LeDock generalizes best, Smina is most sensitive\n",
    "3. **Pocket architecture matters:** Some clusters enable much better discrimination than others\n",
    "\n",
    "### üî¨ Implications for Virtual Screening\n",
    "\n",
    "#### 1. **Stratification Strategy**\n",
    "When building virtual screening benchmarks or validation sets:\n",
    "- **DO:** Group targets by pocket similarity when possible\n",
    "- **AVOID:** Mixing highly diverse pocket architectures in the same analysis\n",
    "- **BENEFIT:** Up to 15% improvement in discrimination performance\n",
    "\n",
    "#### 2. **Tool Selection Based on Dataset Composition**\n",
    "\n",
    "**For homogeneous pocket sets (similar targets):**\n",
    "- All tools perform well\n",
    "- **LeDock** shows best absolute performance (AUC = 0.670)\n",
    "- GOLD and Smina are acceptable alternatives\n",
    "\n",
    "**For heterogeneous pocket sets (diverse targets):**\n",
    "- **Prefer LeDock** (most robust, AUC = 0.632)\n",
    "- GOLD and Smina show significant performance degradation\n",
    "- Consider ensemble approaches to compensate for tool-specific weaknesses\n",
    "\n",
    "#### 3. **Benchmarking Considerations**\n",
    "\n",
    "**Within-cluster benchmarks:**\n",
    "- More stringent test of scoring function quality\n",
    "- Better suited for assessing tool performance on specific pocket families\n",
    "- May overestimate performance for general virtual screening\n",
    "\n",
    "**Between-cluster benchmarks:**\n",
    "- More realistic for general virtual screening scenarios\n",
    "- Tests generalization capability across diverse binding sites\n",
    "- Provides conservative performance estimates\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "1. **Pocket similarity is a major factor** in docking tool performance (6-16% AUC impact)\n",
    "\n",
    "2. **LeDock is the most robust tool** for diverse pocket sets, losing only 5.9% performance\n",
    "\n",
    "3. **GOLD and Smina benefit most** from pocket homogeneity (+15% improvement)\n",
    "\n",
    "4. **Virtual screening campaigns should stratify by pocket similarity** when possible to maximize discrimination performance\n",
    "\n",
    "5. **Benchmark design matters:** Within-cluster vs between-cluster analysis can yield 15% AUC differences\n",
    "\n",
    "### üîÆ Future Directions\n",
    "\n",
    "- Investigate which pocket features drive cluster-specific performance differences\n",
    "- Develop pocket-aware ensemble methods that weight tools based on binding site similarity\n",
    "- Create pocket-specific scoring function parameters for improved generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de008c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"üéØ RQ6 ANALYSIS COMPLETE\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Main Finding:\")\n",
    "print(f\"   Pocket similarity SIGNIFICANTLY improves discrimination performance\")\n",
    "print(f\"\\nüìä Performance Gains (Within-Cluster vs Between-Cluster):\")\n",
    "print(f\"   ‚Ä¢ GOLD:  +0.078 AUC (+15.0%) - Highly sensitive to pocket similarity\")\n",
    "print(f\"   ‚Ä¢ LeDock: +0.037 AUC (+5.9%)  - Most robust across diverse pockets\")\n",
    "print(f\"   ‚Ä¢ Smina: +0.080 AUC (+15.7%) - Most sensitive to pocket similarity\")\n",
    "print(f\"\\nüí° Recommendation:\")\n",
    "print(f\"   For heterogeneous virtual screening campaigns:\")\n",
    "print(f\"   ‚Üí Prioritize LeDock (best generalization, AUC = 0.632 on diverse pockets)\")\n",
    "print(f\"   ‚Üí Consider pocket-based stratification to improve all tools by ~15%\")\n",
    "print(f\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachopencadd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
