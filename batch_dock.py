#!/usr/bin/env python3
"""
Batch Consensus Docking Script - Main Workflow
This script performs the actual docking jobs using pre-generated mappings.
Only small molecule drugs are processed, as the docking workflow is optimized for small molecules.

Prerequisites:
    1. Run prepare_uniprot_mapping.py first to generate uniprot_gene_mapping.csv
    2. Run extract_cavities.py to generate cavity_mapping.csv
    3. Run identify_required_structures.py to identify required structures (RECOMMENDED)
    4. Run extract_alphafold_models.py to get full AlphaFold structures (RECOMMENDED)
    5. Run fix_required_pdbs.py to add hydrogens to required structures (RECOMMENDED)
    6. Optionally run convert_pdb_to_pdbqt.py for better performance
    7. Ensure all required paths and dependencies are correctly configured

Usage:
    python batch_dock.py

Input:
    - uniprot_gene_mapping.csv: UniProt mapping file (generated by prepare_uniprot_mapping.py)
    - cavity_mapping.csv: Cavity mapping file (generated by extract_cavities.py)
    - small_molecule_drug_links.csv: Small molecule drugs from DrugBank
    - fixed_mapping.csv: Fixed structures mapping file (preferred, generated by fix_required_pdbs.py)
    - alphafold_mapping.csv: AlphaFold mapping file (alternative, generated by extract_alphafold_models.py)
    - pdbqt_mapping.csv: PDBQT mapping file (optional, generated by convert_pdb_to_pdbqt.py)
    - Drug-to-protein interaction TSV file
    - Processed ligand SDF folder

Output:
    - Docking results in consensus_docking_results/ folder
    - Log file: docking_automation.log
"""

import os
import sys
import pandas as pd
import subprocess
from tqdm import tqdm
from pathlib import Path
import re
import time
import logging
import glob
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# --- Configuration ---
# --- IMPORTANT: ADJUST THESE PATHS AS PER YOUR SYSTEM ---
CONSENSUS_DOCKER_SCRIPT = "/home/onur/repos/consensus_docking/consensus_docker.py"
PROCESSED_LIGAND_SDF_FOLDER = "/home/onur/experiments/cavity_space_consensus_docking/drugbank_approved_split" # The output folder from your RDKit 3D processing script
DRUG_TO_PROTEIN_TSV = "/opt/data/multiscale_interactome_data/1_drug_to_protein.tsv"
SMALL_MOLECULE_DRUGS_CSV = "/opt/data/drugbank/small_molecule_drug_links.csv"  # Small molecule drugs only
UNIPROT_MAPPING_CSV = "uniprot_gene_mapping.csv" # Input file from prepare_uniprot_mapping.py
CAVITY_MAPPING_CSV = "cavity_mapping.csv" # Input file from extract_cavities.py
FIXED_MAPPING_CSV = "fixed_mapping.csv" # Preferred input from fix_required_pdbs.py
ALPHAFOLD_MAPPING_CSV = "alphafold_mapping.csv" # Alternative input from extract_alphafold_models.py
PDBQT_MAPPING_CSV = "pdbqt_mapping.csv" # Input file from convert_pdb_to_pdbqt.py (optional)
LOG_FILE = "docking_automation.log"
# --- TEST MODE: Set to True to run only 3 docking jobs for testing ---
TEST_MODE = False
MAX_TEST_JOBS = 40
# --- CONFIRMATION PROMPT: Set to True to skip user confirmation when not in TEST_MODE ---
SKIP_CONFIRMATION = True  # Set to True for nohup/batch execution
# --- TWO-STAGE DOCKING CONFIGURATION ---
# Stage 1: Smina only (high exhaustiveness, low parallelism)
SMINA_MAX_PARALLEL_JOBS = 4     # Maximum parallel jobs for smina stage
SMINA_EXHAUSTIVENESS = 16       # High exhaustiveness for smina
SMINA_TIMEOUT = 300           # Timeout for smina jobs in seconds (5 minutes)
# Stage 2: Gold + LeDock (high parallelism, standard exhaustiveness)
GOLD_LEDOCK_MAX_PARALLEL_JOBS = 60  # Maximum parallel jobs for gold+ledock stage
GOLD_LEDOCK_EXHAUSTIVENESS = 12     # Standard exhaustiveness for gold+ledock
GOLD_LEDOCK_TIMEOUT = 600           # Timeout for gold+ledock jobs in seconds (10 minutes)
# --- Consensus Docker Fixed Arguments (from your example) ---
SMINA_PATH = "/opt/anaconda3/envs/teachopencadd/bin/smina"
LEDOCK_PATH = "/home/onur/software/ledock_linux_x86"
LEPRO_PATH = "/home/onur/software/lepro_linux_x86"
GOLD_PATH = "/opt/goldsuite-5.3.0/bin/gold_auto"
NUM_THREADS = 60
CUTOFF_VALUE = -7

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

logging.info("Starting batch consensus docking workflow.")
if TEST_MODE:
    logging.info(f"*** RUNNING IN TEST MODE - LIMITED TO {MAX_TEST_JOBS} DOCKING JOBS ***")
else:
    logging.info("*** RUNNING IN PRODUCTION MODE ***")
if SKIP_CONFIRMATION:
    logging.info("*** SKIP_CONFIRMATION ENABLED - NO USER PROMPT REQUIRED ***")
logging.info(f"*** TWO-STAGE PARALLEL EXECUTION ENABLED ***")
logging.info(f"*** STAGE 1: SMINA - UP TO {SMINA_MAX_PARALLEL_JOBS} CONCURRENT JOBS, EXHAUSTIVENESS {SMINA_EXHAUSTIVENESS}, TIMEOUT {SMINA_TIMEOUT/60:.1f} MINUTES ***")
logging.info(f"*** STAGE 2: GOLD/LEDOCK - UP TO {GOLD_LEDOCK_MAX_PARALLEL_JOBS} CONCURRENT JOBS, TIMEOUT {GOLD_LEDOCK_TIMEOUT/60:.1f} MINUTES ***")
logging.info(f"Consensus Docker Script: {CONSENSUS_DOCKER_SCRIPT}")
logging.info(f"Processed Ligand SDF Folder: {PROCESSED_LIGAND_SDF_FOLDER}")
logging.info(f"Drug-to-Protein TSV: {DRUG_TO_PROTEIN_TSV}")
logging.info(f"UniProt Mapping CSV: {UNIPROT_MAPPING_CSV}")
logging.info(f"Cavity Mapping CSV: {CAVITY_MAPPING_CSV}")
logging.info(f"PDBQT Mapping CSV: {PDBQT_MAPPING_CSV}")
if os.path.exists(PDBQT_MAPPING_CSV):
    logging.info("PDBQT mapping found - will use pre-converted PDBQT files for faster docking")
else:
    logging.info("PDBQT mapping not found - will use PDB files (slower conversion during docking)")

def load_uniprot_mapping(mapping_file):
    """
    Load pre-generated UniProt mapping from CSV file.
    
    Args:
        mapping_file (str): Path to the UniProt mapping CSV file
        
    Returns:
        pd.DataFrame: DataFrame with UniProt mapping data
    """
    if not os.path.exists(mapping_file):
        logging.critical(f"UniProt mapping file not found: {mapping_file}")
        logging.critical("Please run prepare_uniprot_mapping.py first to generate the mapping file.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['Entry', 'Mapped_Gene_Name']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.critical(f"Required columns missing from mapping file: {missing_columns}")
            logging.critical("Please regenerate the mapping file using prepare_uniprot_mapping.py")
            sys.exit(1)
        
        logging.info(f"Loaded UniProt mapping with {len(df)} entries from {mapping_file}")
        return df[['Entry', 'Mapped_Gene_Name']]
        
    except Exception as e:
        logging.critical(f"Error loading UniProt mapping file: {e}")
        sys.exit(1)

def load_small_molecule_drugs():
    """
    Load small molecule drugs list from DrugBank.
    
    Returns:
        set: Set of small molecule drug IDs
    """
    if not os.path.exists(SMALL_MOLECULE_DRUGS_CSV):
        logging.critical(f"Small molecule drugs CSV not found: {SMALL_MOLECULE_DRUGS_CSV}")
        sys.exit(1)
    
    try:
        df = pd.read_csv(SMALL_MOLECULE_DRUGS_CSV)
        small_molecule_ids = set(df['DrugBank ID'].unique())
        logging.info(f"Loaded {len(small_molecule_ids)} small molecule drug IDs")
        return small_molecule_ids
    except Exception as e:
        logging.critical(f"Error loading small molecule drugs: {e}")
        sys.exit(1)

def load_best_mapping_with_pdbqt():
    """
    Load the best available mapping file with PDBQT paths.
    Prioritizes fixed PDB mapping, then AlphaFold mapping, then original cavity mapping.
    
    Returns
    -------
    dict: Dictionary mapping UniProt IDs to lists of (receptor_pdb, pocket_pdb, receptor_pdbqt) tuples
    str: Source file used
    """
    # Define priority order
    mapping_files = [
        (FIXED_MAPPING_CSV, "fixed structures mapping"),
        (ALPHAFOLD_MAPPING_CSV, "AlphaFold mapping"),
        (CAVITY_MAPPING_CSV, "original cavity mapping")
    ]
    
    mapping_file = None
    mapping_type = None
    
    # Find the best available mapping file
    for file_path, file_type in mapping_files:
        if os.path.exists(file_path):
            mapping_file = file_path
            mapping_type = file_type
            logging.info(f"Using {mapping_type}: {mapping_file}")
            break
    
    if not mapping_file:
        logging.critical("No mapping file found!")
        logging.critical("Please run extract_cavities.py and fix_required_pdbs.py first.")
        sys.exit(1)
    
    # Load the mapping with PDBQT support
    pdbqt_file = PDBQT_MAPPING_CSV if os.path.exists(PDBQT_MAPPING_CSV) else None
    mapping_dict = load_cavity_mapping(mapping_file, pdbqt_file)
    
    return mapping_dict, mapping_file

def load_cavity_mapping(mapping_file, pdbqt_mapping_file=None):
    """
    Load cavity mapping from a pre-generated CSV file, optionally with PDBQT paths.
    
    Args:
        mapping_file (str): Path to the cavity mapping CSV file
        pdbqt_mapping_file (str, optional): Path to the PDBQT mapping CSV file
        
    Returns:
        dict: Dictionary mapping UniProt IDs to lists of (receptor_pdb, pocket_pdb, receptor_pdbqt) tuples
    """
    if not os.path.exists(mapping_file):
        logging.critical(f"Cavity mapping file not found: {mapping_file}")
        logging.critical("Please run extract_cavities.py first to generate the cavity mapping file.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['UniProt_ID', 'Receptor_PDB', 'Pocket_PDB']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.critical(f"Required columns missing from cavity mapping file: {missing_columns}")
            logging.critical("Please regenerate the cavity mapping file using extract_cavities.py")
            sys.exit(1)
        
        # Load PDBQT mapping if available
        pdbqt_mapping = {}
        if pdbqt_mapping_file and os.path.exists(pdbqt_mapping_file):
            logging.info(f"Loading PDBQT mapping from {pdbqt_mapping_file}")
            try:
                pdbqt_df = pd.read_csv(pdbqt_mapping_file)
                if 'Receptor_PDB' in pdbqt_df.columns and 'Receptor_PDBQT' in pdbqt_df.columns:
                    # Create mapping from PDB path to PDBQT path
                    pdbqt_mapping = dict(zip(pdbqt_df['Receptor_PDB'], pdbqt_df['Receptor_PDBQT']))
                    logging.info(f"Loaded {len(pdbqt_mapping)} PDBQT mappings")
                else:
                    logging.warning("PDBQT mapping file missing required columns, ignoring PDBQT files")
            except Exception as e:
                logging.warning(f"Error loading PDBQT mapping: {e}. Continuing without PDBQT files.")
        
        pdb_info = {}
        for _, row in df.iterrows():
            uniprot_id = row['UniProt_ID']
            receptor_pdb = row['Receptor_PDB']
            pocket_pdb = row['Pocket_PDB']
            
            # Verify that the files still exist
            if not os.path.exists(receptor_pdb):
                logging.warning(f"Receptor PDB file not found: {receptor_pdb}")
                continue
            if not os.path.exists(pocket_pdb):
                logging.warning(f"Pocket PDB file not found: {pocket_pdb}")
                continue
            
            # Get corresponding PDBQT file if available
            receptor_pdbqt = pdbqt_mapping.get(receptor_pdb)
            if receptor_pdbqt and not os.path.exists(receptor_pdbqt):
                logging.warning(f"PDBQT file not found: {receptor_pdbqt}, will use PDB file")
                receptor_pdbqt = None
            
            if uniprot_id not in pdb_info:
                pdb_info[uniprot_id] = []
            pdb_info[uniprot_id].append((receptor_pdb, pocket_pdb, receptor_pdbqt))
        
        total_pairs = sum(len(pairs) for pairs in pdb_info.values())
        pdbqt_pairs = sum(1 for pairs in pdb_info.values() for _, _, pdbqt in pairs if pdbqt)
        
        logging.info(f"Loaded cavity mapping for {len(pdb_info)} UniProt IDs with {total_pairs} cavity pairs from {mapping_file}")
        logging.info(f"Found {pdbqt_pairs} cavity pairs with pre-converted PDBQT files")
        
        # Debug: Show some sample UniProt IDs for debugging
        sample_ids = list(pdb_info.keys())[:5]
        logging.info(f"Sample UniProt IDs in cavity mapping: {sample_ids}")
        
        return pdb_info
        
    except Exception as e:
        logging.critical(f"Error loading cavity mapping file: {e}")
        sys.exit(1)

def check_docking_completion(output_folder):
    """
    Check if a docking job was completed successfully by examining the output folder.
    
    Args:
        output_folder (str): Path to the docking output folder
        
    Returns:
        bool: True if the job appears to be completed, False otherwise
    """
    if not os.path.exists(output_folder):
        return False
    
    # Check if the folder has any content
    if not os.listdir(output_folder):
        return False
    
    # Look for typical consensus docking output files
    # Adjust these patterns based on what consensus_docker.py actually produces
    success_indicators = [
        "consensus_results.csv",
        "consensus_scores.txt", 
        "docking_summary.log",
        "*.sdf",  # Any SDF files (docked poses)
        "*/results.txt"  # Results in subdirectories
    ]
    
    for pattern in success_indicators:
        if glob.glob(os.path.join(output_folder, pattern)):
            logging.debug(f"Found completion indicator: {pattern} in {output_folder}")
            return True
    
    # If no specific indicators, check if there are multiple files/folders
    # indicating some substantial output was generated
    contents = os.listdir(output_folder)
    if len(contents) >= 3:  # Arbitrary threshold - adjust as needed
        logging.debug(f"Output folder {output_folder} has {len(contents)} items, considering complete")
        return True
    
    return False

# --- NOTE: Cavity extraction is now handled by extract_cavities.py ---
# Run extract_cavities.py first to generate cavity_mapping.csv before running this script

def run_single_smina_job(job_data):
    """
    Run a single smina docking job - Stage 1 of two-stage docking.
    
    Parameters
    ----------
    job_data : dict
        Dictionary containing all job parameters
        
    Returns
    -------
    dict
        Results of the smina docking job
    """
    # Extract job parameters
    job_idx = job_data['job_idx']
    drugbank_id = job_data['drugbank_id']
    uniprot_id = job_data['uniprot_id']
    gene_name = job_data['gene_name']
    ligand_sdf = job_data['ligand_sdf']
    receptor_pdb = job_data['receptor_pdb']
    receptor_pdbqt = job_data['receptor_pdbqt']
    pocket_pdb = job_data['pocket_pdb']
    current_outfolder = job_data['current_outfolder']
    
    # Get configuration from job_data
    CONSENSUS_DOCKER_SCRIPT = job_data['consensus_docker_script']
    SMINA_PATH = job_data['smina_path']
    NUM_THREADS = job_data['num_threads']
    CUTOFF_VALUE = job_data['cutoff_value']
    EXHAUSTIVENESS = job_data['exhaustiveness']
    
    process_id = os.getpid()
    
    try:
        # Check if smina output already exists
        smina_output_dir = os.path.join(current_outfolder, "smina")
        if os.path.exists(smina_output_dir) and os.listdir(smina_output_dir):
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'skipped',
                'message': 'Smina job already completed',
                'process_id': process_id
            }
        
        # Build the smina-only command
        command = [
            "python", CONSENSUS_DOCKER_SCRIPT,
            "--use_smina",
            "--outfolder", current_outfolder,
            "--smina_path", SMINA_PATH,
            "--ligand_sdf", ligand_sdf,
            "--receptor_pdb", receptor_pdb,
            "--pocket_pdb", pocket_pdb,
            "--num_threads", str(NUM_THREADS),
            "--cutoff_value", str(CUTOFF_VALUE),
            "--exhaustiveness", str(EXHAUSTIVENESS)
        ]
        
        # Add PDBQT file if available for faster processing
        if receptor_pdbqt and os.path.exists(receptor_pdbqt):
            command.extend(["--receptor_pdbqt", receptor_pdbqt])
        
        # Run the command with timeout
        timeout_seconds = job_data.get('timeout', 600)  # Default 10 minutes
        try:
            result = subprocess.run(command, capture_output=True, text=True, 
                                  check=False, timeout=timeout_seconds)
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'timeout',
                'message': f'Smina docking timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)',
                'process_id': process_id
            }
        
        if result.returncode == 0:
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'completed',
                'message': f'Successfully completed smina docking for {drugbank_id} into {pocket_pdb}',
                'process_id': process_id
            }
        else:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'failed',
                'message': f'Smina docking failed. Return code: {result.returncode}',
                'stdout': result.stdout,
                'stderr': result.stderr,
                'process_id': process_id
            }
    
    except Exception as e:
        return {
            'success': False,
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            'action': 'error',
            'message': f'Exception occurred in smina docking: {str(e)}',
            'process_id': process_id
        }

def run_single_gold_ledock_job(job_data):
    """
    Run a single gold/ledock docking job - Stage 2 of two-stage docking.
    
    Parameters
    ----------
    job_data : dict
        Dictionary containing all job parameters
        
    Returns
    -------
    dict
        Results of the gold/ledock docking job
    """
    # Extract job parameters
    job_idx = job_data['job_idx']
    drugbank_id = job_data['drugbank_id']
    uniprot_id = job_data['uniprot_id']
    gene_name = job_data['gene_name']
    ligand_sdf = job_data['ligand_sdf']
    receptor_pdb = job_data['receptor_pdb']
    receptor_pdbqt = job_data['receptor_pdbqt']
    pocket_pdb = job_data['pocket_pdb']
    current_outfolder = job_data['current_outfolder']
    
    # Get configuration from job_data
    CONSENSUS_DOCKER_SCRIPT = job_data['consensus_docker_script']
    LEDOCK_PATH = job_data['ledock_path']
    LEPRO_PATH = job_data['lepro_path']
    GOLD_PATH = job_data['gold_path']
    NUM_THREADS = job_data['num_threads']
    CUTOFF_VALUE = job_data['cutoff_value']
    
    process_id = os.getpid()
    
    try:
        # Check if gold/ledock output already exists
        gold_output_dir = os.path.join(current_outfolder, "gold")
        ledock_output_dir = os.path.join(current_outfolder, "ledock")
        
        if (os.path.exists(gold_output_dir) and os.listdir(gold_output_dir) and
            os.path.exists(ledock_output_dir) and os.listdir(ledock_output_dir)):
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'skipped',
                'message': 'Gold/LeDock jobs already completed',
                'process_id': process_id
            }
        
        # Build the gold/ledock command (write to existing directory)
        command = [
            "python", CONSENSUS_DOCKER_SCRIPT,
            "--use_gold",
            "--use_ledock",
            "--overwrite",  # Allow writing to existing directory
            "--outfolder", current_outfolder,
            "--ledock_path", LEDOCK_PATH,
            "--lepro_path", LEPRO_PATH,
            "--gold_path", GOLD_PATH,
            "--ligand_sdf", ligand_sdf,
            "--receptor_pdb", receptor_pdb,
            "--pocket_pdb", pocket_pdb,
            "--num_threads", str(NUM_THREADS),
            "--cutoff_value", str(CUTOFF_VALUE)
        ]
        
        # Add PDBQT file if available for faster processing
        if receptor_pdbqt and os.path.exists(receptor_pdbqt):
            command.extend(["--receptor_pdbqt", receptor_pdbqt])
        
        # Run the command with timeout
        timeout_seconds = job_data.get('timeout', 600)  # Default 10 minutes
        try:
            result = subprocess.run(command, capture_output=True, text=True, 
                                  check=False, timeout=timeout_seconds)
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'timeout',
                'message': f'Gold/LeDock docking timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)',
                'process_id': process_id
            }
        
        if result.returncode == 0:
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'completed',
                'message': f'Successfully completed gold/ledock docking for {drugbank_id} into {pocket_pdb}',
                'process_id': process_id
            }
        else:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'failed',
                'message': f'Gold/LeDock docking failed. Return code: {result.returncode}',
                'stdout': result.stdout,
                'stderr': result.stderr,
                'process_id': process_id
            }
    
    except Exception as e:
        return {
            'success': False,
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            'action': 'error',
            'message': f'Exception occurred in gold/ledock docking: {str(e)}',
            'process_id': process_id
        }

# --- Part 3: Main Docking Execution Logic ---

def run_docking(
    drug_to_protein_df, 
    uniprot_map_df, 
    pdb_info_dict, 
    processed_ligand_sdf_folder,
    output_base_folder="docking_results"
):
    """
    Iterates through drug-protein pairs, constructs commands, and runs docking.
    """
    logging.info("Starting docking execution.")
    total_docking_jobs = 0
    executed_docking_jobs = 0
    skipped_jobs = 0
    
    os.makedirs(output_base_folder, exist_ok=True)

    # Pre-build mappings for faster lookup
    uniprot_gene_map = dict(zip(uniprot_map_df['Mapped_Gene_Name'], uniprot_map_df['Entry']))
    
    # Debug: Show some sample mappings
    logging.info(f"UniProt-Gene mapping contains {len(uniprot_gene_map)} entries")
    sample_gene_names = list(uniprot_gene_map.keys())[:5]
    logging.info(f"Sample gene names in UniProt mapping: {sample_gene_names}")
    sample_uniprot_ids = [uniprot_gene_map[gene] for gene in sample_gene_names]
    logging.info(f"Corresponding UniProt IDs: {sample_uniprot_ids}")

    docking_jobs = [] # List to store (drugbank_id, uniprot_id, receptor_pdb, pocket_pdb) tuples

    logging.info("Preparing list of docking jobs...")
    for index, row in tqdm(drug_to_protein_df.iterrows(), total=len(drug_to_protein_df), desc="Preparing Jobs"):
        drugbank_id = row['node_1']
        gene_name = row['node_2_name']
        
        ligand_sdf_path = os.path.join(processed_ligand_sdf_folder, f"{drugbank_id}.sdf")
        if not os.path.exists(ligand_sdf_path):
            tqdm.write(f"Warning: Ligand SDF '{ligand_sdf_path}' not found. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue

        uniprot_id = uniprot_gene_map.get(gene_name)
        if not uniprot_id:
            tqdm.write(f"Warning: UniProt ID not found for gene '{gene_name}'. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue
        
        if uniprot_id not in pdb_info_dict:
            # Enhanced debugging for missing UniProt IDs
            logging.debug(f"UniProt ID '{uniprot_id}' not found in cavity mapping. Gene: '{gene_name}', DrugBank: '{drugbank_id}'")
            
            # Check if it's a case sensitivity issue or extra whitespace
            matching_keys = [key for key in pdb_info_dict.keys() if uniprot_id.upper() == key.upper()]
            if matching_keys:
                logging.warning(f"Found case-insensitive match for '{uniprot_id}': {matching_keys[0]}. Consider fixing data consistency.")
            else:
                # Show some nearby keys for debugging
                all_keys = list(pdb_info_dict.keys())
                if all_keys:
                    nearby_keys = all_keys[:5]  # Show first 5 keys
                    logging.debug(f"Available UniProt IDs (first 5): {nearby_keys}")
            
            tqdm.write(f"Warning: No PDB information found for UniProt ID '{uniprot_id}'. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue

        # Each UniProt ID can have multiple pocket/receptor pairs
        for receptor_pdb, pocket_pdb, receptor_pdbqt in pdb_info_dict[uniprot_id]:
            # Create unique job identifier to detect duplicates
            pocket_filename = Path(pocket_pdb).stem
            cavity_match = re.search(r'_cavity_(\d+)', pocket_filename)
            cavity_num = cavity_match.group(1) if cavity_match else "unknown"
            job_signature = f"{drugbank_id}_{uniprot_id}_{cavity_num}"
            
            total_docking_jobs += 1
            docking_jobs.append({
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'ligand_sdf': ligand_sdf_path,
                'receptor_pdb': receptor_pdb,
                'receptor_pdbqt': receptor_pdbqt,  # May be None if not available
                'pocket_pdb': pocket_pdb,
                'job_signature': job_signature  # For duplicate detection
            })
    
    logging.info(f"Total potential docking jobs: {total_docking_jobs}")
    logging.info(f"Skipped {skipped_jobs} jobs due to missing files or mappings.")
    
    # Remove duplicate jobs
    docking_jobs = remove_duplicate_jobs(docking_jobs)
    
    # Save job preparation summary
    save_job_preparation_summary(total_docking_jobs, skipped_jobs, len(docking_jobs))
    
    # Preview jobs before execution
    if docking_jobs:
        preview_df = preview_docking_jobs(docking_jobs, "docking_jobs_preview.csv")
        
        # Ask user for confirmation before proceeding
        print(f"\n{'='*60}")
        print(f"DOCKING JOBS PREVIEW")
        print(f"{'='*60}")
        print(f"Total jobs to execute: {len(docking_jobs)}")
        print(f"Preview saved to: docking_jobs_preview.csv")
        print(f"{'='*60}")
        
        if not TEST_MODE and not SKIP_CONFIRMATION:
            response = input("Do you want to proceed with these docking jobs? (y/n): ").lower().strip()
            if response not in ['y', 'yes']:
                logging.info("User chose not to proceed. Exiting.")
                sys.exit(0)
        elif TEST_MODE:
            logging.info("TEST MODE: Proceeding automatically")
        elif SKIP_CONFIRMATION:
            logging.info("SKIP_CONFIRMATION enabled: Proceeding automatically with all jobs")
    
    # Limit jobs for testing if TEST_MODE is enabled
    if TEST_MODE and len(docking_jobs) > MAX_TEST_JOBS:
        logging.info(f"TEST MODE: Limiting to first {MAX_TEST_JOBS} jobs out of {len(docking_jobs)} available jobs.")
        docking_jobs = docking_jobs[:MAX_TEST_JOBS]
        # Update preview with limited jobs
        if docking_jobs:
            preview_df = preview_docking_jobs(docking_jobs, "docking_jobs_preview_test_mode.csv")

    # Preview docking jobs and save to CSV
    preview_file = "docking_jobs_preview.csv"
    preview_df = preview_docking_jobs(docking_jobs, preview_file)
    
    # Prepare job data for two-stage multiprocessing
    logging.info(f"Preparing {len(docking_jobs)} jobs for two-stage parallel execution...")
    logging.info(f"Stage 1: Smina with {SMINA_MAX_PARALLEL_JOBS} processes, exhaustiveness {SMINA_EXHAUSTIVENESS}")
    logging.info(f"Stage 2: Gold/LeDock with {GOLD_LEDOCK_MAX_PARALLEL_JOBS} processes")
    job_data_list = []
    
    for job_idx, job in enumerate(docking_jobs):
        drugbank_id = job['drugbank_id']
        uniprot_id = job['uniprot_id']
        gene_name = job['gene_name']
        ligand_sdf = job['ligand_sdf']
        receptor_pdb = job['receptor_pdb']
        receptor_pdbqt = job['receptor_pdbqt']  # May be None
        pocket_pdb = job['pocket_pdb']

        # Determine output folder name for this specific job
        pocket_filename = Path(pocket_pdb).stem
        # Extract cavity number using regex
        cavity_match = re.search(r'_cavity_(\d+)', pocket_filename)
        cavity_num = cavity_match.group(1) if cavity_match else "unknown"
        
        # Create a cleaner suffix by removing AF prefix and keeping cavity info
        pocket_suffix = re.sub(r'AF-[A-Z0-9]+(?:-F1-model_v1_)?', '', pocket_filename)
        # If the suffix is too long or complex, use a simpler naming scheme
        if len(pocket_suffix) > 50:  # Arbitrary length limit
            pocket_suffix = f"cavity_{cavity_num}"
        
        output_folder_name = f"{drugbank_id}_{gene_name}_{uniprot_id}_{pocket_suffix}"
        current_outfolder = os.path.join(output_base_folder, output_folder_name)
        
        # Prepare job data dictionary
        job_data = {
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'ligand_sdf': ligand_sdf,
            'receptor_pdb': receptor_pdb,
            'receptor_pdbqt': receptor_pdbqt,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            # Configuration parameters
            'consensus_docker_script': CONSENSUS_DOCKER_SCRIPT,
            'smina_path': SMINA_PATH,
            'ledock_path': LEDOCK_PATH,
            'lepro_path': LEPRO_PATH,
            'gold_path': GOLD_PATH,
            'num_threads': NUM_THREADS,
            'cutoff_value': CUTOFF_VALUE,
            'exhaustiveness': SMINA_EXHAUSTIVENESS,  # Use smina exhaustiveness
            'timeout': SMINA_TIMEOUT
        }
        job_data_list.append(job_data)
    
    # === STAGE 1: RUN SMINA DOCKING ===
    logging.info(f"\n=== STAGE 1: SMINA DOCKING ===")
    logging.info(f"Starting smina docking with {SMINA_MAX_PARALLEL_JOBS} processes...")
    logging.info(f"Using exhaustiveness {SMINA_EXHAUSTIVENESS}")
    logging.info(f"Timeout per job: {SMINA_TIMEOUT/60:.1f} minutes")
    
    start_time = time.time()  # Overall start time for both stages
    stage1_start_time = start_time
    stage1_completed_jobs = 0
    stage1_failed_jobs = 0
    stage1_timeout_jobs = 0
    stage1_skipped_jobs = 0
    stage1_unique_processes = set()
    last_progress_log = time.time()
    progress_log_interval = 30  # Log progress every 30 seconds
    
    with ProcessPoolExecutor(max_workers=SMINA_MAX_PARALLEL_JOBS) as executor:
        # Submit all smina jobs
        future_to_job = {executor.submit(run_single_smina_job, job_data): job_data for job_data in job_data_list}
        
        # Process results with progress bar
        completed_futures = 0
        for future in tqdm(as_completed(future_to_job), total=len(job_data_list), 
                         desc="Stage 1: Smina Docking", unit="job"):
            try:
                result = future.result()
                completed_futures += 1
                
                # Track unique processes
                if 'process_id' in result:
                    stage1_unique_processes.add(result['process_id'])
                
                if result['success']:
                    if result['action'] == 'completed':
                        stage1_completed_jobs += 1
                        logging.info(f"Smina Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                    elif result['action'] == 'skipped':
                        stage1_skipped_jobs += 1
                        logging.debug(f"Smina Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                else:
                    if result['action'] == 'timeout':
                        stage1_timeout_jobs += 1
                        logging.warning(f"Smina Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                    else:
                        stage1_failed_jobs += 1
                        logging.error(f"Smina Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                        if 'stdout' in result and result['stdout']:
                            logging.error(f"STDOUT: {result['stdout']}")
                        if 'stderr' in result and result['stderr']:
                            logging.error(f"STDERR: {result['stderr']}")
                
                # Log progress periodically to the log file
                current_time = time.time()
                if current_time - last_progress_log >= progress_log_interval:
                    percent_complete = (completed_futures / len(job_data_list)) * 100
                    elapsed_time = current_time - stage1_start_time
                    avg_time_per_job = elapsed_time / completed_futures if completed_futures > 0 else 0
                    remaining_jobs = len(job_data_list) - completed_futures
                    estimated_time_remaining = remaining_jobs * avg_time_per_job
                    
                    # Format elapsed time and ETA in human-readable format
                    def format_time(seconds):
                        if seconds < 60:
                            return f"{seconds:.0f}s"
                        elif seconds < 3600:
                            return f"{seconds/60:.1f}m"
                        else:
                            return f"{seconds/3600:.1f}h"
                    
                    elapsed_str = format_time(elapsed_time)
                    eta_str = format_time(estimated_time_remaining) if estimated_time_remaining > 0 else "N/A"
                    
                    logging.info(f"STAGE 1 PROGRESS: {percent_complete:.1f}% complete "
                               f"({completed_futures}/{len(job_data_list)} jobs processed) | "
                               f"✓{stage1_completed_jobs} completed, ✗{stage1_failed_jobs} failed, ⏱{stage1_timeout_jobs} timeout, ◊{stage1_skipped_jobs} skipped | "
                               f"Elapsed: {elapsed_str}, ETA: {eta_str}")
                    last_progress_log = current_time
                        
            except Exception as e:
                stage1_failed_jobs += 1
                completed_futures += 1
                job_data = future_to_job[future]
                logging.error(f"Exception in smina job {job_data['job_idx']+1}: {e}")
    
    # Stage 1 summary
    stage1_elapsed_time = time.time() - stage1_start_time
    logging.info(f"\n=== STAGE 1 COMPLETED ===")
    logging.info(f"Smina jobs completed successfully: {stage1_completed_jobs}")
    logging.info(f"Smina jobs skipped (already completed): {stage1_skipped_jobs}")
    logging.info(f"Smina jobs failed: {stage1_failed_jobs}")
    logging.info(f"Smina jobs timed out: {stage1_timeout_jobs}")
    logging.info(f"Stage 1 execution time: {stage1_elapsed_time:.2f} seconds")
    logging.info(f"Used {len(stage1_unique_processes)} unique processes out of {SMINA_MAX_PARALLEL_JOBS} configured")
    
    # === STAGE 2: RUN GOLD/LEDOCK DOCKING ===
    logging.info(f"\n=== STAGE 2: GOLD/LEDOCK DOCKING ===")
    logging.info(f"Starting gold/ledock docking with {GOLD_LEDOCK_MAX_PARALLEL_JOBS} processes...")
    logging.info(f"Writing to existing directories created in Stage 1")
    logging.info(f"Timeout per job: {GOLD_LEDOCK_TIMEOUT/60:.1f} minutes")
    
    stage2_start_time = time.time()
    stage2_completed_jobs = 0
    stage2_failed_jobs = 0
    stage2_timeout_jobs = 0
    stage2_skipped_jobs = 0
    stage2_unique_processes = set()
    last_progress_log = time.time()
    
    with ProcessPoolExecutor(max_workers=GOLD_LEDOCK_MAX_PARALLEL_JOBS) as executor:
        # Update job data for stage 2 (gold/ledock) with correct timeout
        stage2_job_data_list = []
        for job_data in job_data_list:
            stage2_job_data = job_data.copy()
            stage2_job_data['timeout'] = GOLD_LEDOCK_TIMEOUT
            stage2_job_data_list.append(stage2_job_data)
        
        # Submit all gold/ledock jobs
        future_to_job = {executor.submit(run_single_gold_ledock_job, job_data): job_data for job_data in stage2_job_data_list}
        
        # Process results with progress bar
        completed_futures = 0
        for future in tqdm(as_completed(future_to_job), total=len(stage2_job_data_list), 
                         desc="Stage 2: Gold/LeDock Docking", unit="job"):
            try:
                result = future.result()
                completed_futures += 1
                
                # Track unique processes
                if 'process_id' in result:
                    stage2_unique_processes.add(result['process_id'])
                
                if result['success']:
                    if result['action'] == 'completed':
                        stage2_completed_jobs += 1
                        logging.info(f"Gold/LeDock Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                    elif result['action'] == 'skipped':
                        stage2_skipped_jobs += 1
                        logging.debug(f"Gold/LeDock Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                else:
                    if result['action'] == 'timeout':
                        stage2_timeout_jobs += 1
                        logging.warning(f"Gold/LeDock Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                    else:
                        stage2_failed_jobs += 1
                        logging.error(f"Gold/LeDock Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                        if 'stdout' in result and result['stdout']:
                            logging.error(f"STDOUT: {result['stdout']}")
                        if 'stderr' in result and result['stderr']:
                            logging.error(f"STDERR: {result['stderr']}")
                
                # Log progress periodically to the log file
                current_time = time.time()
                if current_time - last_progress_log >= progress_log_interval:
                    percent_complete = (completed_futures / len(stage2_job_data_list)) * 100
                    elapsed_time = current_time - stage2_start_time
                    avg_time_per_job = elapsed_time / completed_futures if completed_futures > 0 else 0
                    remaining_jobs = len(stage2_job_data_list) - completed_futures
                    estimated_time_remaining = remaining_jobs * avg_time_per_job
                    
                    # Format elapsed time and ETA in human-readable format
                    def format_time(seconds):
                        if seconds < 60:
                            return f"{seconds:.0f}s"
                        elif seconds < 3600:
                            return f"{seconds/60:.1f}m"
                        else:
                            return f"{seconds/3600:.1f}h"
                    
                    elapsed_str = format_time(elapsed_time)
                    eta_str = format_time(estimated_time_remaining) if estimated_time_remaining > 0 else "N/A"
                    
                    logging.info(f"STAGE 2 PROGRESS: {percent_complete:.1f}% complete "
                               f"({completed_futures}/{len(stage2_job_data_list)} jobs processed) | "
                               f"✓{stage2_completed_jobs} completed, ✗{stage2_failed_jobs} failed, ⏱{stage2_timeout_jobs} timeout, ◊{stage2_skipped_jobs} skipped | "
                               f"Elapsed: {elapsed_str}, ETA: {eta_str}")
                    last_progress_log = current_time
                        
            except Exception as e:
                stage2_failed_jobs += 1
                completed_futures += 1
                job_data = future_to_job[future]
                logging.error(f"Exception in gold/ledock job {job_data['job_idx']+1}: {e}")
    
    # Calculate final statistics
    stage2_elapsed_time = time.time() - stage2_start_time
    total_elapsed_time = time.time() - start_time
    total_completed_jobs = stage1_completed_jobs + stage2_completed_jobs
    total_skipped_jobs = stage1_skipped_jobs + stage2_skipped_jobs
    total_failed_jobs = stage1_failed_jobs + stage2_failed_jobs
    total_timeout_jobs = stage1_timeout_jobs + stage2_timeout_jobs
    
    # Final summary
    logging.info(f"\n=== STAGE 2 COMPLETED ===")
    logging.info(f"Gold/LeDock jobs completed successfully: {stage2_completed_jobs}")
    logging.info(f"Gold/LeDock jobs skipped (already completed): {stage2_skipped_jobs}")
    logging.info(f"Gold/LeDock jobs failed: {stage2_failed_jobs}")
    logging.info(f"Gold/LeDock jobs timed out: {stage2_timeout_jobs}")
    logging.info(f"Stage 2 execution time: {stage2_elapsed_time:.2f} seconds")
    logging.info(f"Used {len(stage2_unique_processes)} unique processes out of {GOLD_LEDOCK_MAX_PARALLEL_JOBS} configured")
    
    logging.info(f"\n=== FINAL DOCKING SUMMARY ===")
    if TEST_MODE:
        logging.info(f"*** TEST MODE COMPLETED - RAN {min(len(docking_jobs), MAX_TEST_JOBS)} OUT OF {total_docking_jobs} POTENTIAL JOBS ***")
    logging.info(f"Total potential docking jobs (before checking existence): {total_docking_jobs}")
    logging.info(f"=== STAGE 1 (SMINA) RESULTS ===")
    logging.info(f"Smina jobs completed successfully: {stage1_completed_jobs}")
    logging.info(f"Smina jobs skipped (already completed): {stage1_skipped_jobs}")
    logging.info(f"Smina jobs failed: {stage1_failed_jobs}")
    logging.info(f"Smina jobs timed out (>{SMINA_TIMEOUT/60:.1f} minutes): {stage1_timeout_jobs}")
    logging.info(f"=== STAGE 2 (GOLD/LEDOCK) RESULTS ===")
    logging.info(f"Gold/LeDock jobs completed successfully: {stage2_completed_jobs}")
    logging.info(f"Gold/LeDock jobs skipped (already completed): {stage2_skipped_jobs}")
    logging.info(f"Gold/LeDock jobs failed: {stage2_failed_jobs}")
    logging.info(f"Gold/LeDock jobs timed out (>{GOLD_LEDOCK_TIMEOUT/60:.1f} minutes): {stage2_timeout_jobs}")
    logging.info(f"=== OVERALL RESULTS ===")
    logging.info(f"Total jobs attempted: {len(docking_jobs)}")
    logging.info(f"Total jobs skipped initially (missing ligand/uniprot/pdb info): {skipped_jobs}")
    logging.info(f"Stage 1 execution time: {stage1_elapsed_time:.2f} seconds")
    logging.info(f"Stage 2 execution time: {stage2_elapsed_time:.2f} seconds") 
    logging.info(f"Total execution time: {total_elapsed_time:.2f} seconds")
    logging.info(f"Average time per job: {total_elapsed_time/len(docking_jobs):.2f} seconds")
    logging.info(f"Two-stage docking workflow finished.")

def preview_docking_jobs(docking_jobs, preview_file="docking_jobs_preview.csv"):
    """
    Create a preview of all docking jobs and save to CSV file.
    
    Parameters
    ----------
    docking_jobs : list
        List of docking job dictionaries
    preview_file : str
        Path to save the preview CSV file
        
    Returns
    -------
    pd.DataFrame
        DataFrame containing the preview information
    """
    if not docking_jobs:
        logging.warning("No docking jobs to preview")
        return pd.DataFrame()
    
    # Create preview data
    preview_data = []
    for i, job in enumerate(docking_jobs):
        # Extract cavity number from pocket filename
        pocket_filename = Path(job['pocket_pdb']).stem
        cavity_match = re.search(r'_cavity_(\d+)', pocket_filename)
        cavity_num = cavity_match.group(1) if cavity_match else "unknown"
        
        # Check if PDBQT file exists
        pdbqt_exists = job['receptor_pdbqt'] and os.path.exists(job['receptor_pdbqt'])
        
        preview_data.append({
            'Job_Index': i + 1,
            'DrugBank_ID': job['drugbank_id'],
            'Gene_Name': job['gene_name'],
            'UniProt_ID': job['uniprot_id'],
            'Cavity_Number': cavity_num,
            'Job_Signature': job['job_signature'],
            'Ligand_SDF': job['ligand_sdf'],
            'Receptor_PDB': job['receptor_pdb'],
            'Receptor_PDBQT': job['receptor_pdbqt'] if job['receptor_pdbqt'] else 'N/A',
            'PDBQT_Exists': pdbqt_exists,
            'Pocket_PDB': job['pocket_pdb'],
            'Ligand_Exists': os.path.exists(job['ligand_sdf']),
            'Receptor_Exists': os.path.exists(job['receptor_pdb']),
            'Pocket_Exists': os.path.exists(job['pocket_pdb'])
        })
    
    # Create DataFrame
    preview_df = pd.DataFrame(preview_data)
    
    # Save to CSV
    preview_df.to_csv(preview_file, index=False)
    
    # Print summary statistics
    total_jobs = len(preview_df)
    unique_ligands = preview_df['DrugBank_ID'].nunique()
    unique_proteins = preview_df['UniProt_ID'].nunique()
    unique_cavities = len(preview_df.groupby(['UniProt_ID', 'Cavity_Number']))
    pdbqt_available = preview_df['PDBQT_Exists'].sum()
    
    print(f"\n{'='*60}")
    print(f"DOCKING JOBS PREVIEW SUMMARY")
    print(f"{'='*60}")
    print(f"Total jobs to execute: {total_jobs}")
    print(f"Unique ligands (DrugBank IDs): {unique_ligands}")
    print(f"Unique proteins (UniProt IDs): {unique_proteins}")
    print(f"Unique protein-cavity combinations: {unique_cavities}")
    print(f"Jobs with pre-converted PDBQT files: {pdbqt_available}")
    print(f"Preview saved to: {preview_file}")
    print(f"{'='*60}")
    
    return preview_df

def remove_duplicate_jobs(docking_jobs):
    """
    Remove duplicate docking jobs based on job signature.
    
    Parameters
    ----------
    docking_jobs : list
        List of docking job dictionaries
        
    Returns
    -------
    list
        List of unique docking jobs
    """
    seen_signatures = set()
    unique_jobs = []
    duplicates_removed = 0
    
    for job in docking_jobs:
        signature = job['job_signature']
        if signature not in seen_signatures:
            seen_signatures.add(signature)
            unique_jobs.append(job)
        else:
            duplicates_removed += 1
            logging.debug(f"Removed duplicate job: {signature}")
    
    if duplicates_removed > 0:
        logging.info(f"Removed {duplicates_removed} duplicate jobs")
        logging.info(f"Unique jobs remaining: {len(unique_jobs)}")
    
    return unique_jobs

def save_job_preparation_summary(total_jobs, skipped_jobs, final_jobs):
    """
    Save a summary of job preparation statistics.
    
    Parameters
    ----------
    total_jobs : int
        Total number of potential jobs
    skipped_jobs : int
        Number of jobs skipped due to missing files
    final_jobs : int
        Number of jobs after deduplication
    """
    summary_data = {
        'Metric': [
            'Total potential jobs',
            'Jobs skipped (missing files)',
            'Jobs after deduplication',
            'Duplicates removed'
        ],
        'Count': [
            total_jobs,
            skipped_jobs,
            final_jobs,
            total_jobs - skipped_jobs - final_jobs
        ]
    }
    
    summary_df = pd.DataFrame(summary_data)
    summary_file = "job_preparation_summary.csv"
    summary_df.to_csv(summary_file, index=False)
    
    logging.info(f"Job preparation summary saved to: {summary_file}")

# --- Main execution block ---
if __name__ == "__main__":
    # --- Step 0: Validate initial configurations ---
    if not os.path.exists(CONSENSUS_DOCKER_SCRIPT):
        logging.critical(f"Error: Consensus Docker script not found at '{CONSENSUS_DOCKER_SCRIPT}'. Please correct the path.")
        sys.exit(1)
    if not os.path.exists(PROCESSED_LIGAND_SDF_FOLDER):
        logging.critical(f"Error: Processed ligand SDF folder not found at '{PROCESSED_LIGAND_SDF_FOLDER}'. Please run the RDKit 3D processing first.")
        sys.exit(1)
    if not os.path.exists(DRUG_TO_PROTEIN_TSV):
        logging.critical(f"Error: Drug-to-protein TSV file not found at '{DRUG_TO_PROTEIN_TSV}'. Please correct the path.")
        sys.exit(1)
    if not os.path.exists(UNIPROT_MAPPING_CSV):
        logging.critical(f"Error: UniProt mapping file not found at '{UNIPROT_MAPPING_CSV}'. Please run prepare_uniprot_mapping.py first.")
        sys.exit(1)
    if not os.path.exists(CAVITY_MAPPING_CSV):
        logging.critical(f"Error: Cavity mapping file not found at '{CAVITY_MAPPING_CSV}'. Please run extract_cavities.py first.")
        sys.exit(1)

    # --- Step 1: Load pre-generated UniProt mapping and filter for small molecules ---
    logging.info("Step 1: Loading pre-generated UniProt mapping and filtering for small molecules.")
    try:
        drug_protein_data = pd.read_csv(DRUG_TO_PROTEIN_TSV, sep='\t')
        logging.info(f"Loaded {len(drug_protein_data)} drug-protein interactions from '{DRUG_TO_PROTEIN_TSV}'.")
        
        # Load small molecule drugs and filter interactions
        small_molecule_ids = load_small_molecule_drugs()
        initial_interactions = len(drug_protein_data)
        drug_protein_data = drug_protein_data[
            drug_protein_data['node_1'].isin(small_molecule_ids)
        ]
        logging.info(f"Filtered to small molecules: {len(drug_protein_data)}/{initial_interactions} interactions")
        
        # Load the pre-generated UniProt mapping
        uniprot_df = load_uniprot_mapping(UNIPROT_MAPPING_CSV)
        
    except Exception as e:
        logging.critical(f"Error in Step 1 (Loading data): {e}")
        sys.exit(1)

    # --- Step 2: Load pre-generated cavity mapping ---
    logging.info("Step 2: Loading pre-generated cavity mapping.")
    try:
        # Load the best available mapping (AlphaFold preferred)
        pdb_data, mapping_source = load_best_mapping_with_pdbqt()
        logging.info(f"Loaded cavity mapping from: {mapping_source}")
        
        if not pdb_data:
            logging.warning("No PDB data loaded from mapping. No docking jobs will be performed.")
            sys.exit(0) # Exit gracefully if no PDBs were found
            
        # Debug: Check for overlap between UniProt mappings and cavity data
        uniprot_gene_ids = set(uniprot_df['Entry'].values)
        cavity_uniprot_ids = set(pdb_data.keys())
        overlap = uniprot_gene_ids.intersection(cavity_uniprot_ids)
        
        logging.info(f"UniProt IDs in gene mapping: {len(uniprot_gene_ids)}")
        logging.info(f"UniProt IDs in cavity mapping: {len(cavity_uniprot_ids)}")
        logging.info(f"Overlapping UniProt IDs: {len(overlap)}")
        
        if len(overlap) == 0:
            logging.warning("No overlapping UniProt IDs found between gene mapping and cavity mapping!")
            logging.warning("This suggests a data format mismatch. Check your input files.")
        
    except Exception as e:
        logging.critical(f"Error in Step 2 (Cavity mapping loading): {e}")
        sys.exit(1)

    # --- Step 3: Run Docking ---
    logging.info("Step 3: Running consensus docking jobs.")
    try:
        docking_output_base = "consensus_docking_results"
        run_docking(drug_protein_data, uniprot_df, pdb_data, PROCESSED_LIGAND_SDF_FOLDER, docking_output_base)
    except Exception as e:
        logging.critical(f"Error in Step 3 (Docking execution): {e}")
        sys.exit(1)