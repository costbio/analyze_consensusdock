#!/usr/bin/env python3
"""
Batch Consensus Docking Script - Main Workflow
This script performs the actual docking jobs using pre-generated mappings.
Only small molecule drugs are processed, as the docking workflow is optimized for small molecules.

Prerequisites:
    1. Run prepare_uniprot_mapping.py first to generate uniprot_gene_mapping.csv
    2. Run extract_cavities.py to generate cavity_mapping.csv
    3. Run identify_required_structures.py to identify required structures (RECOMMENDED)
    4. Run extract_alphafold_models.py to get full AlphaFold structures (RECOMMENDED)
    5. Run fix_required_pdbs.py to add hydrogens to required structures (RECOMMENDED)
    6. Optionally run convert_pdb_to_pdbqt.py for better performance
    7. Ensure all required paths and dependencies are correctly configured

Usage:
    python batch_dock.py

Input:
    - uniprot_gene_mapping.csv: UniProt mapping file (generated by prepare_uniprot_mapping.py)
    - cavity_mapping.csv: Cavity mapping file (generated by extract_cavities.py)
    - small_molecule_drug_links.csv: Small molecule drugs from DrugBank
    - fixed_mapping.csv: Fixed structures mapping file (preferred, generated by fix_required_pdbs.py)
    - alphafold_mapping.csv: AlphaFold mapping file (alternative, generated by extract_alphafold_models.py)
    - pdbqt_mapping.csv: PDBQT mapping file (optional, generated by convert_pdb_to_pdbqt.py)
    - Drug-to-protein interaction TSV file
    - Processed ligand SDF folder

Output:
    - Docking results in consensus_docking_results/ folder
    - Log file: docking_automation.log
"""

import os
import sys
import pandas as pd
import subprocess
from tqdm import tqdm
from pathlib import Path
import re
import time
import logging
import glob

# --- Configuration ---
# --- IMPORTANT: ADJUST THESE PATHS AS PER YOUR SYSTEM ---
CONSENSUS_DOCKER_SCRIPT = "/home/onur/repos/consensus_docking/consensus_docker.py"
PROCESSED_LIGAND_SDF_FOLDER = "/home/onur/experiments/cavity_space_consensus_docking/drugbank_approved_split" # The output folder from your RDKit 3D processing script
DRUG_TO_PROTEIN_TSV = "/opt/data/multiscale_interactome_data/1_drug_to_protein.tsv"
SMALL_MOLECULE_DRUGS_CSV = "/opt/data/drugbank/small_molecule_drug_links.csv"  # Small molecule drugs only
UNIPROT_MAPPING_CSV = "uniprot_gene_mapping.csv" # Input file from prepare_uniprot_mapping.py
CAVITY_MAPPING_CSV = "cavity_mapping.csv" # Input file from extract_cavities.py
FIXED_MAPPING_CSV = "fixed_mapping.csv" # Preferred input from fix_required_pdbs.py
ALPHAFOLD_MAPPING_CSV = "alphafold_mapping.csv" # Alternative input from extract_alphafold_models.py
PDBQT_MAPPING_CSV = "pdbqt_mapping.csv" # Input file from convert_pdb_to_pdbqt.py (optional)
LOG_FILE = "docking_automation.log"
# --- TEST MODE: Set to True to run only 3 docking jobs for testing ---
TEST_MODE = True
MAX_TEST_JOBS = 3
# --- Consensus Docker Fixed Arguments (from your example) ---
SMINA_PATH = "/opt/anaconda3/envs/teachopencadd/bin/smina"
LEDOCK_PATH = "/home/onur/software/ledock_linux_x86"
LEPRO_PATH = "/home/onur/software/lepro_linux_x86"
GOLD_PATH = "/opt/goldsuite-5.3.0/bin/gold_auto"
NUM_THREADS = 60
CUTOFF_VALUE = -7

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

logging.info("Starting batch consensus docking workflow.")
if TEST_MODE:
    logging.info(f"*** RUNNING IN TEST MODE - LIMITED TO {MAX_TEST_JOBS} DOCKING JOBS ***")
logging.info(f"Consensus Docker Script: {CONSENSUS_DOCKER_SCRIPT}")
logging.info(f"Processed Ligand SDF Folder: {PROCESSED_LIGAND_SDF_FOLDER}")
logging.info(f"Drug-to-Protein TSV: {DRUG_TO_PROTEIN_TSV}")
logging.info(f"UniProt Mapping CSV: {UNIPROT_MAPPING_CSV}")
logging.info(f"Cavity Mapping CSV: {CAVITY_MAPPING_CSV}")
logging.info(f"PDBQT Mapping CSV: {PDBQT_MAPPING_CSV}")
if os.path.exists(PDBQT_MAPPING_CSV):
    logging.info("PDBQT mapping found - will use pre-converted PDBQT files for faster docking")
else:
    logging.info("PDBQT mapping not found - will use PDB files (slower conversion during docking)")

def load_uniprot_mapping(mapping_file):
    """
    Load pre-generated UniProt mapping from CSV file.
    
    Args:
        mapping_file (str): Path to the UniProt mapping CSV file
        
    Returns:
        pd.DataFrame: DataFrame with UniProt mapping data
    """
    if not os.path.exists(mapping_file):
        logging.critical(f"UniProt mapping file not found: {mapping_file}")
        logging.critical("Please run prepare_uniprot_mapping.py first to generate the mapping file.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['Entry', 'Mapped_Gene_Name']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.critical(f"Required columns missing from mapping file: {missing_columns}")
            logging.critical("Please regenerate the mapping file using prepare_uniprot_mapping.py")
            sys.exit(1)
        
        logging.info(f"Loaded UniProt mapping with {len(df)} entries from {mapping_file}")
        return df[['Entry', 'Mapped_Gene_Name']]
        
    except Exception as e:
        logging.critical(f"Error loading UniProt mapping file: {e}")
        sys.exit(1)

def load_small_molecule_drugs():
    """
    Load small molecule drugs list from DrugBank.
    
    Returns:
        set: Set of small molecule drug IDs
    """
    if not os.path.exists(SMALL_MOLECULE_DRUGS_CSV):
        logging.critical(f"Small molecule drugs CSV not found: {SMALL_MOLECULE_DRUGS_CSV}")
        sys.exit(1)
    
    try:
        df = pd.read_csv(SMALL_MOLECULE_DRUGS_CSV)
        small_molecule_ids = set(df['DrugBank ID'].unique())
        logging.info(f"Loaded {len(small_molecule_ids)} small molecule drug IDs")
        return small_molecule_ids
    except Exception as e:
        logging.critical(f"Error loading small molecule drugs: {e}")
        sys.exit(1)

def load_best_mapping_with_pdbqt():
    """
    Load the best available mapping file with PDBQT paths.
    Prioritizes fixed PDB mapping, then AlphaFold mapping, then original cavity mapping.
    
    Returns
    -------
    dict: Dictionary mapping UniProt IDs to lists of (receptor_pdb, pocket_pdb, receptor_pdbqt) tuples
    str: Source file used
    """
    # Define priority order
    mapping_files = [
        (FIXED_MAPPING_CSV, "fixed structures mapping"),
        (ALPHAFOLD_MAPPING_CSV, "AlphaFold mapping"),
        (CAVITY_MAPPING_CSV, "original cavity mapping")
    ]
    
    mapping_file = None
    mapping_type = None
    
    # Find the best available mapping file
    for file_path, file_type in mapping_files:
        if os.path.exists(file_path):
            mapping_file = file_path
            mapping_type = file_type
            logging.info(f"Using {mapping_type}: {mapping_file}")
            break
    
    if not mapping_file:
        logging.critical("No mapping file found!")
        logging.critical("Please run extract_cavities.py and fix_required_pdbs.py first.")
        sys.exit(1)
    
    # Load the mapping with PDBQT support
    pdbqt_file = PDBQT_MAPPING_CSV if os.path.exists(PDBQT_MAPPING_CSV) else None
    mapping_dict = load_cavity_mapping(mapping_file, pdbqt_file)
    
    return mapping_dict, mapping_file

def load_cavity_mapping(mapping_file, pdbqt_mapping_file=None):
    """
    Load cavity mapping from a pre-generated CSV file, optionally with PDBQT paths.
    
    Args:
        mapping_file (str): Path to the cavity mapping CSV file
        pdbqt_mapping_file (str, optional): Path to the PDBQT mapping CSV file
        
    Returns:
        dict: Dictionary mapping UniProt IDs to lists of (receptor_pdb, pocket_pdb, receptor_pdbqt) tuples
    """
    if not os.path.exists(mapping_file):
        logging.critical(f"Cavity mapping file not found: {mapping_file}")
        logging.critical("Please run extract_cavities.py first to generate the cavity mapping file.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['UniProt_ID', 'Receptor_PDB', 'Pocket_PDB']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.critical(f"Required columns missing from cavity mapping file: {missing_columns}")
            logging.critical("Please regenerate the cavity mapping file using extract_cavities.py")
            sys.exit(1)
        
        # Load PDBQT mapping if available
        pdbqt_mapping = {}
        if pdbqt_mapping_file and os.path.exists(pdbqt_mapping_file):
            logging.info(f"Loading PDBQT mapping from {pdbqt_mapping_file}")
            try:
                pdbqt_df = pd.read_csv(pdbqt_mapping_file)
                if 'Receptor_PDB' in pdbqt_df.columns and 'Receptor_PDBQT' in pdbqt_df.columns:
                    # Create mapping from PDB path to PDBQT path
                    pdbqt_mapping = dict(zip(pdbqt_df['Receptor_PDB'], pdbqt_df['Receptor_PDBQT']))
                    logging.info(f"Loaded {len(pdbqt_mapping)} PDBQT mappings")
                else:
                    logging.warning("PDBQT mapping file missing required columns, ignoring PDBQT files")
            except Exception as e:
                logging.warning(f"Error loading PDBQT mapping: {e}. Continuing without PDBQT files.")
        
        pdb_info = {}
        for _, row in df.iterrows():
            uniprot_id = row['UniProt_ID']
            receptor_pdb = row['Receptor_PDB']
            pocket_pdb = row['Pocket_PDB']
            
            # Verify that the files still exist
            if not os.path.exists(receptor_pdb):
                logging.warning(f"Receptor PDB file not found: {receptor_pdb}")
                continue
            if not os.path.exists(pocket_pdb):
                logging.warning(f"Pocket PDB file not found: {pocket_pdb}")
                continue
            
            # Get corresponding PDBQT file if available
            receptor_pdbqt = pdbqt_mapping.get(receptor_pdb)
            if receptor_pdbqt and not os.path.exists(receptor_pdbqt):
                logging.warning(f"PDBQT file not found: {receptor_pdbqt}, will use PDB file")
                receptor_pdbqt = None
            
            if uniprot_id not in pdb_info:
                pdb_info[uniprot_id] = []
            pdb_info[uniprot_id].append((receptor_pdb, pocket_pdb, receptor_pdbqt))
        
        total_pairs = sum(len(pairs) for pairs in pdb_info.values())
        pdbqt_pairs = sum(1 for pairs in pdb_info.values() for _, _, pdbqt in pairs if pdbqt)
        
        logging.info(f"Loaded cavity mapping for {len(pdb_info)} UniProt IDs with {total_pairs} cavity pairs from {mapping_file}")
        logging.info(f"Found {pdbqt_pairs} cavity pairs with pre-converted PDBQT files")
        
        # Debug: Show some sample UniProt IDs for debugging
        sample_ids = list(pdb_info.keys())[:5]
        logging.info(f"Sample UniProt IDs in cavity mapping: {sample_ids}")
        
        return pdb_info
        
    except Exception as e:
        logging.critical(f"Error loading cavity mapping file: {e}")
        sys.exit(1)

def check_docking_completion(output_folder):
    """
    Check if a docking job was completed successfully by examining the output folder.
    
    Args:
        output_folder (str): Path to the docking output folder
        
    Returns:
        bool: True if the job appears to be completed, False otherwise
    """
    if not os.path.exists(output_folder):
        return False
    
    # Check if the folder has any content
    if not os.listdir(output_folder):
        return False
    
    # Look for typical consensus docking output files
    # Adjust these patterns based on what consensus_docker.py actually produces
    success_indicators = [
        "consensus_results.csv",
        "consensus_scores.txt", 
        "docking_summary.log",
        "*.sdf",  # Any SDF files (docked poses)
        "*/results.txt"  # Results in subdirectories
    ]
    
    for pattern in success_indicators:
        if glob.glob(os.path.join(output_folder, pattern)):
            logging.debug(f"Found completion indicator: {pattern} in {output_folder}")
            return True
    
    # If no specific indicators, check if there are multiple files/folders
    # indicating some substantial output was generated
    contents = os.listdir(output_folder)
    if len(contents) >= 3:  # Arbitrary threshold - adjust as needed
        logging.debug(f"Output folder {output_folder} has {len(contents)} items, considering complete")
        return True
    
    return False

# --- NOTE: Cavity extraction is now handled by extract_cavities.py ---
# Run extract_cavities.py first to generate cavity_mapping.csv before running this script

# --- Part 3: Main Docking Execution Logic ---

def run_docking(
    drug_to_protein_df, 
    uniprot_map_df, 
    pdb_info_dict, 
    processed_ligand_sdf_folder,
    output_base_folder="docking_results"
):
    """
    Iterates through drug-protein pairs, constructs commands, and runs docking.
    """
    logging.info("Starting docking execution.")
    total_docking_jobs = 0
    executed_docking_jobs = 0
    skipped_jobs = 0
    
    os.makedirs(output_base_folder, exist_ok=True)

    # Pre-build mappings for faster lookup
    uniprot_gene_map = dict(zip(uniprot_map_df['Mapped_Gene_Name'], uniprot_map_df['Entry']))
    
    # Debug: Show some sample mappings
    logging.info(f"UniProt-Gene mapping contains {len(uniprot_gene_map)} entries")
    sample_gene_names = list(uniprot_gene_map.keys())[:5]
    logging.info(f"Sample gene names in UniProt mapping: {sample_gene_names}")
    sample_uniprot_ids = [uniprot_gene_map[gene] for gene in sample_gene_names]
    logging.info(f"Corresponding UniProt IDs: {sample_uniprot_ids}")

    docking_jobs = [] # List to store (drugbank_id, uniprot_id, receptor_pdb, pocket_pdb) tuples

    logging.info("Preparing list of docking jobs...")
    for index, row in tqdm(drug_to_protein_df.iterrows(), total=len(drug_to_protein_df), desc="Preparing Jobs"):
        drugbank_id = row['node_1']
        gene_name = row['node_2_name']
        
        ligand_sdf_path = os.path.join(processed_ligand_sdf_folder, f"{drugbank_id}.sdf")
        if not os.path.exists(ligand_sdf_path):
            tqdm.write(f"Warning: Ligand SDF '{ligand_sdf_path}' not found. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue

        uniprot_id = uniprot_gene_map.get(gene_name)
        if not uniprot_id:
            tqdm.write(f"Warning: UniProt ID not found for gene '{gene_name}'. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue
        
        if uniprot_id not in pdb_info_dict:
            # Enhanced debugging for missing UniProt IDs
            logging.debug(f"UniProt ID '{uniprot_id}' not found in cavity mapping. Gene: '{gene_name}', DrugBank: '{drugbank_id}'")
            
            # Check if it's a case sensitivity issue or extra whitespace
            matching_keys = [key for key in pdb_info_dict.keys() if uniprot_id.upper() == key.upper()]
            if matching_keys:
                logging.warning(f"Found case-insensitive match for '{uniprot_id}': {matching_keys[0]}. Consider fixing data consistency.")
            else:
                # Show some nearby keys for debugging
                all_keys = list(pdb_info_dict.keys())
                if all_keys:
                    nearby_keys = all_keys[:5]  # Show first 5 keys
                    logging.debug(f"Available UniProt IDs (first 5): {nearby_keys}")
            
            tqdm.write(f"Warning: No PDB information found for UniProt ID '{uniprot_id}'. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue

        # Each UniProt ID can have multiple pocket/receptor pairs
        for receptor_pdb, pocket_pdb, receptor_pdbqt in pdb_info_dict[uniprot_id]:
            total_docking_jobs += 1
            docking_jobs.append({
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'ligand_sdf': ligand_sdf_path,
                'receptor_pdb': receptor_pdb,
                'receptor_pdbqt': receptor_pdbqt,  # May be None if not available
                'pocket_pdb': pocket_pdb
            })
    
    logging.info(f"Total potential docking jobs: {total_docking_jobs}")
    logging.info(f"Skipped {skipped_jobs} jobs due to missing files or mappings.")
    
    # Limit jobs for testing if TEST_MODE is enabled
    if TEST_MODE and len(docking_jobs) > MAX_TEST_JOBS:
        logging.info(f"TEST MODE: Limiting to first {MAX_TEST_JOBS} jobs out of {len(docking_jobs)} available jobs.")
        docking_jobs = docking_jobs[:MAX_TEST_JOBS]

    start_time = time.time()
    for job_idx, job in enumerate(tqdm(docking_jobs, desc="Executing Docking Jobs", unit="job")):
        drugbank_id = job['drugbank_id']
        uniprot_id = job['uniprot_id']
        gene_name = job['gene_name']
        ligand_sdf = job['ligand_sdf']
        receptor_pdb = job['receptor_pdb']
        receptor_pdbqt = job['receptor_pdbqt']  # May be None
        pocket_pdb = job['pocket_pdb']

        # Determine output folder name for this specific job
        # Example: DB12010_ACVR1_Q16539_cavity_result_1, DB12010_ACVR1_Q16539_cavity_result_2, etc.
        # We need to extract the specific cavity ID from the pocket_pdb filename
        pocket_filename = Path(pocket_pdb).stem
        # Extract cavity number using regex
        cavity_match = re.search(r'_cavity_(\d+)', pocket_filename)
        cavity_num = cavity_match.group(1) if cavity_match else "unknown"
        
        # Create a cleaner suffix by removing AF prefix and keeping cavity info
        pocket_suffix = re.sub(r'AF-[A-Z0-9]+(?:-F1-model_v1_)?', '', pocket_filename)
        # If the suffix is too long or complex, use a simpler naming scheme
        if len(pocket_suffix) > 50:  # Arbitrary length limit
            pocket_suffix = f"cavity_{cavity_num}"
        
        output_folder_name = f"{drugbank_id}_{gene_name}_{uniprot_id}_{pocket_suffix}"
        current_outfolder = os.path.join(output_base_folder, output_folder_name)
        
        # Check if the job was already completed using our improved check
        if check_docking_completion(current_outfolder):
            tqdm.write(f"Skipping already completed job: {current_outfolder}")
            executed_docking_jobs += 1 # Count as completed, not necessarily executed in this run
            continue
        
        # Note: Don't create the output folder here - consensus_docker.py creates it itself
        # and will error if the folder already exists

        command = [
            "python", CONSENSUS_DOCKER_SCRIPT,
            "--outfolder", current_outfolder,
            "--smina_path", SMINA_PATH,
            "--ledock_path", LEDOCK_PATH,
            "--lepro_path", LEPRO_PATH,
            "--gold_path", GOLD_PATH,
            "--ligand_sdf", ligand_sdf,
            "--receptor_pdb", receptor_pdb,
            "--pocket_pdb", pocket_pdb,
            "--num_threads", str(NUM_THREADS),
            "--cutoff_value", str(CUTOFF_VALUE)
        ]
        
        # Add PDBQT file if available for faster processing
        if receptor_pdbqt and os.path.exists(receptor_pdbqt):
            command.extend(["--receptor_pdbqt", receptor_pdbqt])
            logging.debug(f"Using pre-converted PDBQT file: {receptor_pdbqt}")
        else:
            logging.debug(f"No PDBQT file available, consensus_docker.py will convert from PDB")

        try:
            # Use subprocess.run for better control and error handling
            result = subprocess.run(command, capture_output=True, text=True, check=False)
            
            if result.returncode == 0:
                logging.info(f"Successfully docked {drugbank_id} into {pocket_pdb}. Output in: {current_outfolder}")
                executed_docking_jobs += 1
            else:
                logging.error(f"Docking failed for {drugbank_id} into {pocket_pdb}. Return code: {result.returncode}")
                logging.error(f"STDOUT: {result.stdout}")
                logging.error(f"STDERR: {result.stderr}")
                tqdm.write(f"Error: Docking failed for {drugbank_id} into {pocket_pdb}. Check log for details.")

        except FileNotFoundError:
            logging.error(f"Error: Command not found. Is '{CONSENSUS_DOCKER_SCRIPT}' correct and executable?")
            sys.exit(1)
        except Exception as e:
            logging.error(f"An unexpected error occurred during docking for {drugbank_id} into {pocket_pdb}: {e}")
            tqdm.write(f"Error: An unexpected error occurred. Check log for details.")

        # ETA Calculation
        elapsed_time = time.time() - start_time
        avg_time_per_job = elapsed_time / (job_idx + 1)
        remaining_jobs = len(docking_jobs) - (job_idx + 1)
        eta_seconds = avg_time_per_job * remaining_jobs
        
        m, s = divmod(eta_seconds, 60)
        h, m = divmod(m, 60)
        tqdm.write(f"Estimated time remaining: {int(h)}h {int(m)}m {int(s)}s")

    logging.info(f"\n--- Docking Summary ---")
    if TEST_MODE:
        logging.info(f"*** TEST MODE COMPLETED - RAN {min(len(docking_jobs), MAX_TEST_JOBS)} OUT OF {total_docking_jobs} POTENTIAL JOBS ***")
    logging.info(f"Total potential docking jobs (before checking existence): {total_docking_jobs}")
    logging.info(f"Total jobs attempted/completed (including skips if output exists): {executed_docking_jobs}")
    logging.info(f"Total jobs skipped initially (missing ligand/uniprot/pdb info): {skipped_jobs}")
    logging.info(f"Script finished.")

# --- Main execution block ---
if __name__ == "__main__":
    # --- Step 0: Validate initial configurations ---
    if not os.path.exists(CONSENSUS_DOCKER_SCRIPT):
        logging.critical(f"Error: Consensus Docker script not found at '{CONSENSUS_DOCKER_SCRIPT}'. Please correct the path.")
        sys.exit(1)
    if not os.path.exists(PROCESSED_LIGAND_SDF_FOLDER):
        logging.critical(f"Error: Processed ligand SDF folder not found at '{PROCESSED_LIGAND_SDF_FOLDER}'. Please run the RDKit 3D processing first.")
        sys.exit(1)
    if not os.path.exists(DRUG_TO_PROTEIN_TSV):
        logging.critical(f"Error: Drug-to-protein TSV file not found at '{DRUG_TO_PROTEIN_TSV}'. Please correct the path.")
        sys.exit(1)
    if not os.path.exists(UNIPROT_MAPPING_CSV):
        logging.critical(f"Error: UniProt mapping file not found at '{UNIPROT_MAPPING_CSV}'. Please run prepare_uniprot_mapping.py first.")
        sys.exit(1)
    if not os.path.exists(CAVITY_MAPPING_CSV):
        logging.critical(f"Error: Cavity mapping file not found at '{CAVITY_MAPPING_CSV}'. Please run extract_cavities.py first.")
        sys.exit(1)

    # --- Step 1: Load pre-generated UniProt mapping and filter for small molecules ---
    logging.info("Step 1: Loading pre-generated UniProt mapping and filtering for small molecules.")
    try:
        drug_protein_data = pd.read_csv(DRUG_TO_PROTEIN_TSV, sep='\t')
        logging.info(f"Loaded {len(drug_protein_data)} drug-protein interactions from '{DRUG_TO_PROTEIN_TSV}'.")
        
        # Load small molecule drugs and filter interactions
        small_molecule_ids = load_small_molecule_drugs()
        initial_interactions = len(drug_protein_data)
        drug_protein_data = drug_protein_data[
            drug_protein_data['node_1'].isin(small_molecule_ids)
        ]
        logging.info(f"Filtered to small molecules: {len(drug_protein_data)}/{initial_interactions} interactions")
        
        # Load the pre-generated UniProt mapping
        uniprot_df = load_uniprot_mapping(UNIPROT_MAPPING_CSV)
        
    except Exception as e:
        logging.critical(f"Error in Step 1 (Loading data): {e}")
        sys.exit(1)

    # --- Step 2: Load pre-generated cavity mapping ---
    logging.info("Step 2: Loading pre-generated cavity mapping.")
    try:
        # Load the best available mapping (AlphaFold preferred)
        pdb_data, mapping_source = load_best_mapping_with_pdbqt()
        logging.info(f"Loaded cavity mapping from: {mapping_source}")
        
        if not pdb_data:
            logging.warning("No PDB data loaded from mapping. No docking jobs will be performed.")
            sys.exit(0) # Exit gracefully if no PDBs were found
            
        # Debug: Check for overlap between UniProt mappings and cavity data
        uniprot_gene_ids = set(uniprot_df['Entry'].values)
        cavity_uniprot_ids = set(pdb_data.keys())
        overlap = uniprot_gene_ids.intersection(cavity_uniprot_ids)
        
        logging.info(f"UniProt IDs in gene mapping: {len(uniprot_gene_ids)}")
        logging.info(f"UniProt IDs in cavity mapping: {len(cavity_uniprot_ids)}")
        logging.info(f"Overlapping UniProt IDs: {len(overlap)}")
        
        if len(overlap) == 0:
            logging.warning("No overlapping UniProt IDs found between gene mapping and cavity mapping!")
            logging.warning("This suggests a data format mismatch. Check your input files.")
        
    except Exception as e:
        logging.critical(f"Error in Step 2 (Cavity mapping loading): {e}")
        sys.exit(1)

    # --- Step 3: Run Docking ---
    logging.info("Step 3: Running consensus docking jobs.")
    try:
        docking_output_base = "consensus_docking_results"
        run_docking(drug_protein_data, uniprot_df, pdb_data, PROCESSED_LIGAND_SDF_FOLDER, docking_output_base)
    except Exception as e:
        logging.critical(f"Error in Step 3 (Docking execution): {e}")
        sys.exit(1)