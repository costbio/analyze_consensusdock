#!/usr/bin/env python3
"""
Batch Consensus Docking Script - Main Workflow
This script performs the actual docking jobs using pre-generated mappings.
Only small molecule drugs are processed, as the docking workflow is optimized for small molecules.

Prerequisites:
    1. Run prepare_uniprot_mapping.py first to generate uniprot_gene_mapping.csv
    2. Run extract_cavities.py to generate cavity_mapping.csv
    3. Run identify_required_structures.py to identify required structures (RECOMMENDED)
    4. Run extract_alphafold_models.py to get full AlphaFold structures (RECOMMENDED)
    5. Run fix_required_pdbs.py to add hydrogens to required structures (RECOMMENDED)
    6. Optionally run convert_pdb_to_pdbqt.py for better performance
    7. Ensure all required paths and dependencies are correctly configured

Usage:
    python batch_dock.py

Input:
    - uniprot_gene_mapping.csv: UniProt mapping file (generated by prepare_uniprot_mapping.py)
    - cavity_mapping.csv: Cavity mapping file (generated by extract_cavities.py)
    - small_molecule_drug_links.csv: Small molecule drugs from DrugBank
    - fixed_mapping.csv: Fixed structures mapping file (preferred, generated by fix_required_pdbs.py)
    - alphafold_mapping.csv: AlphaFold mapping file (alternative, generated by extract_alphafold_models.py)
    - pdbqt_mapping.csv: PDBQT mapping file (optional, generated by convert_pdb_to_pdbqt.py)
    - Drug-to-protein interaction TSV file
    - Processed ligand SDF folder

Output:
    - Docking results in consensus_docking_results/ folder
    - Log file: docking_automation.log
"""

import os
import sys
import pandas as pd
import subprocess
import re
import time
import logging
import glob
import multiprocessing
from tqdm import tqdm
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed

# --- Configuration ---
# --- IMPORTANT: ADJUST THESE PATHS AS PER YOUR SYSTEM ---
CONSENSUS_DOCKER_SCRIPT = "/home/onur/repos/consensus_docking/consensus_docker.py"
PROCESSED_LIGAND_SDF_FOLDER = "/media/onur/Elements/cavity_space_consensus_docking/drugbank_approved_split" # The output folder from your RDKit 3D processing script
DRUG_TO_PROTEIN_TSV = "/opt/data/multiscale_interactome_data/1_drug_to_protein.tsv"
SMALL_MOLECULE_DRUGS_CSV = "/opt/data/drugbank/small_molecule_drug_links.csv"  # Small molecule drugs only
UNIPROT_MAPPING_CSV = "uniprot_gene_mapping.csv" # Input file from prepare_uniprot_mapping.py
CAVITY_MAPPING_CSV = "cavity_mapping.csv" # Input file from extract_cavities.py
FIXED_MAPPING_CSV = "fixed_mapping.csv" # Preferred input from fix_required_pdbs.py
ALPHAFOLD_MAPPING_CSV = "alphafold_mapping.csv" # Alternative input from extract_alphafold_models.py
PDBQT_MAPPING_CSV = "pdbqt_mapping.csv" # Input file from convert_pdb_to_pdbqt.py (optional)
LOG_FILE = "docking_automation.log"
# --- TEST MODE: Set to True to run only 3 docking jobs for testing ---
TEST_MODE = False
MAX_TEST_JOBS = 40
# --- CONFIRMATION PROMPT: Set to True to skip user confirmation when not in TEST_MODE ---
SKIP_CONFIRMATION = True  # Set to True for nohup/batch execution
# --- THREE-STAGE DOCKING CONFIGURATION ---
# Stage 1: Smina only (high exhaustiveness, low parallelism)
SMINA_MAX_PARALLEL_JOBS = 4     # Maximum parallel jobs for smina stage
SMINA_EXHAUSTIVENESS = 16       # High exhaustiveness for smina
SMINA_TIMEOUT = 300           # Timeout for smina jobs in seconds (5 minutes)
# Stage 2: Gold (medium parallelism, standard exhaustiveness)
GOLD_MAX_PARALLEL_JOBS = 60     # Maximum parallel jobs for gold stage
GOLD_EXHAUSTIVENESS = 12        # Standard exhaustiveness for gold
GOLD_TIMEOUT = 600              # Timeout for gold jobs in seconds (10 minutes)
# Stage 3: LeDock (high parallelism, standard exhaustiveness)
LEDOCK_MAX_PARALLEL_JOBS = 60   # Maximum parallel jobs for ledock stage
LEDOCK_EXHAUSTIVENESS = 12      # Standard exhaustiveness for ledock
LEDOCK_TIMEOUT = 600            # Timeout for ledock jobs in seconds (10 minutes)
# Stage 4: RMSD calculation (if all tools completed but final results missing)
RMSD_MAX_PARALLEL_JOBS = 60     # Maximum parallel jobs for RMSD calculation
RMSD_TIMEOUT = 300              # Timeout for RMSD calculation in seconds (5 minutes)
# --- Consensus Docker Fixed Arguments (from your example) ---
SMINA_PATH = "/opt/anaconda3/envs/teachopencadd/bin/smina"
LEDOCK_PATH = "/home/onur/software/ledock_linux_x86"
LEPRO_PATH = "/home/onur/software/lepro_linux_x86"
GOLD_PATH = "/opt/goldsuite-5.3.0/bin/gold_auto"
NUM_THREADS = 60
CUTOFF_VALUE = -7

# Set up logging
logging.basicConfig(filename=LOG_FILE, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

logging.info("Starting batch consensus docking workflow.")
if TEST_MODE:
    logging.info(f"*** RUNNING IN TEST MODE - LIMITED TO {MAX_TEST_JOBS} DOCKING JOBS ***")
else:
    logging.info("*** RUNNING IN PRODUCTION MODE ***")
if SKIP_CONFIRMATION:
    logging.info("*** SKIP_CONFIRMATION ENABLED - NO USER PROMPT REQUIRED ***")
logging.info(f"*** THREE-STAGE PARALLEL EXECUTION ENABLED ***")
logging.info(f"*** STAGE 1: SMINA - UP TO {SMINA_MAX_PARALLEL_JOBS} CONCURRENT JOBS, EXHAUSTIVENESS {SMINA_EXHAUSTIVENESS}, TIMEOUT {SMINA_TIMEOUT/60:.1f} MINUTES ***")
logging.info(f"*** STAGE 2: GOLD - UP TO {GOLD_MAX_PARALLEL_JOBS} CONCURRENT JOBS, EXHAUSTIVENESS {GOLD_EXHAUSTIVENESS}, TIMEOUT {GOLD_TIMEOUT/60:.1f} MINUTES ***")
logging.info(f"*** STAGE 3: LEDOCK - UP TO {LEDOCK_MAX_PARALLEL_JOBS} CONCURRENT JOBS, EXHAUSTIVENESS {LEDOCK_EXHAUSTIVENESS}, TIMEOUT {LEDOCK_TIMEOUT/60:.1f} MINUTES ***")
logging.info(f"*** STAGE 4: RMSD CALCULATION - UP TO {RMSD_MAX_PARALLEL_JOBS} CONCURRENT JOBS, TIMEOUT {RMSD_TIMEOUT/60:.1f} MINUTES ***")
logging.info(f"Consensus Docker Script: {CONSENSUS_DOCKER_SCRIPT}")
logging.info(f"Processed Ligand SDF Folder: {PROCESSED_LIGAND_SDF_FOLDER}")
logging.info(f"Drug-to-Protein TSV: {DRUG_TO_PROTEIN_TSV}")
logging.info(f"UniProt Mapping CSV: {UNIPROT_MAPPING_CSV}")
logging.info(f"Cavity Mapping CSV: {CAVITY_MAPPING_CSV}")
logging.info(f"PDBQT Mapping CSV: {PDBQT_MAPPING_CSV}")
if os.path.exists(PDBQT_MAPPING_CSV):
    logging.info("PDBQT mapping found - will use pre-converted PDBQT files for faster docking")
else:
    logging.info("PDBQT mapping not found - will use PDB files (slower conversion during docking)")

def load_uniprot_mapping(mapping_file):
    """
    Load pre-generated UniProt mapping from CSV file.
    
    Args:
        mapping_file (str): Path to the UniProt mapping CSV file
        
    Returns:
        pd.DataFrame: DataFrame with UniProt mapping data
    """
    if not os.path.exists(mapping_file):
        logging.critical(f"UniProt mapping file not found: {mapping_file}")
        logging.critical("Please run prepare_uniprot_mapping.py first to generate the mapping file.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['Entry', 'Mapped_Gene_Name']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.critical(f"Required columns missing from mapping file: {missing_columns}")
            logging.critical("Please regenerate the mapping file using prepare_uniprot_mapping.py")
            sys.exit(1)
        
        logging.info(f"Loaded UniProt mapping with {len(df)} entries from {mapping_file}")
        return df[['Entry', 'Mapped_Gene_Name']]
        
    except Exception as e:
        logging.critical(f"Error loading UniProt mapping file: {e}")
        sys.exit(1)

def load_small_molecule_drugs():
    """
    Load small molecule drugs list from DrugBank.
    
    Returns:
        set: Set of small molecule drug IDs
    """
    if not os.path.exists(SMALL_MOLECULE_DRUGS_CSV):
        logging.critical(f"Small molecule drugs CSV not found: {SMALL_MOLECULE_DRUGS_CSV}")
        sys.exit(1)
    
    try:
        df = pd.read_csv(SMALL_MOLECULE_DRUGS_CSV)
        small_molecule_ids = set(df['DrugBank ID'].unique())
        logging.info(f"Loaded {len(small_molecule_ids)} small molecule drug IDs")
        return small_molecule_ids
    except Exception as e:
        logging.critical(f"Error loading small molecule drugs: {e}")
        sys.exit(1)

def load_best_mapping_with_pdbqt():
    """
    Load the best available mapping file with PDBQT paths.
    Prioritizes fixed PDB mapping, then AlphaFold mapping, then original cavity mapping.
    
    Returns
    -------
    dict: Dictionary mapping UniProt IDs to lists of (receptor_pdb, pocket_pdb, receptor_pdbqt) tuples
    str: Source file used
    """
    # Define priority order
    mapping_files = [
        (FIXED_MAPPING_CSV, "fixed structures mapping"),
        (ALPHAFOLD_MAPPING_CSV, "AlphaFold mapping"),
        (CAVITY_MAPPING_CSV, "original cavity mapping")
    ]
    
    mapping_file = None
    mapping_type = None
    
    # Find the best available mapping file
    for file_path, file_type in mapping_files:
        if os.path.exists(file_path):
            mapping_file = file_path
            mapping_type = file_type
            logging.info(f"Using {mapping_type}: {mapping_file}")
            break
    
    if not mapping_file:
        logging.critical("No mapping file found!")
        logging.critical("Please run extract_cavities.py and fix_required_pdbs.py first.")
        sys.exit(1)
    
    # Load the mapping with PDBQT support
    pdbqt_file = PDBQT_MAPPING_CSV if os.path.exists(PDBQT_MAPPING_CSV) else None
    mapping_dict = load_cavity_mapping(mapping_file, pdbqt_file)
    
    return mapping_dict, mapping_file

def load_cavity_mapping(mapping_file, pdbqt_mapping_file=None):
    """
    Load cavity mapping from a pre-generated CSV file, optionally with PDBQT paths.
    
    Args:
        mapping_file (str): Path to the cavity mapping CSV file
        pdbqt_mapping_file (str, optional): Path to the PDBQT mapping CSV file
        
    Returns:
        dict: Dictionary mapping UniProt IDs to lists of (receptor_pdb, pocket_pdb, receptor_pdbqt) tuples
    """
    if not os.path.exists(mapping_file):
        logging.critical(f"Cavity mapping file not found: {mapping_file}")
        logging.critical("Please run extract_cavities.py first to generate the cavity mapping file.")
        sys.exit(1)
    
    try:
        df = pd.read_csv(mapping_file)
        required_columns = ['UniProt_ID', 'Receptor_PDB', 'Pocket_PDB']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logging.critical(f"Required columns missing from cavity mapping file: {missing_columns}")
            logging.critical("Please regenerate the cavity mapping file using extract_cavities.py")
            sys.exit(1)
        
        # Load PDBQT mapping if available
        pdbqt_mapping = {}
        if pdbqt_mapping_file and os.path.exists(pdbqt_mapping_file):
            logging.info(f"Loading PDBQT mapping from {pdbqt_mapping_file}")
            try:
                pdbqt_df = pd.read_csv(pdbqt_mapping_file)
                if 'Receptor_PDB' in pdbqt_df.columns and 'Receptor_PDBQT' in pdbqt_df.columns:
                    # Create mapping from PDB path to PDBQT path
                    pdbqt_mapping = dict(zip(pdbqt_df['Receptor_PDB'], pdbqt_df['Receptor_PDBQT']))
                    logging.info(f"Loaded {len(pdbqt_mapping)} PDBQT mappings")
                else:
                    logging.warning("PDBQT mapping file missing required columns, ignoring PDBQT files")
            except Exception as e:
                logging.warning(f"Error loading PDBQT mapping: {e}. Continuing without PDBQT files.")
        
        pdb_info = {}
        for _, row in df.iterrows():
            uniprot_id = row['UniProt_ID']
            receptor_pdb = row['Receptor_PDB']
            pocket_pdb = row['Pocket_PDB']
            
            # Verify that the files still exist
            if not os.path.exists(receptor_pdb):
                logging.warning(f"Receptor PDB file not found: {receptor_pdb}")
                continue
            if not os.path.exists(pocket_pdb):
                logging.warning(f"Pocket PDB file not found: {pocket_pdb}")
                continue
            
            # Get corresponding PDBQT file if available
            receptor_pdbqt = pdbqt_mapping.get(receptor_pdb)
            if receptor_pdbqt and not os.path.exists(receptor_pdbqt):
                logging.warning(f"PDBQT file not found: {receptor_pdbqt}, will use PDB file")
                receptor_pdbqt = None
            
            if uniprot_id not in pdb_info:
                pdb_info[uniprot_id] = []
            pdb_info[uniprot_id].append((receptor_pdb, pocket_pdb, receptor_pdbqt))
        
        total_pairs = sum(len(pairs) for pairs in pdb_info.values())
        pdbqt_pairs = sum(1 for pairs in pdb_info.values() for _, _, pdbqt in pairs if pdbqt)
        
        logging.info(f"Loaded cavity mapping for {len(pdb_info)} UniProt IDs with {total_pairs} cavity pairs from {mapping_file}")
        logging.info(f"Found {pdbqt_pairs} cavity pairs with pre-converted PDBQT files")
        
        # Debug: Show some sample UniProt IDs for debugging
        sample_ids = list(pdb_info.keys())[:5]
        logging.info(f"Sample UniProt IDs in cavity mapping: {sample_ids}")
        
        return pdb_info
        
    except Exception as e:
        logging.critical(f"Error loading cavity mapping file: {e}")
        sys.exit(1)

def check_tool_completion(output_folder, tool_name):
    """
    Check if a specific docking tool has completed successfully.
    
    Args:
        output_folder (str): Path to the docking output folder
        tool_name (str): Name of the tool ('smina', 'gold', 'ledock')
        
    Returns:
        bool: True if the tool has completed with valid results, False otherwise
    """
    if not os.path.exists(output_folder):
        return False
    
    # Check for tool-specific results file
    results_file = os.path.join(output_folder, tool_name, "results.csv")
    if os.path.exists(results_file) and os.path.getsize(results_file) > 100:
        return True
    
    # Check for tool-specific output directory with valid files
    tool_output_dir = os.path.join(output_folder, tool_name)
    if not os.path.exists(tool_output_dir) or not os.listdir(tool_output_dir):
        return False
    
    # Look for valid output files specific to each tool
    valid_files = 0
    for file in os.listdir(tool_output_dir):
        file_path = os.path.join(tool_output_dir, file)
        if not os.path.isfile(file_path) or os.path.getsize(file_path) == 0:
            continue
            
        # Tool-specific validation
        if tool_name == "smina":
            if file.endswith(('.sdf', '.pdb', '.mol2', '.log')):
                valid_files += 1
        elif tool_name == "gold":
            if file.endswith(('.sdf', '.mol2', '.pdb', '.log')):
                valid_files += 1
        elif tool_name == "ledock":
            if file.endswith(('.sdf', '.dok', '.pdb', '.log')):
                valid_files += 1
    
    return valid_files > 0

def check_final_results_completion(output_folder):
    """
    Check if final consensus results with RMSD calculations are complete.
    
    Args:
        output_folder (str): Path to the docking output folder
        
    Returns:
        bool: True if final results are complete, False otherwise
    """
    if not os.path.exists(output_folder):
        return False
    
    # Check for final results file with RMSD data
    final_results_file = os.path.join(output_folder, "final_results.csv")
    if os.path.exists(final_results_file) and os.path.getsize(final_results_file) > 200:
        # Verify that the file contains RMSD data
        try:
            with open(final_results_file, 'r') as f:
                header = f.readline().strip()
                if 'rmsd' in header.lower() or 'consensus' in header.lower():
                    return True
        except:
            pass
    
    return False

def get_job_completion_status(output_folder):
    """
    Get the completion status of all docking tools and final results.
    
    Args:
        output_folder (str): Path to the docking output folder
        
    Returns:
        dict: Dictionary with completion status for each tool and final results
    """
    return {
        'smina': check_tool_completion(output_folder, 'smina'),
        'gold': check_tool_completion(output_folder, 'gold'),
        'ledock': check_tool_completion(output_folder, 'ledock'),
        'final_results': check_final_results_completion(output_folder)
    }

# --- NOTE: Cavity extraction is now handled by extract_cavities.py ---
# Run extract_cavities.py first to generate cavity_mapping.csv before running this script

def run_single_smina_job(job_data):
    """
    Run a single smina docking job - Stage 1 of three-stage docking.
    
    Parameters
    ----------
    job_data : dict
        Dictionary containing all job parameters
        
    Returns
    -------
    dict
        Results of the smina docking job
    """
    # Extract job parameters
    job_idx = job_data['job_idx']
    drugbank_id = job_data['drugbank_id']
    uniprot_id = job_data['uniprot_id']
    gene_name = job_data['gene_name']
    ligand_sdf = job_data['ligand_sdf']
    receptor_pdb = job_data['receptor_pdb']
    receptor_pdbqt = job_data['receptor_pdbqt']
    pocket_pdb = job_data['pocket_pdb']
    current_outfolder = job_data['current_outfolder']
    
    # Get configuration from job_data
    CONSENSUS_DOCKER_SCRIPT = job_data['consensus_docker_script']
    SMINA_PATH = job_data['smina_path']
    NUM_THREADS = job_data['num_threads']
    CUTOFF_VALUE = job_data['cutoff_value']
    EXHAUSTIVENESS = job_data.get('exhaustiveness', 8)  # Default to 8 if not set
    
    process_id = os.getpid()
    
    try:
        # Check if smina output already exists
        if check_tool_completion(current_outfolder, 'smina'):
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'skipped',
                'message': 'Smina job already completed with valid output',
                'process_id': process_id
            }
        
        # Build the smina-only command
        command = [
            "python", CONSENSUS_DOCKER_SCRIPT,
            "--use_smina",
            "--overwrite",  # Allow writing to existing directory
            "--outfolder", current_outfolder,
            "--smina_path", SMINA_PATH,
            "--ligand_sdf", ligand_sdf,
            "--receptor_pdb", receptor_pdb,
            "--pocket_pdb", pocket_pdb,
            "--num_threads", str(NUM_THREADS),
            "--cutoff_value", str(CUTOFF_VALUE),
            "--exhaustiveness", str(EXHAUSTIVENESS)
        ]
        
        # Add PDBQT file if available for faster processing
        if receptor_pdbqt and os.path.exists(receptor_pdbqt):
            command.extend(["--receptor_pdbqt", receptor_pdbqt])
        
        # Run the command with timeout
        timeout_seconds = job_data.get('timeout', 600)  # Default 10 minutes
        try:
            result = subprocess.run(command, capture_output=True, text=True, 
                                  check=False, timeout=timeout_seconds)
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'timeout',
                'message': f'Smina docking timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)',
                'process_id': process_id
            }
        
        if result.returncode == 0:
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'completed',
                'message': f'Successfully completed smina docking for {drugbank_id} into {pocket_pdb}',
                'process_id': process_id
            }
        else:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'failed',
                'message': f'Smina docking failed. Return code: {result.returncode}',
                'stdout': result.stdout,
                'stderr': result.stderr,
                'process_id': process_id
            }
    
    except Exception as e:
        return {
            'success': False,
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            'action': 'error',
            'message': f'Exception occurred in smina docking: {str(e)}',
            'process_id': process_id
        }

def run_single_gold_job(job_data):
    """
    Run a single gold docking job - Stage 2 of three-stage docking.
    
    Parameters
    ----------
    job_data : dict
        Dictionary containing all job parameters
        
    Returns
    -------
    dict
        Results of the gold docking job
    """
    # Extract job parameters
    job_idx = job_data['job_idx']
    drugbank_id = job_data['drugbank_id']
    uniprot_id = job_data['uniprot_id']
    gene_name = job_data['gene_name']
    ligand_sdf = job_data['ligand_sdf']
    receptor_pdb = job_data['receptor_pdb']
    receptor_pdbqt = job_data['receptor_pdbqt']
    pocket_pdb = job_data['pocket_pdb']
    current_outfolder = job_data['current_outfolder']
    
    # Get configuration from job_data
    CONSENSUS_DOCKER_SCRIPT = job_data['consensus_docker_script']
    GOLD_PATH = job_data['gold_path']
    NUM_THREADS = job_data['num_threads']
    CUTOFF_VALUE = job_data['cutoff_value']
    EXHAUSTIVENESS = job_data.get('exhaustiveness', 8)  # Default to 8 if not set
    
    process_id = os.getpid()
    
    try:
        # Check if gold output already exists
        if check_tool_completion(current_outfolder, 'gold'):
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'skipped',
                'message': 'Gold job already completed with valid output',
                'process_id': process_id
            }
        
        # Build the gold-only command
        command = [
            "python", CONSENSUS_DOCKER_SCRIPT,
            "--use_gold",
            "--overwrite",  # Allow writing to existing directory
            "--outfolder", current_outfolder,
            "--gold_path", GOLD_PATH,
            "--ligand_sdf", ligand_sdf,
            "--receptor_pdb", receptor_pdb,
            "--pocket_pdb", pocket_pdb,
            "--num_threads", str(NUM_THREADS),
            "--cutoff_value", str(CUTOFF_VALUE),
            "--exhaustiveness", str(EXHAUSTIVENESS)
        ]
        
        # Add PDBQT file if available for faster processing
        if receptor_pdbqt and os.path.exists(receptor_pdbqt):
            command.extend(["--receptor_pdbqt", receptor_pdbqt])
        
        # Run the command with timeout
        timeout_seconds = job_data.get('timeout', 600)  # Default 10 minutes
        try:
            result = subprocess.run(command, capture_output=True, text=True, 
                                  check=False, timeout=timeout_seconds)
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'timeout',
                'message': f'Gold docking timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)',
                'process_id': process_id
            }
        
        if result.returncode == 0:
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'completed',
                'message': f'Successfully completed gold docking for {drugbank_id} into {pocket_pdb}',
                'process_id': process_id
            }
        else:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'failed',
                'message': f'Gold docking failed. Return code: {result.returncode}',
                'stdout': result.stdout,
                'stderr': result.stderr,
                'process_id': process_id
            }
    
    except Exception as e:
        return {
            'success': False,
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            'action': 'error',
            'message': f'Exception occurred in gold docking: {str(e)}',
            'process_id': process_id
        }

def run_single_ledock_job(job_data):
    """
    Run a single ledock docking job - Stage 3 of three-stage docking.
    
    Parameters
    ----------
    job_data : dict
        Dictionary containing all job parameters
        
    Returns
    -------
    dict
        Results of the ledock docking job
    """
    # Extract job parameters
    job_idx = job_data['job_idx']
    drugbank_id = job_data['drugbank_id']
    uniprot_id = job_data['uniprot_id']
    gene_name = job_data['gene_name']
    ligand_sdf = job_data['ligand_sdf']
    receptor_pdb = job_data['receptor_pdb']
    receptor_pdbqt = job_data['receptor_pdbqt']
    pocket_pdb = job_data['pocket_pdb']
    current_outfolder = job_data['current_outfolder']
    
    # Get configuration from job_data
    CONSENSUS_DOCKER_SCRIPT = job_data['consensus_docker_script']
    LEDOCK_PATH = job_data['ledock_path']
    LEPRO_PATH = job_data['lepro_path']
    NUM_THREADS = job_data['num_threads']
    CUTOFF_VALUE = job_data['cutoff_value']
    EXHAUSTIVENESS = job_data.get('exhaustiveness', 8)  # Default to 8 if not set
    
    process_id = os.getpid()
    
    try:
        # Check if ledock output already exists
        if check_tool_completion(current_outfolder, 'ledock'):
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'skipped',
                'message': 'LeDock job already completed with valid output',
                'process_id': process_id
            }
        
        # Build the ledock-only command
        command = [
            "python", CONSENSUS_DOCKER_SCRIPT,
            "--use_ledock",
            "--overwrite",  # Allow writing to existing directory
            "--outfolder", current_outfolder,
            "--ledock_path", LEDOCK_PATH,
            "--lepro_path", LEPRO_PATH,
            "--ligand_sdf", ligand_sdf,
            "--receptor_pdb", receptor_pdb,
            "--pocket_pdb", pocket_pdb,
            "--num_threads", str(NUM_THREADS),
            "--cutoff_value", str(CUTOFF_VALUE),
            "--exhaustiveness", str(EXHAUSTIVENESS)
        ]
        
        # Add PDBQT file if available for faster processing
        if receptor_pdbqt and os.path.exists(receptor_pdbqt):
            command.extend(["--receptor_pdbqt", receptor_pdbqt])
        
        # Run the command with timeout
        timeout_seconds = job_data.get('timeout', 600)  # Default 10 minutes
        try:
            result = subprocess.run(command, capture_output=True, text=True, 
                                  check=False, timeout=timeout_seconds)
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'timeout',
                'message': f'LeDock docking timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)',
                'process_id': process_id
            }
        
        if result.returncode == 0:
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'completed',
                'message': f'Successfully completed ledock docking for {drugbank_id} into {pocket_pdb}',
                'process_id': process_id
            }
        else:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'failed',
                'message': f'LeDock docking failed. Return code: {result.returncode}',
                'stdout': result.stdout,
                'stderr': result.stderr,
                'process_id': process_id
            }
    
    except Exception as e:
        return {
            'success': False,
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            'action': 'error',
            'message': f'Exception occurred in ledock docking: {str(e)}',
            'process_id': process_id
        }

def run_single_rmsd_job(job_data):
    """
    Run RMSD calculation for a job where all tools completed but final results are missing.
    
    Parameters
    ----------
    job_data : dict
        Dictionary containing all job parameters
        
    Returns
    -------
    dict
        Results of the RMSD calculation
    """
    # Extract job parameters
    job_idx = job_data['job_idx']
    drugbank_id = job_data['drugbank_id']
    uniprot_id = job_data['uniprot_id']
    gene_name = job_data['gene_name']
    ligand_sdf = job_data['ligand_sdf']
    receptor_pdb = job_data['receptor_pdb']
    receptor_pdbqt = job_data['receptor_pdbqt']
    pocket_pdb = job_data['pocket_pdb']
    current_outfolder = job_data['current_outfolder']
    
    # Get configuration from job_data
    CONSENSUS_DOCKER_SCRIPT = job_data['consensus_docker_script']
    NUM_THREADS = job_data['num_threads']
    CUTOFF_VALUE = job_data['cutoff_value']
    
    process_id = os.getpid()
    
    try:
        # Check if final results already exist
        if check_final_results_completion(current_outfolder):
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'skipped',
                'message': 'Final results already completed with valid RMSD data',
                'process_id': process_id
            }
        
        # Build the RMSD-only command (run consensus docker with skip flags to only calculate RMSD)
        command = [
            "python", CONSENSUS_DOCKER_SCRIPT,
            "--skip_docking",  # Skip actual docking, only calculate RMSD
            "--overwrite",  # Allow writing to existing directory
            "--outfolder", current_outfolder,
            "--ligand_sdf", ligand_sdf,
            "--receptor_pdb", receptor_pdb,
            "--pocket_pdb", pocket_pdb,
            "--num_threads", str(NUM_THREADS),
            "--cutoff_value", str(CUTOFF_VALUE)
        ]
        
        # Add PDBQT file if available
        if receptor_pdbqt and os.path.exists(receptor_pdbqt):
            command.extend(["--receptor_pdbqt", receptor_pdbqt])
        
        # Run the command with timeout
        timeout_seconds = job_data.get('timeout', 300)  # Default 5 minutes
        try:
            result = subprocess.run(command, capture_output=True, text=True, 
                                  check=False, timeout=timeout_seconds)
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'timeout',
                'message': f'RMSD calculation timed out after {timeout_seconds} seconds ({timeout_seconds/60:.1f} minutes)',
                'process_id': process_id
            }
        
        if result.returncode == 0:
            return {
                'success': True,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'completed',
                'message': f'Successfully completed RMSD calculation for {drugbank_id} into {pocket_pdb}',
                'process_id': process_id
            }
        else:
            return {
                'success': False,
                'job_idx': job_idx,
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'pocket_pdb': pocket_pdb,
                'current_outfolder': current_outfolder,
                'action': 'failed',
                'message': f'RMSD calculation failed. Return code: {result.returncode}',
                'stdout': result.stdout,
                'stderr': result.stderr,
                'process_id': process_id
            }
    
    except Exception as e:
        return {
            'success': False,
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            'action': 'error',
            'message': f'Exception occurred in RMSD calculation: {str(e)}',
            'process_id': process_id
        }

# --- Part 3: Main Docking Execution Logic ---

def run_stage_jobs(job_data_list, stage_name, job_function, max_workers, timeout, exhaustiveness=None):
    """
    Run a stage of docking jobs in parallel.
    
    Parameters
    ----------
    job_data_list : list
        List of job data dictionaries
    stage_name : str
        Name of the stage (for logging)
    job_function : callable
        Function to run each job
    max_workers : int
        Maximum number of parallel workers
    timeout : int
        Timeout for each job in seconds
    exhaustiveness : int, optional
        Exhaustiveness parameter for the docking tool
        
    Returns
    -------
    dict
        Statistics about the stage execution
    """
    logging.info(f"\n=== {stage_name.upper()} ===")
    logging.info(f"Starting {stage_name} with {max_workers} processes...")
    logging.info(f"Timeout per job: {timeout/60:.1f} minutes")
    if exhaustiveness is not None:
        logging.info(f"Exhaustiveness: {exhaustiveness}")
    
    stage_start_time = time.time()
    completed_jobs = 0
    failed_jobs = 0
    timeout_jobs = 0
    skipped_jobs = 0
    unique_processes = set()
    last_progress_log = time.time()
    progress_log_interval = 30  # Log progress every 30 seconds
    
    # Update job data with correct timeout and exhaustiveness
    stage_job_data_list = []
    for job_data in job_data_list:
        stage_job_data = job_data.copy()
        stage_job_data['timeout'] = timeout
        if exhaustiveness is not None:
            stage_job_data['exhaustiveness'] = exhaustiveness
            logging.debug(f"Setting exhaustiveness to {exhaustiveness} for job {stage_job_data.get('job_idx', 'unknown')}")
        else:
            logging.debug(f"No exhaustiveness set for job {stage_job_data.get('job_idx', 'unknown')}")
        stage_job_data_list.append(stage_job_data)
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Submit all jobs
        future_to_job = {executor.submit(job_function, job_data): job_data for job_data in stage_job_data_list}
        
        # Process results with progress bar
        completed_futures = 0
        for future in tqdm(as_completed(future_to_job), total=len(stage_job_data_list), 
                         desc=f"{stage_name}", unit="job"):
            try:
                result = future.result()
                completed_futures += 1
                
                # Track unique processes
                if 'process_id' in result:
                    unique_processes.add(result['process_id'])
                
                if result['success']:
                    if result['action'] == 'completed':
                        completed_jobs += 1
                        logging.info(f"{stage_name} Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                    elif result['action'] == 'skipped':
                        skipped_jobs += 1
                        logging.debug(f"{stage_name} Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                else:
                    if result['action'] == 'timeout':
                        timeout_jobs += 1
                        logging.warning(f"{stage_name} Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                    else:
                        failed_jobs += 1
                        logging.error(f"{stage_name} Job {result['job_idx']+1}: {result['message']} (Process: {result['process_id']})")
                        if 'stdout' in result and result['stdout']:
                            logging.error(f"STDOUT: {result['stdout']}")
                        if 'stderr' in result and result['stderr']:
                            logging.error(f"STDERR: {result['stderr']}")
                
                # Log progress periodically
                current_time = time.time()
                if current_time - last_progress_log >= progress_log_interval:
                    percent_complete = (completed_futures / len(stage_job_data_list)) * 100
                    elapsed_time = current_time - stage_start_time
                    avg_time_per_job = elapsed_time / completed_futures if completed_futures > 0 else 0
                    remaining_jobs = len(stage_job_data_list) - completed_futures
                    estimated_time_remaining = remaining_jobs * avg_time_per_job
                    
                    # Format elapsed time and ETA
                    def format_time(seconds):
                        if seconds < 60:
                            return f"{seconds:.0f}s"
                        elif seconds < 3600:
                            return f"{seconds/60:.1f}m"
                        else:
                            return f"{seconds/3600:.1f}h"
                    
                    elapsed_str = format_time(elapsed_time)
                    eta_str = format_time(estimated_time_remaining) if estimated_time_remaining > 0 else "N/A"
                    
                    logging.info(f"{stage_name.upper()} PROGRESS: {percent_complete:.1f}% complete "
                               f"({completed_futures}/{len(stage_job_data_list)} jobs processed) | "
                               f"✓{completed_jobs} completed, ✗{failed_jobs} failed, ⏱{timeout_jobs} timeout, ◊{skipped_jobs} skipped | "
                               f"Elapsed: {elapsed_str}, ETA: {eta_str}")
                    last_progress_log = current_time
                        
            except Exception as e:
                failed_jobs += 1
                completed_futures += 1
                job_data = future_to_job[future]
                logging.error(f"Exception in {stage_name} job {job_data['job_idx']+1}: {e}")
    
    # Stage summary
    stage_elapsed_time = time.time() - stage_start_time
    logging.info(f"\n=== {stage_name.upper()} COMPLETED ===")
    logging.info(f"{stage_name} jobs completed successfully: {completed_jobs}")
    logging.info(f"{stage_name} jobs skipped (already completed): {skipped_jobs}")
    logging.info(f"{stage_name} jobs failed: {failed_jobs}")
    logging.info(f"{stage_name} jobs timed out: {timeout_jobs}")
    logging.info(f"{stage_name} execution time: {stage_elapsed_time:.2f} seconds")
    logging.info(f"Used {len(unique_processes)} unique processes out of {max_workers} configured")
    
    return {
        'completed': completed_jobs,
        'skipped': skipped_jobs,
        'failed': failed_jobs,
        'timeout': timeout_jobs,
        'elapsed_time': stage_elapsed_time,
        'unique_processes': len(unique_processes)
    }

def run_docking(
    drug_to_protein_df, 
    uniprot_map_df, 
    pdb_info_dict, 
    processed_ligand_sdf_folder,
    output_base_folder="docking_results"
):
    """
    Iterates through drug-protein pairs, constructs commands, and runs three-stage docking.
    """
    logging.info("Starting three-stage docking execution.")
    total_docking_jobs = 0
    executed_docking_jobs = 0
    skipped_jobs = 0
    
    os.makedirs(output_base_folder, exist_ok=True)

    # Pre-build mappings for faster lookup
    uniprot_gene_map = dict(zip(uniprot_map_df['Mapped_Gene_Name'], uniprot_map_df['Entry']))
    
    # Debug: Show some sample mappings
    logging.info(f"UniProt-Gene mapping contains {len(uniprot_gene_map)} entries")
    sample_gene_names = list(uniprot_gene_map.keys())[:5]
    logging.info(f"Sample gene names in UniProt mapping: {sample_gene_names}")
    sample_uniprot_ids = [uniprot_gene_map[gene] for gene in sample_gene_names]
    logging.info(f"Corresponding UniProt IDs: {sample_uniprot_ids}")

    docking_jobs = [] # List to store job dictionaries

    logging.info("Preparing list of docking jobs...")
    for index, row in tqdm(drug_to_protein_df.iterrows(), total=len(drug_to_protein_df), desc="Preparing Jobs"):
        drugbank_id = row['node_1']
        gene_name = row['node_2_name']
        
        ligand_sdf_path = os.path.join(processed_ligand_sdf_folder, f"{drugbank_id}.sdf")
        if not os.path.exists(ligand_sdf_path):
            tqdm.write(f"Warning: Ligand SDF '{ligand_sdf_path}' not found. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue

        uniprot_id = uniprot_gene_map.get(gene_name)
        if not uniprot_id:
            tqdm.write(f"Warning: UniProt ID not found for gene '{gene_name}'. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue
        
        if uniprot_id not in pdb_info_dict:
            logging.debug(f"UniProt ID '{uniprot_id}' not found in cavity mapping. Gene: '{gene_name}', DrugBank: '{drugbank_id}'")
            tqdm.write(f"Warning: No PDB information found for UniProt ID '{uniprot_id}'. Skipping {drugbank_id}-{gene_name} pair.")
            skipped_jobs += 1
            continue

        # Each UniProt ID can have multiple pocket/receptor pairs
        for receptor_pdb, pocket_pdb, receptor_pdbqt in pdb_info_dict[uniprot_id]:
            # Create unique job identifier
            pocket_filename = Path(pocket_pdb).stem
            cavity_match = re.search(r'_cavity_(\d+)', pocket_filename)
            cavity_num = cavity_match.group(1) if cavity_match else "unknown"
            job_signature = f"{drugbank_id}_{uniprot_id}_{cavity_num}"
            
            total_docking_jobs += 1
            docking_jobs.append({
                'drugbank_id': drugbank_id,
                'uniprot_id': uniprot_id,
                'gene_name': gene_name,
                'ligand_sdf': ligand_sdf_path,
                'receptor_pdb': receptor_pdb,
                'receptor_pdbqt': receptor_pdbqt,  # May be None if not available
                'pocket_pdb': pocket_pdb,
                'job_signature': job_signature  # For duplicate detection
            })
    
    logging.info(f"Total potential docking jobs: {total_docking_jobs}")
    logging.info(f"Skipped {skipped_jobs} jobs due to missing files or mappings.")
    
    # Remove duplicate jobs
    docking_jobs = remove_duplicate_jobs(docking_jobs)
    
    # Save job preparation summary
    save_job_preparation_summary(total_docking_jobs, skipped_jobs, len(docking_jobs))
    
    # Preview jobs before execution
    if docking_jobs:
        preview_df = preview_docking_jobs(docking_jobs, "docking_jobs_preview.csv")
        
        # Ask user for confirmation before proceeding
        print(f"\n{'='*60}")
        print(f"DOCKING JOBS PREVIEW")
        print(f"{'='*60}")
        print(f"Total jobs to execute: {len(docking_jobs)}")
        print(f"Preview saved to: docking_jobs_preview.csv")
        print(f"{'='*60}")
        
        if not TEST_MODE and not SKIP_CONFIRMATION:
            response = input("Do you want to proceed with these docking jobs? (y/n): ").lower().strip()
            if response not in ['y', 'yes']:
                logging.info("User chose not to proceed. Exiting.")
                sys.exit(0)
        elif TEST_MODE:
            logging.info("TEST MODE: Proceeding automatically")
        elif SKIP_CONFIRMATION:
            logging.info("SKIP_CONFIRMATION enabled: Proceeding automatically with all jobs")
    
    # Limit jobs for testing if TEST_MODE is enabled
    if TEST_MODE and len(docking_jobs) > MAX_TEST_JOBS:
        logging.info(f"TEST MODE: Limiting to first {MAX_TEST_JOBS} jobs out of {len(docking_jobs)} available jobs.")
        docking_jobs = docking_jobs[:MAX_TEST_JOBS]
        # Update preview with limited jobs
        if docking_jobs:
            preview_df = preview_docking_jobs(docking_jobs, "docking_jobs_preview_test_mode.csv")

    # Prepare job data for three-stage multiprocessing
    logging.info(f"Preparing {len(docking_jobs)} jobs for three-stage parallel execution...")
    logging.info(f"Stage 1: Smina with {SMINA_MAX_PARALLEL_JOBS} processes, exhaustiveness {SMINA_EXHAUSTIVENESS}")
    logging.info(f"Stage 2: Gold with {GOLD_MAX_PARALLEL_JOBS} processes, exhaustiveness {GOLD_EXHAUSTIVENESS}")
    logging.info(f"Stage 3: LeDock with {LEDOCK_MAX_PARALLEL_JOBS} processes, exhaustiveness {LEDOCK_EXHAUSTIVENESS}")
    logging.info(f"Stage 4: RMSD with {RMSD_MAX_PARALLEL_JOBS} processes")
    
    job_data_list = []
    
    for job_idx, job in enumerate(docking_jobs):
        drugbank_id = job['drugbank_id']
        uniprot_id = job['uniprot_id']
        gene_name = job['gene_name']
        ligand_sdf = job['ligand_sdf']
        receptor_pdb = job['receptor_pdb']
        receptor_pdbqt = job['receptor_pdbqt']  # May be None
        pocket_pdb = job['pocket_pdb']

        # Determine output folder name for this specific job
        pocket_filename = Path(pocket_pdb).stem
        # Extract cavity number using regex
        cavity_match = re.search(r'_cavity_(\d+)', pocket_filename)
        cavity_num = cavity_match.group(1) if cavity_match else "unknown"
        
        # Create a cleaner suffix
        pocket_suffix = re.sub(r'AF-[A-Z0-9]+(?:-F1-model_v1_)?', '', pocket_filename)
        if len(pocket_suffix) > 50:  # Arbitrary length limit
            pocket_suffix = f"cavity_{cavity_num}"
        
        output_folder_name = f"{drugbank_id}_{gene_name}_{uniprot_id}_{pocket_suffix}"
        current_outfolder = os.path.join(output_base_folder, output_folder_name)
        
        # Prepare job data dictionary
        job_data = {
            'job_idx': job_idx,
            'drugbank_id': drugbank_id,
            'uniprot_id': uniprot_id,
            'gene_name': gene_name,
            'ligand_sdf': ligand_sdf,
            'receptor_pdb': receptor_pdb,
            'receptor_pdbqt': receptor_pdbqt,
            'pocket_pdb': pocket_pdb,
            'current_outfolder': current_outfolder,
            # Configuration parameters
            'consensus_docker_script': CONSENSUS_DOCKER_SCRIPT,
            'smina_path': SMINA_PATH,
            'ledock_path': LEDOCK_PATH,
            'lepro_path': LEPRO_PATH,
            'gold_path': GOLD_PATH,
            'num_threads': NUM_THREADS,
            'cutoff_value': CUTOFF_VALUE
        }
        job_data_list.append(job_data)
    
    # Start overall timer
    start_time = time.time()
    
    # === STAGE 1: RUN SMINA DOCKING ===
    stage1_stats = run_stage_jobs(
        job_data_list=job_data_list, 
        stage_name="Stage 1: Smina Docking", 
        job_function=run_single_smina_job,
        max_workers=SMINA_MAX_PARALLEL_JOBS,
        timeout=SMINA_TIMEOUT,
        exhaustiveness=SMINA_EXHAUSTIVENESS
    )
    
    # === STAGE 2: RUN GOLD DOCKING ===
    stage2_stats = run_stage_jobs(
        job_data_list=job_data_list, 
        stage_name="Stage 2: Gold Docking", 
        job_function=run_single_gold_job,
        max_workers=GOLD_MAX_PARALLEL_JOBS,
        timeout=GOLD_TIMEOUT,
        exhaustiveness=GOLD_EXHAUSTIVENESS
    )
    
    # === STAGE 3: RUN LEDOCK DOCKING ===
    stage3_stats = run_stage_jobs(
        job_data_list=job_data_list, 
        stage_name="Stage 3: LeDock Docking", 
        job_function=run_single_ledock_job,
        max_workers=LEDOCK_MAX_PARALLEL_JOBS,
        timeout=LEDOCK_TIMEOUT,
        exhaustiveness=LEDOCK_EXHAUSTIVENESS
    )
    
    # === STAGE 4: RUN RMSD CALCULATION FOR INCOMPLETE JOBS ===
    # Filter jobs that need RMSD calculation (all tools complete but no final results)
    rmsd_jobs = []
    for job_data in job_data_list:
        status = get_job_completion_status(job_data['current_outfolder'])
        if status['smina'] and status['gold'] and status['ledock'] and not status['final_results']:
            rmsd_jobs.append(job_data)
    
    stage4_stats = {'completed': 0, 'skipped': 0, 'failed': 0, 'timeout': 0, 'elapsed_time': 0, 'unique_processes': 0}
    if rmsd_jobs:
        logging.info(f"Found {len(rmsd_jobs)} jobs needing RMSD calculation")
        stage4_stats = run_stage_jobs(
            job_data_list=rmsd_jobs, 
            stage_name="Stage 4: RMSD Calculation", 
            job_function=run_single_rmsd_job,
            max_workers=RMSD_MAX_PARALLEL_JOBS,
            timeout=RMSD_TIMEOUT
        )
    else:
        logging.info("No jobs need RMSD calculation - all are either incomplete or already have final results")
    
    # Calculate final statistics
    total_elapsed_time = time.time() - start_time
    
    # Final summary
    logging.info(f"\n=== FINAL THREE-STAGE DOCKING SUMMARY ===")
    if TEST_MODE:
        logging.info(f"*** TEST MODE COMPLETED - RAN {min(len(docking_jobs), MAX_TEST_JOBS)} OUT OF {total_docking_jobs} POTENTIAL JOBS ***")
    
    logging.info(f"Total potential docking jobs (before checking existence): {total_docking_jobs}")
    logging.info(f"Total jobs attempted: {len(docking_jobs)}")
    logging.info(f"Total jobs skipped initially (missing ligand/uniprot/pdb info): {skipped_jobs}")
    
    logging.info(f"=== STAGE 1 (SMINA) RESULTS ===")
    logging.info(f"Smina jobs completed successfully: {stage1_stats['completed']}")
    logging.info(f"Smina jobs skipped (already completed): {stage1_stats['skipped']}")
    logging.info(f"Smina jobs failed: {stage1_stats['failed']}")
    logging.info(f"Smina jobs timed out (>{SMINA_TIMEOUT/60:.1f} minutes): {stage1_stats['timeout']}")
    
    logging.info(f"=== STAGE 2 (GOLD) RESULTS ===")
    logging.info(f"Gold jobs completed successfully: {stage2_stats['completed']}")
    logging.info(f"Gold jobs skipped (already completed): {stage2_stats['skipped']}")
    logging.info(f"Gold jobs failed: {stage2_stats['failed']}")
    logging.info(f"Gold jobs timed out (>{GOLD_TIMEOUT/60:.1f} minutes): {stage2_stats['timeout']}")
    
    logging.info(f"=== STAGE 3 (LEDOCK) RESULTS ===")
    logging.info(f"LeDock jobs completed successfully: {stage3_stats['completed']}")
    logging.info(f"LeDock jobs skipped (already completed): {stage3_stats['skipped']}")
    logging.info(f"LeDock jobs failed: {stage3_stats['failed']}")
    logging.info(f"LeDock jobs timed out (>{LEDOCK_TIMEOUT/60:.1f} minutes): {stage3_stats['timeout']}")
    
    logging.info(f"=== STAGE 4 (RMSD) RESULTS ===")
    logging.info(f"RMSD jobs completed successfully: {stage4_stats['completed']}")
    logging.info(f"RMSD jobs skipped (already completed): {stage4_stats['skipped']}")
    logging.info(f"RMSD jobs failed: {stage4_stats['failed']}")
    logging.info(f"RMSD jobs timed out (>{RMSD_TIMEOUT/60:.1f} minutes): {stage4_stats['timeout']}")
    
    logging.info(f"=== OVERALL TIMING ===")
    logging.info(f"Stage 1 execution time: {stage1_stats['elapsed_time']:.2f} seconds")
    logging.info(f"Stage 2 execution time: {stage2_stats['elapsed_time']:.2f} seconds") 
    logging.info(f"Stage 3 execution time: {stage3_stats['elapsed_time']:.2f} seconds")
    logging.info(f"Stage 4 execution time: {stage4_stats['elapsed_time']:.2f} seconds")
    logging.info(f"Total execution time: {total_elapsed_time:.2f} seconds")
    logging.info(f"Average time per job: {total_elapsed_time/len(docking_jobs):.2f} seconds")
    logging.info(f"Three-stage docking workflow finished.")

def preview_docking_jobs(docking_jobs, preview_file="docking_jobs_preview.csv"):
    """
    Create a preview of all docking jobs and save to CSV file.
    
    Parameters
    ----------
    docking_jobs : list
        List of docking job dictionaries
    preview_file : str
        Path to save the preview CSV file
        
    Returns
    -------
    pd.DataFrame
        DataFrame containing the preview information
    """
    if not docking_jobs:
        logging.warning("No docking jobs to preview")
        return pd.DataFrame()
    
    # Create preview data
    preview_data = []
    for i, job in enumerate(docking_jobs):
        # Extract cavity number from pocket filename
        pocket_filename = Path(job['pocket_pdb']).stem
        cavity_match = re.search(r'_cavity_(\d+)', pocket_filename)
        cavity_num = cavity_match.group(1) if cavity_match else "unknown"
        
        # Check if PDBQT file exists
        pdbqt_exists = job['receptor_pdbqt'] and os.path.exists(job['receptor_pdbqt'])
        
        preview_data.append({
            'Job_Index': i + 1,
            'DrugBank_ID': job['drugbank_id'],
            'Gene_Name': job['gene_name'],
            'UniProt_ID': job['uniprot_id'],
            'Cavity_Number': cavity_num,
            'Job_Signature': job['job_signature'],
            'Ligand_SDF': job['ligand_sdf'],
            'Receptor_PDB': job['receptor_pdb'],
            'Receptor_PDBQT': job['receptor_pdbqt'] if job['receptor_pdbqt'] else 'N/A',
            'PDBQT_Exists': pdbqt_exists,
            'Pocket_PDB': job['pocket_pdb'],
            'Ligand_Exists': os.path.exists(job['ligand_sdf']),
            'Receptor_Exists': os.path.exists(job['receptor_pdb']),
            'Pocket_Exists': os.path.exists(job['pocket_pdb'])
        })
    
    # Create DataFrame
    preview_df = pd.DataFrame(preview_data)
    
    # Save to CSV
    preview_df.to_csv(preview_file, index=False)
    
    # Print summary statistics
    total_jobs = len(preview_df)
    unique_ligands = preview_df['DrugBank_ID'].nunique()
    unique_proteins = preview_df['UniProt_ID'].nunique()
    unique_cavities = len(preview_df.groupby(['UniProt_ID', 'Cavity_Number']))
    pdbqt_available = preview_df['PDBQT_Exists'].sum()
    
    print(f"\n{'='*60}")
    print(f"DOCKING JOBS PREVIEW SUMMARY")
    print(f"{'='*60}")
    print(f"Total jobs to execute: {total_jobs}")
    print(f"Unique ligands (DrugBank IDs): {unique_ligands}")
    print(f"Unique proteins (UniProt IDs): {unique_proteins}")
    print(f"Unique protein-cavity combinations: {unique_cavities}")
    print(f"Jobs with pre-converted PDBQT files: {pdbqt_available}")
    print(f"Preview saved to: {preview_file}")
    print(f"{'='*60}")
    
    return preview_df

def remove_duplicate_jobs(docking_jobs):
    """
    Remove duplicate docking jobs based on job signature.
    
    Parameters
    ----------
    docking_jobs : list
        List of docking job dictionaries
        
    Returns
    -------
    list
        List of unique docking jobs
    """
    seen_signatures = set()
    unique_jobs = []
    duplicates_removed = 0
    
    for job in docking_jobs:
        signature = job['job_signature']
        if signature not in seen_signatures:
            seen_signatures.add(signature)
            unique_jobs.append(job)
        else:
            duplicates_removed += 1
            logging.debug(f"Removed duplicate job: {signature}")
    
    if duplicates_removed > 0:
        logging.info(f"Removed {duplicates_removed} duplicate jobs")
        logging.info(f"Unique jobs remaining: {len(unique_jobs)}")
    
    return unique_jobs

def save_job_preparation_summary(total_jobs, skipped_jobs, final_jobs):
    """
    Save a summary of job preparation statistics.
    
    Parameters
    ----------
    total_jobs : int
        Total number of potential jobs
    skipped_jobs : int
        Number of jobs skipped due to missing files
    final_jobs : int
        Number of jobs after deduplication
    """
    summary_data = {
        'Metric': [
            'Total potential jobs',
            'Jobs skipped (missing files)',
            'Jobs after deduplication',
            'Duplicates removed'
        ],
        'Count': [
            total_jobs,
            skipped_jobs,
            final_jobs,
            total_jobs - skipped_jobs - final_jobs
        ]
    }
    
    summary_df = pd.DataFrame(summary_data)
    summary_file = "job_preparation_summary.csv"
    summary_df.to_csv(summary_file, index=False)
    
    logging.info(f"Job preparation summary saved to: {summary_file}")

# --- Main execution block ---
if __name__ == "__main__":
    # --- Step 0: Validate initial configurations ---
    if not os.path.exists(CONSENSUS_DOCKER_SCRIPT):
        logging.critical(f"Error: Consensus Docker script not found at '{CONSENSUS_DOCKER_SCRIPT}'. Please correct the path.")
        sys.exit(1)
    if not os.path.exists(PROCESSED_LIGAND_SDF_FOLDER):
        logging.critical(f"Error: Processed ligand SDF folder not found at '{PROCESSED_LIGAND_SDF_FOLDER}'. Please run the RDKit 3D processing first.")
        sys.exit(1)
    if not os.path.exists(DRUG_TO_PROTEIN_TSV):
        logging.critical(f"Error: Drug-to-protein TSV file not found at '{DRUG_TO_PROTEIN_TSV}'. Please correct the path.")
        sys.exit(1)
    if not os.path.exists(UNIPROT_MAPPING_CSV):
        logging.critical(f"Error: UniProt mapping file not found at '{UNIPROT_MAPPING_CSV}'. Please run prepare_uniprot_mapping.py first.")
        sys.exit(1)
    if not os.path.exists(CAVITY_MAPPING_CSV):
        logging.critical(f"Error: Cavity mapping file not found at '{CAVITY_MAPPING_CSV}'. Please run extract_cavities.py first.")
        sys.exit(1)

    # --- Step 1: Load pre-generated UniProt mapping and filter for small molecules ---
    logging.info("Step 1: Loading pre-generated UniProt mapping and filtering for small molecules.")
    try:
        drug_protein_data = pd.read_csv(DRUG_TO_PROTEIN_TSV, sep='\t')
        logging.info(f"Loaded {len(drug_protein_data)} drug-protein interactions from '{DRUG_TO_PROTEIN_TSV}'.")
        
        # Load small molecule drugs and filter interactions
        small_molecule_ids = load_small_molecule_drugs()
        initial_interactions = len(drug_protein_data)
        drug_protein_data = drug_protein_data[
            drug_protein_data['node_1'].isin(small_molecule_ids)
        ]
        logging.info(f"Filtered to small molecules: {len(drug_protein_data)}/{initial_interactions} interactions")
        
        # Load the pre-generated UniProt mapping
        uniprot_df = load_uniprot_mapping(UNIPROT_MAPPING_CSV)
        
    except Exception as e:
        logging.critical(f"Error in Step 1 (Loading data): {e}")
        sys.exit(1)

    # --- Step 2: Load pre-generated cavity mapping ---
    logging.info("Step 2: Loading pre-generated cavity mapping.")
    try:
        # Load the best available mapping (AlphaFold preferred)
        pdb_data, mapping_source = load_best_mapping_with_pdbqt()
        logging.info(f"Loaded cavity mapping from: {mapping_source}")
        
        if not pdb_data:
            logging.warning("No PDB data loaded from mapping. No docking jobs will be performed.")
            sys.exit(0) # Exit gracefully if no PDBs were found
            
        # Debug: Check for overlap between UniProt mappings and cavity data
        uniprot_gene_ids = set(uniprot_df['Entry'].values)
        cavity_uniprot_ids = set(pdb_data.keys())
        overlap = uniprot_gene_ids.intersection(cavity_uniprot_ids)
        
        logging.info(f"UniProt IDs in gene mapping: {len(uniprot_gene_ids)}")
        logging.info(f"UniProt IDs in cavity mapping: {len(cavity_uniprot_ids)}")
        logging.info(f"Overlapping UniProt IDs: {len(overlap)}")
        
        if len(overlap) == 0:
            logging.warning("No overlapping UniProt IDs found between gene mapping and cavity mapping!")
            logging.warning("This suggests a data format mismatch. Check your input files.")
        
    except Exception as e:
        logging.critical(f"Error in Step 2 (Cavity mapping loading): {e}")
        sys.exit(1)

    # --- Step 3: Run Docking ---
    logging.info("Step 3: Running consensus docking jobs.")
    try:
        docking_output_base = "consensus_docking_results"
        run_docking(drug_protein_data, uniprot_df, pdb_data, PROCESSED_LIGAND_SDF_FOLDER, docking_output_base)
    except Exception as e:
        logging.critical(f"Error in Step 3 (Docking execution): {e}")
        sys.exit(1)