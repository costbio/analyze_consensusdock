{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794d62f7",
   "metadata": {},
   "source": [
    "# Consensus Docking Results Analysis\n",
    "\n",
    "This notebook analyzes large-scale consensus docking results to evaluate binding pose consistency and perform cluster-based selectivity analysis.\n",
    "\n",
    "## üöÄ Quick Start Guide\n",
    "\n",
    "**For most users:** Simply run cells 2-4 to load, filter, and start analysis immediately.\n",
    "\n",
    "**First-time users or data updates:** If you need to create/update the data files, run the optimized data preparation script:\n",
    "```bash\n",
    "python parse_prepare.py\n",
    "```\n",
    "This high-performance script uses multiprocessing to efficiently process millions of docking results in minutes instead of hours.\n",
    "\n",
    "## üìä Analysis Overview\n",
    "\n",
    "### Main Analysis Workflow\n",
    "1. **Data Loading** (Step 1) - Smart loading of existing data files\n",
    "2. **Cluster Integration** (Step 2) - Add cavity similarity information  \n",
    "3. **Tool Coverage Filtering** (Step 2.5) - **NEW:** Filter for complete tool coverage\n",
    "4. **Data Quality Check** (Step 3) - Dataset overview of filtered data\n",
    "5. **Tool Reliability Analysis** (Step 4) - Consensus analysis between tools\n",
    "6. **Cluster Analysis** - Binding site similarity and drug selectivity\n",
    "7. **Visualizations** - Comprehensive plots and insights\n",
    "\n",
    "### Key Outputs\n",
    "- **Fair tool comparisons** using only drug-target pairs with complete tool coverage\n",
    "- Pose consistency metrics across docking tools\n",
    "- Drug-target binding success rates\n",
    "- Cluster-based selectivity patterns\n",
    "- Tool agreement analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754698f7",
   "metadata": {},
   "source": [
    "## üì• Step 1: Smart Data Loading\n",
    "\n",
    "This cell automatically detects and loads the best available data source. Run this first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52fa8e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for existing data files...\n",
      "‚úÖ Found Parquet file: combined_consensus_docking_results.parquet\n",
      "üìñ Loading data (this is the fastest option)...\n",
      "   Shape: (28487256, 30)\n",
      "   Memory: 9700.8 MB\n",
      "‚úÖ Data loaded successfully!\n",
      "\n",
      "üìä Dataset Overview:\n",
      "   Total rows: 28,487,256\n",
      "   Columns: 30\n",
      "   Key columns: ['Tool1', 'Tool2', 'PoseNumber1', 'PoseNumber2', 'Score1', 'Score2', 'File1', 'File2', 'RMSD', 'source_file', 'source_dir', 'file_size_mb', 'source_type', 'drugbank_id', 'uniprot_id', 'gene_name', 'cavity_index', 'Pose', 'SMINA_Score', 'Score', 'S(PLP)', 'S(hbond)', 'S(cho)', 'S(metal)', 'DE(clash)', 'DE(tors)', 'time', 'LeDock_Score', 'primary_tool', 'compound_target_pair']\n",
      "‚úÖ All required columns present\n",
      "   Shape: (28487256, 30)\n",
      "   Memory: 9700.8 MB\n",
      "‚úÖ Data loaded successfully!\n",
      "\n",
      "üìä Dataset Overview:\n",
      "   Total rows: 28,487,256\n",
      "   Columns: 30\n",
      "   Key columns: ['Tool1', 'Tool2', 'PoseNumber1', 'PoseNumber2', 'Score1', 'Score2', 'File1', 'File2', 'RMSD', 'source_file', 'source_dir', 'file_size_mb', 'source_type', 'drugbank_id', 'uniprot_id', 'gene_name', 'cavity_index', 'Pose', 'SMINA_Score', 'Score', 'S(PLP)', 'S(hbond)', 'S(cho)', 'S(metal)', 'DE(clash)', 'DE(tors)', 'time', 'LeDock_Score', 'primary_tool', 'compound_target_pair']\n",
      "‚úÖ All required columns present\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üì• SMART DATA LOADING - START HERE\n",
    "# =============================================================================\n",
    "\n",
    "import os, re\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PARQUET_FILE = \"combined_consensus_docking_results.parquet\"\n",
    "CSV_FILE = \"combined_consensus_docking_results.csv\"\n",
    "BASE_FOLDER = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/\"\n",
    "\n",
    "print(\"üîç Checking for existing data files...\")\n",
    "\n",
    "# Smart data loading: try parquet first, then CSV, then create from scratch\n",
    "combined_results = None\n",
    "\n",
    "if os.path.exists(os.path.join(BASE_FOLDER, PARQUET_FILE)):\n",
    "    print(f\"‚úÖ Found Parquet file: {PARQUET_FILE}\")\n",
    "    print(\"üìñ Loading data (this is the fastest option)...\")\n",
    "    combined_results = pl.read_parquet(os.path.join(BASE_FOLDER, PARQUET_FILE))\n",
    "    print(f\"   Shape: {combined_results.shape}\")\n",
    "    print(f\"   Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "\n",
    "elif os.path.exists(os.path.join(BASE_FOLDER, CSV_FILE)):\n",
    "    print(f\"‚úÖ Found CSV file: {CSV_FILE}\")\n",
    "    print(\"üìñ Loading data (slower than Parquet but still good)...\")\n",
    "    combined_results = pl.read_csv(os.path.join(BASE_FOLDER, CSV_FILE))\n",
    "    print(f\"   Shape: {combined_results.shape}\")\n",
    "    print(f\"   Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No preprocessed data files found!\")\n",
    "    print(f\"   Looking for: {PARQUET_FILE} or {CSV_FILE}\")\n",
    "    print(\"\\nÔøΩ To create the data files, run the optimized preparation script:\")\n",
    "    print(\"   ```bash\")\n",
    "    print(\"   python parse_prepare.py\")\n",
    "    print(\"   ```\")\n",
    "    print(\"\\n‚ö° This high-performance script features:\")\n",
    "    print(\"   ‚Ä¢ Multiprocessing across all CPU cores\")\n",
    "    print(\"   ‚Ä¢ Progress bars for visual feedback\")\n",
    "    print(\"   ‚Ä¢ Processing rate: ~25,000 records/second\")\n",
    "    print(\"   ‚Ä¢ Creates both CSV and Parquet formats\")\n",
    "    print(\"   ‚Ä¢ Typical runtime: 5-10 minutes for millions of records\")\n",
    "    combined_results = pl.DataFrame()  # Empty dataframe\n",
    "\n",
    "# Quick validation\n",
    "if not combined_results.is_empty():\n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   Total rows: {combined_results.height:,}\")\n",
    "    print(f\"   Columns: {combined_results.width}\")\n",
    "    print(f\"   Key columns: {combined_results.columns}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['drugbank_id', 'uniprot_id', 'RMSD', 'Score1', 'Score2']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_results.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è  Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required columns present\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No data available for analysis\")\n",
    "    print(\"   Please run: python parse_prepare.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc223b1a",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 1.5: Clarify Score Column Names\n",
    "\n",
    "Add tool-specific score columns for easier downstream analysis. This creates new columns like `GOLD_Score`, `Smina_Score`, and `LeDock_Score` based on the Tool1/Tool2 and Score1/Score2 values, making it clearer which score belongs to which docking tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7b8a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è CLARIFYING SCORE COLUMN NAMES\n",
      "==================================================\n",
      "üìä Creating tool-specific score columns...\n",
      "   Detected tools: ['GOLD', 'LeDock', 'Smina']\n",
      "\n",
      "‚úÖ Created tool-specific score columns:\n",
      "   Detected tools: ['GOLD', 'LeDock', 'Smina']\n",
      "\n",
      "‚úÖ Created tool-specific score columns:\n",
      "   GOLD_Score: 17,050,379/28,487,256 non-null values (59.9%)\n",
      "   Smina_Score: 20,997,094/28,487,256 non-null values (73.7%)\n",
      "   LeDock_Score: 17,785,175/28,487,256 non-null values (62.4%)\n",
      "\n",
      "üìã Sample data with tool-specific scores:\n",
      "shape: (5, 7)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Tool1  ‚îÜ Score1 ‚îÜ Tool2 ‚îÜ Score2 ‚îÜ GOLD_Score ‚îÜ Smina_Score ‚îÜ LeDock_Score ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---   ‚îÜ ---    ‚îÜ ---        ‚îÜ ---         ‚îÜ ---          ‚îÇ\n",
      "‚îÇ str    ‚îÜ f64    ‚îÜ str   ‚îÜ f64    ‚îÜ f64        ‚îÜ f64         ‚îÜ f64          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 53.78  ‚îÜ 53.78      ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 69.2   ‚îÜ 69.2       ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 44.47  ‚îÜ 44.47      ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 60.9   ‚îÜ 60.9       ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 58.48  ‚îÜ 58.48      ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üéØ Score columns clarified successfully!\n",
      "   GOLD_Score: 17,050,379/28,487,256 non-null values (59.9%)\n",
      "   Smina_Score: 20,997,094/28,487,256 non-null values (73.7%)\n",
      "   LeDock_Score: 17,785,175/28,487,256 non-null values (62.4%)\n",
      "\n",
      "üìã Sample data with tool-specific scores:\n",
      "shape: (5, 7)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Tool1  ‚îÜ Score1 ‚îÜ Tool2 ‚îÜ Score2 ‚îÜ GOLD_Score ‚îÜ Smina_Score ‚îÜ LeDock_Score ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---   ‚îÜ ---    ‚îÜ ---        ‚îÜ ---         ‚îÜ ---          ‚îÇ\n",
      "‚îÇ str    ‚îÜ f64    ‚îÜ str   ‚îÜ f64    ‚îÜ f64        ‚îÜ f64         ‚îÜ f64          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 53.78  ‚îÜ 53.78      ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 69.2   ‚îÜ 69.2       ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 44.47  ‚îÜ 44.47      ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 60.9   ‚îÜ 60.9       ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îÇ LeDock ‚îÜ -9.74  ‚îÜ GOLD  ‚îÜ 58.48  ‚îÜ 58.48      ‚îÜ null        ‚îÜ -9.74        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üéØ Score columns clarified successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üè∑Ô∏è STEP 1.5: CLARIFY SCORE COLUMN NAMES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üè∑Ô∏è CLARIFYING SCORE COLUMN NAMES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    # Check if we have the required columns\n",
    "    if all(col in combined_results.columns for col in ['Tool1', 'Tool2', 'Score1', 'Score2']):\n",
    "        print(\"üìä Creating tool-specific score columns...\")\n",
    "        \n",
    "        # Get unique tools to see what we're working with\n",
    "        tools = set(combined_results['Tool1'].unique().to_list() + combined_results['Tool2'].unique().to_list())\n",
    "        tools = [t for t in tools if t is not None]\n",
    "        print(f\"   Detected tools: {sorted(tools)}\")\n",
    "        \n",
    "        # Create new columns for each tool's score\n",
    "        # We'll initialize them as null and then populate based on Tool1/Tool2 values\n",
    "        combined_results = combined_results.with_columns([\n",
    "            # For GOLD - check both Tool1 and Tool2\n",
    "            pl.when(pl.col('Tool1') == 'GOLD')\n",
    "              .then(pl.col('Score1'))\n",
    "              .when(pl.col('Tool2') == 'GOLD')\n",
    "              .then(pl.col('Score2'))\n",
    "              .otherwise(None)\n",
    "              .alias('GOLD_Score'),\n",
    "            \n",
    "            # For Smina - check both Tool1 and Tool2\n",
    "            pl.when(pl.col('Tool1') == 'Smina')\n",
    "              .then(pl.col('Score1'))\n",
    "              .when(pl.col('Tool2') == 'Smina')\n",
    "              .then(pl.col('Score2'))\n",
    "              .otherwise(None)\n",
    "              .alias('Smina_Score'),\n",
    "            \n",
    "            # For LeDock - check both Tool1 and Tool2\n",
    "            pl.when(pl.col('Tool1') == 'LeDock')\n",
    "              .then(pl.col('Score1'))\n",
    "              .when(pl.col('Tool2') == 'LeDock')\n",
    "              .then(pl.col('Score2'))\n",
    "              .otherwise(None)\n",
    "              .alias('LeDock_Score')\n",
    "        ])\n",
    "        \n",
    "        # Verify the new columns\n",
    "        print(f\"\\n‚úÖ Created tool-specific score columns:\")\n",
    "        for tool in ['GOLD', 'Smina', 'LeDock']:\n",
    "            col_name = f'{tool}_Score'\n",
    "            if col_name in combined_results.columns:\n",
    "                non_null_count = combined_results[col_name].drop_nulls().len()\n",
    "                total_count = combined_results.height\n",
    "                print(f\"   {col_name}: {non_null_count:,}/{total_count:,} non-null values ({non_null_count/total_count*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nüìã Sample data with tool-specific scores:\")\n",
    "        sample_cols = ['Tool1', 'Score1', 'Tool2', 'Score2', 'GOLD_Score', 'Smina_Score', 'LeDock_Score']\n",
    "        available_cols = [col for col in sample_cols if col in combined_results.columns]\n",
    "        sample = combined_results.select(available_cols).head(5)\n",
    "        print(sample)\n",
    "        \n",
    "        print(f\"\\nüéØ Score columns clarified successfully!\")\n",
    "        \n",
    "    else:\n",
    "        missing_cols = [col for col in ['Tool1', 'Tool2', 'Score1', 'Score2'] if col not in combined_results.columns]\n",
    "        print(f\"‚ö†Ô∏è  Missing required columns: {missing_cols}\")\n",
    "        print(\"   Skipping score column clarification...\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for score column clarification\")\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc9474",
   "metadata": {},
   "source": [
    "## üß¨ Step 2: Cluster Integration\n",
    "\n",
    "Add cavity cluster information for advanced analysis (run once per session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582dcac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÉÔ∏è CAVITY CLUSTER INTEGRATION\n",
      "==================================================\n",
      "üìñ Loading cavity cluster data...\n",
      "üìñ Loaded 12,943 clusters from CavitySpace\n",
      "ÔøΩ Extracting cavity identifiers from source paths...\n",
      "‚úÖ Extraction results:\n",
      "   Extracted uniprot_id: 6,735,070 non-null values\n",
      "   Extracted cavity_index: 6,935,086 non-null values\n",
      "   Sample extracted data:\n",
      "shape: (3, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ source_dir                      ‚îÜ extracted_uniprot_id ‚îÜ extracted_cavity_index ‚îÇ\n",
      "‚îÇ ---                             ‚îÜ ---                  ‚îÜ ---                    ‚îÇ\n",
      "‚îÇ str                             ‚îÜ str                  ‚îÜ i64                    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ /media/onur/Elements/cavity_sp‚Ä¶ ‚îÜ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ /media/onur/Elements/cavity_sp‚Ä¶ ‚îÜ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ /media/onur/Elements/cavity_sp‚Ä¶ ‚îÜ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üîÑ Processing cluster file to create mapping...\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'Q7RTN6_3GNI_B_C6'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'P32754_5EC3_A_C1'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'Q9H4L5_7CYZ_B_C7'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'P06732_1I0E_A_C1'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'P27338_1S3E_A_C4'\n",
      "üìä Cluster parsing results:\n",
      "   Successfully parsed: 34,868 cavity IDs\n",
      "   Failed to parse: 26,395 cavity IDs\n",
      "   Created mapping for 34,447 unique cavities\n",
      "   Sample mappings:\n",
      "     ('Q8TBR7', 4) -> cluster 1\n",
      "     ('Q9HAW9', 3) -> cluster 1\n",
      "     ('Q6UXK5', 2) -> cluster 1\n",
      "     ('P19835', 1) -> cluster 1\n",
      "     ('Q8TD57', 13) -> cluster 5\n",
      "‚úÖ Extraction results:\n",
      "   Extracted uniprot_id: 6,735,070 non-null values\n",
      "   Extracted cavity_index: 6,935,086 non-null values\n",
      "   Sample extracted data:\n",
      "shape: (3, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ source_dir                      ‚îÜ extracted_uniprot_id ‚îÜ extracted_cavity_index ‚îÇ\n",
      "‚îÇ ---                             ‚îÜ ---                  ‚îÜ ---                    ‚îÇ\n",
      "‚îÇ str                             ‚îÜ str                  ‚îÜ i64                    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ /media/onur/Elements/cavity_sp‚Ä¶ ‚îÜ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ /media/onur/Elements/cavity_sp‚Ä¶ ‚îÜ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ /media/onur/Elements/cavity_sp‚Ä¶ ‚îÜ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üîÑ Processing cluster file to create mapping...\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'Q7RTN6_3GNI_B_C6'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'P32754_5EC3_A_C1'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'Q9H4L5_7CYZ_B_C7'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'P06732_1I0E_A_C1'\n",
      "   ‚ö†Ô∏è Failed to parse cavity ID: 'P27338_1S3E_A_C4'\n",
      "üìä Cluster parsing results:\n",
      "   Successfully parsed: 34,868 cavity IDs\n",
      "   Failed to parse: 26,395 cavity IDs\n",
      "   Created mapping for 34,447 unique cavities\n",
      "   Sample mappings:\n",
      "     ('Q8TBR7', 4) -> cluster 1\n",
      "     ('Q9HAW9', 3) -> cluster 1\n",
      "     ('Q6UXK5', 2) -> cluster 1\n",
      "     ('P19835', 1) -> cluster 1\n",
      "     ('Q8TD57', 13) -> cluster 5\n",
      "\n",
      "üîç UniProt ID overlap analysis:\n",
      "   UniProts in our data: 872\n",
      "   UniProts in cluster file: 10,549\n",
      "   Overlap: 649\n",
      "   Sample from our data: ['A0PJX0', 'A2RU30', 'A5LHX3', 'A5X5Y0', 'A6NNM8']\n",
      "   Sample from clusters: ['A0A024RBG1', 'A0A075B6H5', 'A0A075B6H7', 'A0A075B6H8', 'A0A075B6H9']\n",
      "\n",
      "üîÑ Applying cluster mapping...\n",
      "\n",
      "üîç UniProt ID overlap analysis:\n",
      "   UniProts in our data: 872\n",
      "   UniProts in cluster file: 10,549\n",
      "   Overlap: 649\n",
      "   Sample from our data: ['A0PJX0', 'A2RU30', 'A5LHX3', 'A5X5Y0', 'A6NNM8']\n",
      "   Sample from clusters: ['A0A024RBG1', 'A0A075B6H5', 'A0A075B6H7', 'A0A075B6H8', 'A0A075B6H9']\n",
      "\n",
      "üîÑ Applying cluster mapping...\n",
      "‚úÖ Cluster mapping complete:\n",
      "   Mapped: 1,323,009/28,487,256 (4.6%)\n",
      "   Unique clusters: 116\n",
      "‚úÖ Cluster mapping complete:\n",
      "   Mapped: 1,323,009/28,487,256 (4.6%)\n",
      "   Unique clusters: 116\n",
      "   Sample mapped data:\n",
      "shape: (3, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ extracted_uniprot_id ‚îÜ extracted_cavity_index ‚îÜ cavity_cluster_id ‚îÇ\n",
      "‚îÇ ---                  ‚îÜ ---                    ‚îÜ ---               ‚îÇ\n",
      "‚îÇ str                  ‚îÜ i64                    ‚îÜ i64               ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ O94788               ‚îÜ 1                      ‚îÜ 2                 ‚îÇ\n",
      "‚îÇ O94788               ‚îÜ 1                      ‚îÜ 2                 ‚îÇ\n",
      "‚îÇ O94788               ‚îÜ 1                      ‚îÜ 2                 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üîç Debugging unmapped entries:\n",
      "   Sample mapped data:\n",
      "shape: (3, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ extracted_uniprot_id ‚îÜ extracted_cavity_index ‚îÜ cavity_cluster_id ‚îÇ\n",
      "‚îÇ ---                  ‚îÜ ---                    ‚îÜ ---               ‚îÇ\n",
      "‚îÇ str                  ‚îÜ i64                    ‚îÜ i64               ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ O94788               ‚îÜ 1                      ‚îÜ 2                 ‚îÇ\n",
      "‚îÇ O94788               ‚îÜ 1                      ‚îÜ 2                 ‚îÇ\n",
      "‚îÇ O94788               ‚îÜ 1                      ‚îÜ 2                 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üîç Debugging unmapped entries:\n",
      "   Sample unmapped entries:\n",
      "shape: (5, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ extracted_uniprot_id ‚îÜ extracted_cavity_index ‚îÇ\n",
      "‚îÇ ---                  ‚îÜ ---                    ‚îÇ\n",
      "‚îÇ str                  ‚îÜ i64                    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "üéØ Ready for analysis with shape: (28487256, 37)\n",
      "   Sample unmapped entries:\n",
      "shape: (5, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ extracted_uniprot_id ‚îÜ extracted_cavity_index ‚îÇ\n",
      "‚îÇ ---                  ‚îÜ ---                    ‚îÇ\n",
      "‚îÇ str                  ‚îÜ i64                    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îÇ null                 ‚îÜ null                   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "   ‚úì Key (None, None) correctly not in cluster mapping\n",
      "üéØ Ready for analysis with shape: (28487256, 37)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üóÉÔ∏è STEP 2: CAVITY CLUSTER INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üóÉÔ∏è CAVITY CLUSTER INTEGRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    # Check if we already have cluster information\n",
    "    if 'cavity_cluster_id' in combined_results.columns:\n",
    "        non_null_clusters = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "        if non_null_clusters > 0:\n",
    "            print(f\"‚úÖ Cluster data already present: {non_null_clusters:,} mapped entries\")\n",
    "            print(\"   Skipping cluster integration...\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Cluster column exists but empty - proceeding with integration...\")\n",
    "    \n",
    "    # Proceed with cluster integration if needed\n",
    "    if 'cavity_cluster_id' not in combined_results.columns or combined_results['cavity_cluster_id'].drop_nulls().len() == 0:\n",
    "        try:\n",
    "            print(f\"üìñ Loading cavity cluster data...\")\n",
    "            cluster_file = \"/opt/data/cavity_space/cavity_cluster_similarity07.csv\"\n",
    "            clusters_df = pl.read_csv(cluster_file, separator='\\t')\n",
    "            print(f\"üìñ Loaded {clusters_df.height:,} clusters from CavitySpace\")\n",
    "            \n",
    "            # Extract uniprot_id and cavity_index from source_dir if not already present\n",
    "            print(f\"ÔøΩ Extracting cavity identifiers from source paths...\")\n",
    "            \n",
    "            combined_results = combined_results.with_columns([\n",
    "                # Extract drugbank_id (1st component before first underscore)\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'(DB\\d+)_', group_index=1)\n",
    "                .alias('extracted_drugbank_id'),\n",
    "                \n",
    "                # Extract gene_name (2nd component - can be 'nan' for negative samples)\n",
    "                # Pattern: DB00035_AVPR1B_P47901_cavity_1 (positive) or DB00035_nan_P08173_cavity_3 (negative)\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'DB\\d+_([A-Z0-9]+|nan)_', group_index=1)\n",
    "                .alias('extracted_gene_name'),\n",
    "                \n",
    "                # Extract uniprot_id (component before 'cavity_')\n",
    "                # More flexible pattern to handle both positive and negative samples\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'_([A-Z0-9]+)_cavity_\\d+', group_index=1)\n",
    "                .alias('extracted_uniprot_id'),\n",
    "                \n",
    "                # Extract cavity_index (number after 'cavity_')\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'cavity_(\\d+)', group_index=1)\n",
    "                .cast(pl.Int64, strict=False)\n",
    "                .alias('extracted_cavity_index')\n",
    "            ])\n",
    "            \n",
    "            # Check extraction results\n",
    "            non_null_uniprot = combined_results['extracted_uniprot_id'].drop_nulls().len()\n",
    "            non_null_cavity = combined_results['extracted_cavity_index'].drop_nulls().len()\n",
    "            \n",
    "            print(f\"‚úÖ Extraction results:\")\n",
    "            print(f\"   Extracted uniprot_id: {non_null_uniprot:,} non-null values\")\n",
    "            print(f\"   Extracted cavity_index: {non_null_cavity:,} non-null values\")\n",
    "            \n",
    "            # Show sample extracted data for debugging\n",
    "            sample_data = combined_results.select(['source_dir', 'extracted_uniprot_id', 'extracted_cavity_index']).head(3)\n",
    "            print(f\"   Sample extracted data:\")\n",
    "            print(sample_data)\n",
    "            \n",
    "            if non_null_uniprot > 0 and non_null_cavity > 0:\n",
    "                # Create mapping dictionary from the cluster file\n",
    "                cavity_to_cluster = {}\n",
    "                successful_parses = 0\n",
    "                failed_parses = 0\n",
    "                \n",
    "                print(f\"üîÑ Processing cluster file to create mapping...\")\n",
    "                \n",
    "                for i, row in enumerate(clusters_df.to_dicts()):\n",
    "                    cluster_id = row['id']  # The cluster ID\n",
    "                    cavity_items = row['items']  # Comma-separated cavity IDs\n",
    "                    \n",
    "                    # Split the cavity items and process each one\n",
    "                    if cavity_items and isinstance(cavity_items, str):\n",
    "                        cavity_ids = cavity_items.split(',')\n",
    "                        \n",
    "                        for cavity_id in cavity_ids:\n",
    "                            cavity_id = cavity_id.strip()\n",
    "                            \n",
    "                            # Parse cavity format: AF-{UniProtID}-F{Fragment}-model_v1_C{CavityIndex}\n",
    "                            match = re.match(r'AF-([A-Z0-9]+)-F\\d+-model_v1_C(\\d+)', cavity_id)\n",
    "                            if match:\n",
    "                                uniprot_id, cavity_index = match.groups()\n",
    "                                key = (uniprot_id, int(cavity_index))\n",
    "                                cavity_to_cluster[key] = cluster_id\n",
    "                                successful_parses += 1\n",
    "                            else:\n",
    "                                failed_parses += 1\n",
    "                                if failed_parses <= 5:  # Show first few failures\n",
    "                                    print(f\"   ‚ö†Ô∏è Failed to parse cavity ID: '{cavity_id}'\")\n",
    "                \n",
    "                print(f\"üìä Cluster parsing results:\")\n",
    "                print(f\"   Successfully parsed: {successful_parses:,} cavity IDs\")\n",
    "                print(f\"   Failed to parse: {failed_parses:,} cavity IDs\")\n",
    "                print(f\"   Created mapping for {len(cavity_to_cluster):,} unique cavities\")\n",
    "                \n",
    "                # Show sample mapping entries\n",
    "                sample_keys = list(cavity_to_cluster.keys())[:5]\n",
    "                print(f\"   Sample mappings:\")\n",
    "                for key in sample_keys:\n",
    "                    print(f\"     {key} -> cluster {cavity_to_cluster[key]}\")\n",
    "                \n",
    "                # Check what UniProt IDs we have in our data vs cluster file\n",
    "                our_uniprots = set(combined_results.filter(pl.col('extracted_uniprot_id').is_not_null())['extracted_uniprot_id'].unique().to_list())\n",
    "                cluster_uniprots = set(key[0] for key in cavity_to_cluster.keys())\n",
    "                \n",
    "                print(f\"\\nüîç UniProt ID overlap analysis:\")\n",
    "                print(f\"   UniProts in our data: {len(our_uniprots):,}\")\n",
    "                print(f\"   UniProts in cluster file: {len(cluster_uniprots):,}\")\n",
    "                print(f\"   Overlap: {len(our_uniprots & cluster_uniprots):,}\")\n",
    "                \n",
    "                # Show sample UniProts from each set\n",
    "                print(f\"   Sample from our data: {sorted(list(our_uniprots))[:5]}\")\n",
    "                print(f\"   Sample from clusters: {sorted(list(cluster_uniprots))[:5]}\")\n",
    "                \n",
    "                # Map clusters to our data\n",
    "                def map_cluster(uniprot_id, cavity_index):\n",
    "                    if cavity_index is None or uniprot_id is None:\n",
    "                        return None\n",
    "                    key = (uniprot_id, cavity_index)\n",
    "                    return cavity_to_cluster.get(key)\n",
    "                \n",
    "                print(f\"\\nüîÑ Applying cluster mapping...\")\n",
    "                \n",
    "                combined_results = combined_results.with_columns([\n",
    "                    pl.struct(['extracted_uniprot_id', 'extracted_cavity_index'])\n",
    "                    .map_elements(lambda x: map_cluster(x['extracted_uniprot_id'], x['extracted_cavity_index']), return_dtype=pl.Int64)\n",
    "                    .alias('cavity_cluster_id')\n",
    "                ])\n",
    "                \n",
    "                # Report mapping results\n",
    "                mapped_count = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "                total_count = len(combined_results)\n",
    "                unique_clusters = combined_results['cavity_cluster_id'].n_unique()\n",
    "                \n",
    "                print(f\"‚úÖ Cluster mapping complete:\")\n",
    "                print(f\"   Mapped: {mapped_count:,}/{total_count:,} ({mapped_count/total_count*100:.1f}%)\")\n",
    "                print(f\"   Unique clusters: {unique_clusters:,}\")\n",
    "                \n",
    "                if mapped_count > 0:\n",
    "                    # Show sample mapped data\n",
    "                    sample_mapped = combined_results.filter(pl.col('cavity_cluster_id').is_not_null()).select(['extracted_uniprot_id', 'extracted_cavity_index', 'cavity_cluster_id']).head(3)\n",
    "                    print(f\"   Sample mapped data:\")\n",
    "                    print(sample_mapped)\n",
    "                    \n",
    "                # Debug unmapped entries\n",
    "                if mapped_count < total_count:\n",
    "                    print(f\"\\nüîç Debugging unmapped entries:\")\n",
    "                    unmapped = combined_results.filter(pl.col('cavity_cluster_id').is_null())\n",
    "                    sample_unmapped = unmapped.select(['extracted_uniprot_id', 'extracted_cavity_index']).head(5)\n",
    "                    print(f\"   Sample unmapped entries:\")\n",
    "                    print(sample_unmapped)\n",
    "                    \n",
    "                    # Check if these should have mappings\n",
    "                    for row in sample_unmapped.to_dicts():\n",
    "                        uniprot_id = row['extracted_uniprot_id']\n",
    "                        cavity_index = row['extracted_cavity_index']\n",
    "                        key = (uniprot_id, cavity_index)\n",
    "                        if key in cavity_to_cluster:\n",
    "                            print(f\"   ‚ùó Key {key} should map to cluster {cavity_to_cluster[key]} but doesn't!\")\n",
    "                        else:\n",
    "                            print(f\"   ‚úì Key {key} correctly not in cluster mapping\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Extraction failed - adding empty cluster column...\")\n",
    "                combined_results = combined_results.with_columns([\n",
    "                    pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "                ])\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è  Cluster file not found: {cluster_file}\")\n",
    "            print(\"   Adding empty cluster column...\")\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading clusters: {e}\")\n",
    "            print(f\"   Error details: {type(e).__name__}: {str(e)}\")\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "            ])\n",
    "    \n",
    "    print(f\"üéØ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for cluster integration\")\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fb3b5",
   "metadata": {},
   "source": [
    "## üß™ Step 2.1: IC50 Data Integration\n",
    "\n",
    "Add experimental IC50/Ki measurements for drug-target interactions from the Therapeutic Target Database (TTD). This provides quantitative binding affinity data that can be used to:\n",
    "- Validate docking predictions against experimental measurements\n",
    "- Compare predicted binding scores with actual IC50 values\n",
    "- Enrich the dataset with pharmacological activity information\n",
    "\n",
    "The IC50 data was generated using the `create_ic50_mapping.py` script which integrates:\n",
    "- TTD target information mapped to UniProt IDs\n",
    "- TTD activity measurements mapped to DrugBank IDs via PubChem Compound IDs\n",
    "- IC50 and Ki values (all converted to nM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üß™ STEP 2.1: IC50 DATA INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üß™ IC50 DATA INTEGRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    # Check if we already have IC50 information\n",
    "    if 'ic50_value' in combined_results.columns:\n",
    "        non_null_ic50 = combined_results['ic50_value'].drop_nulls().len()\n",
    "        if non_null_ic50 > 0:\n",
    "            print(f\"‚úÖ IC50 data already present: {non_null_ic50:,} mapped entries\")\n",
    "            print(\"   Skipping IC50 integration...\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  IC50 column exists but empty - proceeding with integration...\")\n",
    "    else:\n",
    "        print(\"üìÇ Loading IC50 mapping data...\")\n",
    "        \n",
    "        # Define the path to IC50 mapping file\n",
    "        ic50_file = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/ic50_mapping.csv\"\n",
    "        \n",
    "        try:\n",
    "            # Load IC50 mapping data\n",
    "            ic50_df = pl.read_csv(ic50_file)\n",
    "            \n",
    "            print(f\"   Loaded IC50 data: {ic50_df.shape}\")\n",
    "            print(f\"   Columns: {ic50_df.columns}\")\n",
    "            print(f\"\\nüìä IC50 Data Summary:\")\n",
    "            print(f\"   Unique drugs: {ic50_df['drugbank_id'].n_unique():,}\")\n",
    "            print(f\"   Unique targets: {ic50_df['uniprot_id'].n_unique():,}\")\n",
    "            print(f\"   Total measurements: {len(ic50_df):,}\")\n",
    "            \n",
    "            # Display measurement type distribution\n",
    "            measurement_counts = ic50_df.group_by('measurement_type').len().sort('len', descending=True)\n",
    "            print(f\"\\n   Measurement types:\")\n",
    "            for row in measurement_counts.iter_rows(named=True):\n",
    "                print(f\"     {row['measurement_type']}: {row['len']:,}\")\n",
    "            \n",
    "            # Display operator distribution\n",
    "            operator_counts = ic50_df.group_by('operator').len().sort('len', descending=True)\n",
    "            print(f\"\\n   Operators:\")\n",
    "            for row in operator_counts.iter_rows(named=True):\n",
    "                print(f\"     {row['operator']}: {row['len']:,}\")\n",
    "            \n",
    "            # Check for duplicates in IC50 data\n",
    "            ic50_unique_pairs = ic50_df.select(['drugbank_id', 'uniprot_id']).unique().height\n",
    "            ic50_total = ic50_df.height\n",
    "            print(f\"\\n‚ö†Ô∏è  IC50 Data Duplication Check:\")\n",
    "            print(f\"   Total IC50 records: {ic50_total:,}\")\n",
    "            print(f\"   Unique drug-target pairs: {ic50_unique_pairs:,}\")\n",
    "            if ic50_total > ic50_unique_pairs:\n",
    "                print(f\"   ‚ö†Ô∏è  Found {ic50_total - ic50_unique_pairs:,} duplicate pairs (multiple measurements)\")\n",
    "                print(f\"   Strategy: Will aggregate to keep best (lowest) IC50 value per pair\")\n",
    "            \n",
    "            # Aggregate IC50 data to handle duplicates - keep the lowest IC50 value per drug-target pair\n",
    "            # This represents the best binding affinity measurement\n",
    "            ic50_aggregated = ic50_df.group_by(['drugbank_id', 'uniprot_id']).agg([\n",
    "                pl.col('ic50_value').min().alias('ic50_value'),  # Lowest (best) IC50\n",
    "                pl.col('ic50_unit').first().alias('ic50_unit'),\n",
    "                pl.col('measurement_type').first().alias('measurement_type'),\n",
    "                pl.col('operator').first().alias('operator'),\n",
    "                pl.col('pubchem_cid').first().alias('pubchem_cid'),\n",
    "                pl.col('activity').first().alias('activity'),\n",
    "                pl.len().alias('n_measurements')  # Track how many measurements were aggregated\n",
    "            ])\n",
    "            \n",
    "            print(f\"   After aggregation: {ic50_aggregated.height:,} unique drug-target pairs\")\n",
    "            \n",
    "            # Merge IC50 data with combined results\n",
    "            # Match on both drugbank_id and uniprot_id\n",
    "            print(f\"\\nüîó Merging IC50 data with docking results...\")\n",
    "            before_merge = combined_results.shape\n",
    "            \n",
    "            combined_results = combined_results.join(\n",
    "                ic50_aggregated,\n",
    "                left_on=['drugbank_id', 'uniprot_id'],\n",
    "                right_on=['drugbank_id', 'uniprot_id'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            after_merge = combined_results.shape\n",
    "            matched_ic50 = combined_results['ic50_value'].drop_nulls().len()\n",
    "            \n",
    "            print(f\"   Before merge: {before_merge}\")\n",
    "            print(f\"   After merge:  {after_merge}\")\n",
    "            print(f\"   ‚úÖ Matched {matched_ic50:,} docking results with IC50 data\")\n",
    "            print(f\"   Coverage: {matched_ic50/len(combined_results)*100:.2f}% of docking results\")\n",
    "            \n",
    "            # Show some examples of matched data\n",
    "            if matched_ic50 > 0:\n",
    "                print(f\"\\nüìã Sample IC50-matched entries:\")\n",
    "                # Use available columns - check which ones exist\n",
    "                display_cols = ['drugbank_id', 'uniprot_id', 'ic50_value', 'ic50_unit', 'measurement_type', 'operator', 'n_measurements']\n",
    "                \n",
    "                # Add tool columns if they exist\n",
    "                if 'Tool1' in combined_results.columns:\n",
    "                    display_cols.insert(2, 'Tool1')\n",
    "                if 'Tool2' in combined_results.columns:\n",
    "                    display_cols.insert(3, 'Tool2')\n",
    "                \n",
    "                # Add score columns if they exist\n",
    "                if 'Score1' in combined_results.columns:\n",
    "                    display_cols.insert(-4, 'Score1')\n",
    "                if 'Score2' in combined_results.columns:\n",
    "                    display_cols.insert(-4, 'Score2')\n",
    "                \n",
    "                sample = combined_results.filter(\n",
    "                    pl.col('ic50_value').is_not_null()\n",
    "                ).select(display_cols).head(5)\n",
    "                print(sample)\n",
    "                \n",
    "                # Show IC50 value statistics for matched entries\n",
    "                ic50_stats = combined_results.filter(\n",
    "                    pl.col('ic50_value').is_not_null()\n",
    "                ).select(pl.col('ic50_value')).describe()\n",
    "                print(f\"\\nüìà IC50 Value Statistics (nM):\")\n",
    "                print(ic50_stats)\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  No IC50 values matched with docking results\")\n",
    "                print(f\"   This may indicate that the docked drug-target pairs\")\n",
    "                print(f\"   don't have experimental measurements in the TTD database\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå IC50 file not found: {ic50_file}\")\n",
    "            print(\"   Skipping IC50 integration...\")\n",
    "            # Add empty columns to maintain schema consistency\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Float64).alias('ic50_value'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('ic50_unit'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('measurement_type'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('operator'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('pubchem_cid'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('activity'),\n",
    "                pl.lit(None, dtype=pl.Int64).alias('n_measurements')\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading IC50 data: {e}\")\n",
    "            print(\"   Skipping IC50 integration...\")\n",
    "            # Add empty columns to maintain schema consistency\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Float64).alias('ic50_value'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('ic50_unit'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('measurement_type'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('operator'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('activity'),\n",
    "                pl.lit(None, dtype=pl.Int64).alias('n_measurements')\n",
    "            ])\n",
    "    \n",
    "    print(f\"üéØ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for IC50 integration\")\n",
    "\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96afb0",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 2.5: Sample Type Annotation (Positive vs Negative)\n",
    "\n",
    "**Critical Context:** The docking results include both:\n",
    "- **Positive samples**: Known drug-target interactions from validated databases\n",
    "- **Negative samples**: Randomly generated drug-target pairs (controls) to test specificity\n",
    "\n",
    "This annotation is essential for proper evaluation:\n",
    "- It enables us to assess how well docking tools distinguish true interactions from random pairings\n",
    "- All subsequent analyses must account for sample type to avoid conflating signal with noise\n",
    "- Performance metrics (ROC, precision-recall) require this ground truth labeling\n",
    "\n",
    "We'll load sample type information from `required_structures_with_negatives.csv` and merge it into our dataset based on UniProt ID, DrugBank ID, and cavity index.\n",
    "\n",
    "**‚ö†Ô∏è Important:** This step must be run BEFORE Step 2.6 (filtering) to enable balanced sample filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdaef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üè∑Ô∏è SAMPLE TYPE ANNOTATION (POSITIVE VS NEGATIVE)\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"üè∑Ô∏è Starting sample type annotation...\")\n",
    "    \n",
    "    # Load the sample type metadata\n",
    "    sample_metadata_file = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/required_structures_with_negatives.csv\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üìñ Loading sample type metadata from:\\n   {sample_metadata_file}\")\n",
    "        \n",
    "        # Load the metadata file\n",
    "        sample_metadata = pl.read_csv(sample_metadata_file)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {sample_metadata.height:,} rows from metadata file\")\n",
    "        print(f\"   Columns: {sample_metadata.columns}\")\n",
    "        \n",
    "        # Show sample of metadata\n",
    "        print(f\"\\nüìã Sample metadata (positive samples):\")\n",
    "        print(sample_metadata.filter(pl.col('sample_type') == 'positive').head(3))\n",
    "        print(f\"\\nüìã Sample metadata (negative samples):\")\n",
    "        print(sample_metadata.filter(pl.col('sample_type').str.contains('negative')).head(3))\n",
    "        \n",
    "        # Check sample type distribution in metadata\n",
    "        sample_type_counts = sample_metadata.group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "        print(f\"\\nüìä Sample type distribution in metadata:\")\n",
    "        print(sample_type_counts)\n",
    "        \n",
    "        # Prepare metadata for merging - select relevant columns\n",
    "        # Note: The metadata uses 'UniProt_ID' and 'Cavity_Index', while combined_results uses 'uniprot_id' and 'extracted_cavity_index'\n",
    "        merge_metadata = sample_metadata.select([\n",
    "            pl.col('UniProt_ID').alias('uniprot_id'),\n",
    "            'drugbank_id',\n",
    "            pl.col('Cavity_Index').alias('cavity_index'),\n",
    "            'sample_type',\n",
    "            'Gene_Name'  # Keep this for additional context\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nüîÑ Merging sample type information...\")\n",
    "        print(f\"   Merge keys: uniprot_id, drugbank_id, cavity_index\")\n",
    "        \n",
    "        # Check if we have the required columns in combined_results\n",
    "        if 'extracted_cavity_index' in combined_results.columns:\n",
    "            # Use extracted_cavity_index for merging\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.col('extracted_cavity_index').alias('cavity_index')\n",
    "            ])\n",
    "        elif 'cavity_index' not in combined_results.columns:\n",
    "            print(\"‚ö†Ô∏è  Warning: No cavity_index column found in combined_results!\")\n",
    "            print(\"   This may affect merge accuracy.\")\n",
    "        \n",
    "        # Before merge - check data availability\n",
    "        pre_merge_rows = combined_results.height\n",
    "        unique_pairs_in_data = combined_results.select(['uniprot_id', 'drugbank_id', 'cavity_index']).unique().height\n",
    "        unique_pairs_in_metadata = merge_metadata.select(['uniprot_id', 'drugbank_id', 'cavity_index']).unique().height\n",
    "        \n",
    "        print(f\"\\nüìä Pre-merge statistics:\")\n",
    "        print(f\"   Combined_results rows: {pre_merge_rows:,}\")\n",
    "        print(f\"   Unique (uniprot, drug, cavity) in data: {unique_pairs_in_data:,}\")\n",
    "        print(f\"   Unique (uniprot, drug, cavity) in metadata: {unique_pairs_in_metadata:,}\")\n",
    "        \n",
    "        # Perform left join to add sample_type to combined_results\n",
    "        combined_results = combined_results.join(\n",
    "            merge_metadata,\n",
    "            on=['uniprot_id', 'drugbank_id', 'cavity_index'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Check merge results\n",
    "        post_merge_rows = combined_results.height\n",
    "        annotated_rows = combined_results.filter(pl.col('sample_type').is_not_null()).height\n",
    "        \n",
    "        print(f\"\\n‚úÖ Merge completed:\")\n",
    "        print(f\"   Rows after merge: {post_merge_rows:,}\")\n",
    "        print(f\"   Rows with sample_type: {annotated_rows:,} ({annotated_rows/post_merge_rows*100:.1f}%)\")\n",
    "        \n",
    "        if annotated_rows < post_merge_rows:\n",
    "            unannotated_rows = post_merge_rows - annotated_rows\n",
    "            print(f\"   ‚ö†Ô∏è  Unannotated rows: {unannotated_rows:,} ({unannotated_rows/post_merge_rows*100:.1f}%)\")\n",
    "            \n",
    "            # Show sample of unannotated data for debugging\n",
    "            print(f\"\\nüìã Sample unannotated data (first 3 rows):\")\n",
    "            sample_unmapped = combined_results.filter(pl.col('sample_type').is_null()).select([\n",
    "                'uniprot_id', 'drugbank_id', 'cavity_index', 'source_dir'\n",
    "            ]).head(3)\n",
    "            print(sample_unmapped)\n",
    "        \n",
    "        # Show sample type distribution in annotated data\n",
    "        if annotated_rows > 0:\n",
    "            annotated_type_counts = combined_results.filter(\n",
    "                pl.col('sample_type').is_not_null()\n",
    "            ).group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "            \n",
    "            print(f\"\\nüìä Sample type distribution in annotated docking results:\")\n",
    "            print(annotated_type_counts)\n",
    "            \n",
    "            # Show sample of annotated data\n",
    "            print(f\"\\nüìã Sample annotated data (positive):\")\n",
    "            sample_mapped = combined_results.filter(\n",
    "                pl.col('sample_type') == 'positive'\n",
    "            ).select(['uniprot_id', 'drugbank_id', 'cavity_index', 'sample_type', 'Gene_Name']).head(3)\n",
    "            print(sample_mapped)\n",
    "            \n",
    "            print(f\"\\nüìã Sample annotated data (negative):\")\n",
    "            sample_mapped = combined_results.filter(\n",
    "                pl.col('sample_type').str.contains('negative')\n",
    "            ).select(['uniprot_id', 'drugbank_id', 'cavity_index', 'sample_type', 'Gene_Name']).head(3)\n",
    "            print(sample_mapped)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Sample type annotation complete!\")\n",
    "        print(f\"   Dataset now includes 'sample_type' column for positive/negative discrimination\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Could not find sample metadata file:\")\n",
    "        print(f\"   {sample_metadata_file}\")\n",
    "        print(f\"   Sample type annotation skipped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during sample type annotation: {e}\")\n",
    "        print(f\"   Sample type annotation skipped.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data available for sample type annotation\")\n",
    "    print(\"   Please load data first (Step 1 & 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04beb6",
   "metadata": {},
   "source": [
    "## üîç Step 2.6: Filter for Complete Tool Coverage and Balanced Samples\n",
    "\n",
    "**Important Filtering Criteria:**\n",
    "\n",
    "1. **Complete Tool Coverage**: Only include drug-target pairs where **ALL THREE tools** (Gold, Smina, LeDock) made predictions. This eliminates bias from partial tool coverage.\n",
    "\n",
    "2. **Balanced Sample Types**: For each drug, ensure equal representation of positive and negative samples:\n",
    "   - Filter out drugs that have only positive OR only negative samples\n",
    "   - Keep only drugs with both sample types present\n",
    "   - Balance the counts to have equal numbers of positive and negative samples per drug\n",
    "\n",
    "**Prerequisites:** \n",
    "- Step 2.5 (Sample Type Annotation) must be completed first\n",
    "- The `sample_type` column must be present in the dataset\n",
    "\n",
    "These filtering steps ensure fair comparison between tools and valid positive vs negative sample discrimination analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üîç FILTER FOR COMPLETE TOOL COVERAGE AND BALANCED SAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"üîç Starting filtering process...\")\n",
    "    print(\"   Criteria: (1) Complete tool coverage, (2) Balanced positive/negative samples\")\n",
    "    \n",
    "    # STEP 1: Filter for final_results source_type only\n",
    "    print(\"\\nüìã Step 1: Filtering for final_results source_type...\")\n",
    "    original_rows = combined_results.height\n",
    "    \n",
    "    if 'source_type' in combined_results.columns:\n",
    "        # Check what source_type values we have\n",
    "        source_types = combined_results['source_type'].unique().to_list()\n",
    "        print(f\"   Available source_types: {source_types}\")\n",
    "        \n",
    "        # Filter for final_results only\n",
    "        combined_results = combined_results.filter(pl.col('source_type') == 'final_results')\n",
    "        filtered_rows = combined_results.height\n",
    "        \n",
    "        print(f\"   Original rows: {original_rows:,}\")\n",
    "        print(f\"   After final_results filter: {filtered_rows:,} ({filtered_rows/original_rows*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No source_type column found, skipping source_type filtering\")\n",
    "    \n",
    "    # STEP 2: Analyze tool coverage\n",
    "    print(\"\\nüìã Step 2: Analyzing tool coverage...\")\n",
    "    \n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        # Filter out null values from tool lists\n",
    "        tool1_list = combined_results.filter(pl.col('Tool1').is_not_null())['Tool1'].unique().to_list()\n",
    "        tool2_list = combined_results.filter(pl.col('Tool2').is_not_null())['Tool2'].unique().to_list()\n",
    "        \n",
    "        # Combine and sort, excluding any None values\n",
    "        all_tools = tool1_list + tool2_list\n",
    "        all_detected_tools = sorted([tool for tool in set(all_tools) if tool is not None])\n",
    "        \n",
    "        print(f\"   All detected tools: {all_detected_tools}\")\n",
    "        \n",
    "        # Define the three main tools we expect\n",
    "        expected_tools = ['GOLD', 'Smina', 'LeDock']\n",
    "        available_expected_tools = [tool for tool in expected_tools if tool in all_detected_tools]\n",
    "        \n",
    "        print(f\"   Expected tools found: {available_expected_tools}\")\n",
    "        \n",
    "        if len(available_expected_tools) >= 2:  # Need at least 2 tools for comparison\n",
    "            print(f\"\\nüìä Step 3: Checking tool coverage per drug-target pair...\")\n",
    "            \n",
    "            # Group by drug-target pairs and check tool coverage\n",
    "            drug_target_groups = combined_results.group_by(['drugbank_id', 'uniprot_id'])\n",
    "            \n",
    "            complete_coverage_pairs = []\n",
    "            coverage_summary = []\n",
    "            \n",
    "            for group_key, group_data in drug_target_groups:\n",
    "                drug = group_key[0]\n",
    "                target = group_key[1]\n",
    "                \n",
    "                # Get unique tools that made predictions for this drug-target pair\n",
    "                tools_t1 = group_data.filter(pl.col('Tool1').is_not_null())['Tool1'].unique().to_list()\n",
    "                tools_t2 = group_data.filter(pl.col('Tool2').is_not_null())['Tool2'].unique().to_list()\n",
    "                tools_in_group = set(tools_t1 + tools_t2)\n",
    "                tools_present = [tool for tool in available_expected_tools if tool in tools_in_group]\n",
    "                \n",
    "                coverage_summary.append({\n",
    "                    'drug': drug,\n",
    "                    'target': target,\n",
    "                    'tools_present': tools_present,\n",
    "                    'n_tools': len(tools_present),\n",
    "                    'complete_coverage': len(tools_present) == len(available_expected_tools),\n",
    "                    'original_rows': group_data.height\n",
    "                })\n",
    "                \n",
    "                # If all expected tools are present, keep this drug-target pair\n",
    "                if len(tools_present) == len(available_expected_tools):\n",
    "                    complete_coverage_pairs.append((drug, target))\n",
    "            \n",
    "            # STEP 3: Filter for complete coverage pairs\n",
    "            if complete_coverage_pairs:\n",
    "                print(f\"   Found {len(complete_coverage_pairs):,} pairs with complete tool coverage\")\n",
    "                \n",
    "                # Create filter for complete coverage pairs using Polars syntax\n",
    "                complete_filter = pl.lit(False)  # Start with False\n",
    "                \n",
    "                for drug, target in complete_coverage_pairs:\n",
    "                    pair_filter = (pl.col('drugbank_id') == drug) & (pl.col('uniprot_id') == target)\n",
    "                    complete_filter = complete_filter | pair_filter\n",
    "                \n",
    "                # Apply the filter\n",
    "                combined_results = combined_results.filter(complete_filter)\n",
    "                final_rows = combined_results.height\n",
    "                \n",
    "                # Report filtering results\n",
    "                original_pairs = len(coverage_summary)\n",
    "                complete_pairs = len(complete_coverage_pairs)\n",
    "                \n",
    "                print(f\"\\nüìà FILTERING RESULTS:\")\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"üìä Original drug-target pairs: {original_pairs:,}\")\n",
    "                print(f\"‚úÖ Complete coverage pairs: {complete_pairs:,} ({complete_pairs/original_pairs*100:.1f}%)\")\n",
    "                print(f\"üìã After source_type filter: {filtered_rows:,}\")\n",
    "                print(f\"üîÑ Final filtered data: {final_rows:,} ({final_rows/filtered_rows*100:.1f}%)\")\n",
    "                \n",
    "                # Tool coverage distribution\n",
    "                coverage_dist = {}\n",
    "                for item in coverage_summary:\n",
    "                    n_tools = item['n_tools']\n",
    "                    coverage_dist[n_tools] = coverage_dist.get(n_tools, 0) + 1\n",
    "                \n",
    "                print(f\"\\nüéØ TOOL COVERAGE DISTRIBUTION:\")\n",
    "                for n_tools in sorted(coverage_dist.keys(), reverse=True):\n",
    "                    count = coverage_dist[n_tools]\n",
    "                    pct = count / original_pairs * 100\n",
    "                    print(f\"   {n_tools} tools: {count:,} pairs ({pct:.1f}%)\")\n",
    "                \n",
    "                print(f\"\\n‚úÖ Dataset filtered for fair tool comparison\")\n",
    "                print(f\"   Only using drug-target pairs where ALL {len(available_expected_tools)} expected tools made predictions\")\n",
    "                \n",
    "                # STEP 4: Filter for balanced positive/negative samples per drug\n",
    "                print(f\"\\nüìã Step 4: Filtering for balanced positive/negative samples...\")\n",
    "                \n",
    "                if 'sample_type' in combined_results.columns:\n",
    "                    # Check if we have sample type information\n",
    "                    sample_type_counts = combined_results.filter(\n",
    "                        pl.col('sample_type').is_not_null()\n",
    "                    ).group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "                    \n",
    "                    print(f\"   Sample type distribution before balancing:\")\n",
    "                    for row in sample_type_counts.iter_rows(named=True):\n",
    "                        print(f\"      {row['sample_type']}: {row['len']:,}\")\n",
    "                    \n",
    "                    # Identify positive and negative sample types\n",
    "                    has_positive = combined_results.filter(\n",
    "                        pl.col('sample_type') == 'positive'\n",
    "                    ).height > 0\n",
    "                    \n",
    "                    has_negative = combined_results.filter(\n",
    "                        pl.col('sample_type').str.contains('negative')\n",
    "                    ).height > 0\n",
    "                    \n",
    "                    if has_positive and has_negative:\n",
    "                        print(f\"   ‚úÖ Both positive and negative samples found\")\n",
    "                        \n",
    "                        # Group by drug and count positive/negative samples\n",
    "                        # We'll count unique (drug, target, cavity) combinations per sample type\n",
    "                        drug_sample_counts = combined_results.filter(\n",
    "                            pl.col('sample_type').is_not_null()\n",
    "                        ).group_by(['drugbank_id', 'sample_type']).agg([\n",
    "                            pl.col('uniprot_id').n_unique().alias('n_targets'),\n",
    "                            pl.col('cavity_index').n_unique().alias('n_cavities'),\n",
    "                            pl.len().alias('n_records')\n",
    "                        ])\n",
    "                        \n",
    "                        # For each drug, check if it has both positive and negative samples\n",
    "                        drugs_with_both = {}\n",
    "                        \n",
    "                        for drug in combined_results['drugbank_id'].unique().to_list():\n",
    "                            drug_samples = drug_sample_counts.filter(\n",
    "                                pl.col('drugbank_id') == drug\n",
    "                            )\n",
    "                            \n",
    "                            pos_count = drug_samples.filter(\n",
    "                                pl.col('sample_type') == 'positive'\n",
    "                            )\n",
    "                            neg_count = drug_samples.filter(\n",
    "                                pl.col('sample_type').str.contains('negative')\n",
    "                            )\n",
    "                            \n",
    "                            has_pos = pos_count.height > 0\n",
    "                            has_neg = neg_count.height > 0\n",
    "                            \n",
    "                            if has_pos and has_neg:\n",
    "                                # Get the counts of unique (target, cavity) combinations\n",
    "                                n_pos = pos_count.select('n_records').sum().item() if has_pos else 0\n",
    "                                n_neg = neg_count.select('n_records').sum().item() if has_neg else 0\n",
    "                                \n",
    "                                drugs_with_both[drug] = {\n",
    "                                    'n_positive': n_pos,\n",
    "                                    'n_negative': n_neg,\n",
    "                                    'min_count': min(n_pos, n_neg)\n",
    "                                }\n",
    "                        \n",
    "                        print(f\"   Drugs with both sample types: {len(drugs_with_both):,}\")\n",
    "                        print(f\"   Drugs filtered out (only one sample type): {combined_results['drugbank_id'].n_unique() - len(drugs_with_both):,}\")\n",
    "                        \n",
    "                        if len(drugs_with_both) > 0:\n",
    "                            # Filter for drugs with both sample types\n",
    "                            valid_drugs = list(drugs_with_both.keys())\n",
    "                            combined_results = combined_results.filter(\n",
    "                                pl.col('drugbank_id').is_in(valid_drugs)\n",
    "                            )\n",
    "                            \n",
    "                            after_drug_filter = combined_results.height\n",
    "                            print(f\"   Rows after drug filtering: {after_drug_filter:,}\")\n",
    "                            \n",
    "                            # Now balance the samples for each drug\n",
    "                            print(f\"\\n   Balancing positive/negative samples for each drug...\")\n",
    "                            \n",
    "                            balanced_chunks = []\n",
    "                            total_pos_kept = 0\n",
    "                            total_neg_kept = 0\n",
    "                            \n",
    "                            for drug, counts in drugs_with_both.items():\n",
    "                                min_count = counts['min_count']\n",
    "                                \n",
    "                                # Get positive samples for this drug\n",
    "                                pos_samples = combined_results.filter(\n",
    "                                    (pl.col('drugbank_id') == drug) &\n",
    "                                    (pl.col('sample_type') == 'positive')\n",
    "                                )\n",
    "                                \n",
    "                                # Get negative samples for this drug\n",
    "                                neg_samples = combined_results.filter(\n",
    "                                    (pl.col('drugbank_id') == drug) &\n",
    "                                    (pl.col('sample_type').str.contains('negative'))\n",
    "                                )\n",
    "                                \n",
    "                                # Take equal number from each (limited by minimum)\n",
    "                                if pos_samples.height > min_count:\n",
    "                                    pos_samples = pos_samples.sample(n=min_count, seed=42)\n",
    "                                if neg_samples.height > min_count:\n",
    "                                    neg_samples = neg_samples.sample(n=min_count, seed=42)\n",
    "                                \n",
    "                                balanced_chunks.append(pos_samples)\n",
    "                                balanced_chunks.append(neg_samples)\n",
    "                                \n",
    "                                total_pos_kept += pos_samples.height\n",
    "                                total_neg_kept += neg_samples.height\n",
    "                            \n",
    "                            # Combine balanced chunks\n",
    "                            if balanced_chunks:\n",
    "                                combined_results = pl.concat(balanced_chunks, how='vertical')\n",
    "                                balanced_rows = combined_results.height\n",
    "                                \n",
    "                                print(f\"   ‚úÖ Balanced sampling complete:\")\n",
    "                                print(f\"      Positive samples kept: {total_pos_kept:,}\")\n",
    "                                print(f\"      Negative samples kept: {total_neg_kept:,}\")\n",
    "                                print(f\"      Total rows: {balanced_rows:,}\")\n",
    "                                print(f\"      Balance ratio: {total_pos_kept/total_neg_kept:.3f}\" if total_neg_kept > 0 else \"      Balance ratio: N/A\")\n",
    "                                \n",
    "                                # Verify final balance\n",
    "                                final_sample_counts = combined_results.group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "                                print(f\"\\n   üìä Final sample type distribution:\")\n",
    "                                for row in final_sample_counts.iter_rows(named=True):\n",
    "                                    print(f\"      {row['sample_type']}: {row['len']:,}\")\n",
    "                            else:\n",
    "                                print(f\"   ‚ö†Ô∏è No balanced data could be created\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ö†Ô∏è No drugs have both positive and negative samples\")\n",
    "                            print(f\"      Skipping balanced sampling\")\n",
    "                    elif has_positive:\n",
    "                        print(f\"   ‚ö†Ô∏è Only positive samples found - cannot balance\")\n",
    "                    elif has_negative:\n",
    "                        print(f\"   ‚ö†Ô∏è Only negative samples found - cannot balance\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è No valid sample type information found\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è No sample_type column found - skipping balanced sampling\")\n",
    "                    print(f\"      Run sample type annotation (Step 2.6) first for balanced sampling\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå No drug-target pairs have complete tool coverage!\")\n",
    "                print(\"   Cannot proceed with fair comparison analysis\")\n",
    "        else:\n",
    "            print(f\"‚ùå Not enough tools found ({len(available_expected_tools)} < 2)\")\n",
    "    else:\n",
    "        print(\"‚ùå Tool1 or Tool2 columns not found\")\n",
    "        \n",
    "    print(f\"\\nüéØ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for filtering\")\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf873fd4",
   "metadata": {},
   "source": [
    "## üìä Step 3: Data Overview & Quality Check\n",
    "\n",
    "Get familiar with the **filtered** dataset structure and check data quality. This step now analyzes the data after filtering for complete tool coverage, ensuring all statistics reflect the dataset used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc109b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä STEP 3: DATA OVERVIEW & QUALITY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"üîç DATASET OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Shape: {combined_results.shape} (rows √ó columns)\")\n",
    "    print(f\"üíæ Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(f\"üìã Columns: {combined_results.width}\")\n",
    "    \n",
    "    print(f\"\\nüìö Column Names:\")\n",
    "    for i, col in enumerate(combined_results.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüß¨ KEY DATASET STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Core identifiers\n",
    "    print(f\"üî¨ Unique Drug-Target Combinations: {combined_results.select(['drugbank_id', 'uniprot_id']).unique().height:,}\")\n",
    "    print(f\"üíä Unique Drugs (DrugBank IDs): {combined_results['drugbank_id'].n_unique():,}\")\n",
    "    print(f\"üß¨ Unique Proteins (UniProt IDs): {combined_results['uniprot_id'].n_unique():,}\")\n",
    "    \n",
    "    # Check for cavity index information\n",
    "    if 'extracted_cavity_index' in combined_results.columns:\n",
    "        print(f\"üï≥Ô∏è  Unique Cavities: {combined_results['extracted_cavity_index'].n_unique():,}\")\n",
    "    elif 'cavity_index' in combined_results.columns:\n",
    "        print(f\"üï≥Ô∏è  Unique Cavities: {combined_results['cavity_index'].n_unique():,}\")\n",
    "    else:\n",
    "        print(\"üï≥Ô∏è  Cavity information: Not available\")\n",
    "    \n",
    "    # Cluster information\n",
    "    if 'cavity_cluster_id' in combined_results.columns:\n",
    "        cluster_mapped = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "        cluster_total = combined_results.height\n",
    "        unique_clusters = combined_results['cavity_cluster_id'].n_unique()\n",
    "        print(f\"üß© Cavity Clusters: {unique_clusters:,} unique clusters\")\n",
    "        print(f\"   Mapped: {cluster_mapped:,}/{cluster_total:,} ({cluster_mapped/cluster_total*100:.1f}%)\")\n",
    "    \n",
    "    # Check for essential analysis columns\n",
    "    print(f\"\\nüîç DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for RMSD columns\n",
    "    rmsd_columns = [col for col in combined_results.columns if 'rmsd' in col.lower()]\n",
    "    if rmsd_columns:\n",
    "        rmsd_col = rmsd_columns[0]\n",
    "        print(f\"‚úÖ RMSD data available: {rmsd_col}\")\n",
    "        \n",
    "        # RMSD statistics\n",
    "        rmsd_stats = combined_results.select([\n",
    "            pl.col(rmsd_col).min().alias('min_rmsd'),\n",
    "            pl.col(rmsd_col).max().alias('max_rmsd'),\n",
    "            pl.col(rmsd_col).mean().alias('mean_rmsd'),\n",
    "            pl.col(rmsd_col).median().alias('median_rmsd'),\n",
    "            (pl.col(rmsd_col) < 2.0).mean().alias('good_poses_pct')\n",
    "        ]).to_pandas().iloc[0]\n",
    "        \n",
    "        print(f\"   Range: {rmsd_stats['min_rmsd']:.2f} - {rmsd_stats['max_rmsd']:.2f} √Ö\")\n",
    "        print(f\"   Mean: {rmsd_stats['mean_rmsd']:.2f} √Ö, Median: {rmsd_stats['median_rmsd']:.2f} √Ö\")\n",
    "        print(f\"   Good poses (RMSD < 2.0 √Ö): {rmsd_stats['good_poses_pct']*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No RMSD columns found - pose consistency analysis may be limited\")\n",
    "    \n",
    "    # Check for score columns\n",
    "    score_columns = [col for col in combined_results.columns if 'score' in col.lower()]\n",
    "    print(f\"‚úÖ Score columns available: {len(score_columns)}\")\n",
    "    for col in score_columns[:5]:  # Show first 5 score columns\n",
    "        print(f\"   - {col}\")\n",
    "    if len(score_columns) > 5:\n",
    "        print(f\"   ... and {len(score_columns) - 5} more\")\n",
    "    \n",
    "    # Tool information\n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        tool1_unique = combined_results.filter(pl.col('Tool1').is_not_null())['Tool1'].n_unique()\n",
    "        tool2_unique = combined_results.filter(pl.col('Tool2').is_not_null())['Tool2'].n_unique()\n",
    "        print(f\"üîß Tool1 variants: {tool1_unique}\")\n",
    "        print(f\"üîß Tool2 variants: {tool2_unique}\")\n",
    "    \n",
    "    # Source information\n",
    "    if 'source_type' in combined_results.columns:\n",
    "        source_types = combined_results['source_type'].value_counts().to_pandas()\n",
    "        print(f\"üìÅ Source types:\")\n",
    "        for _, row in source_types.iterrows():\n",
    "            print(f\"   - {row['source_type']}: {row['count']:,} rows\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset quality check complete\")\n",
    "    print(f\"üéØ Ready for consensus analysis!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for overview\")\n",
    "    print(\"   Please load and filter data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dccec2",
   "metadata": {},
   "source": [
    "## üíæ Step 4: Save Prepared Data for Analysis\n",
    "\n",
    "**Purpose:** Save the filtered, annotated, and quality-checked dataset for later use.\n",
    "\n",
    "This checkpoint preserves the processed data after:\n",
    "- ‚úÖ Data loading and cluster integration (Steps 1-2)\n",
    "- ‚úÖ Sample type annotation (Step 2.5)\n",
    "- ‚úÖ Complete tool coverage filtering (Step 2.6)\n",
    "- ‚úÖ Balanced positive/negative sampling (Step 2.6)\n",
    "- ‚úÖ Data quality verification (Step 3)\n",
    "\n",
    "**Output:** A clean, analysis-ready Parquet file containing:\n",
    "- All consensus docking results with complete tool coverage\n",
    "- Balanced positive and negative samples\n",
    "- Annotated sample types and metadata\n",
    "- Integrated cavity cluster information\n",
    "\n",
    "This file can be reloaded in future sessions to skip preprocessing steps and jump directly to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results.write_parquet(\"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/combined_filtered_annotated_docking_results.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachopencadd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
