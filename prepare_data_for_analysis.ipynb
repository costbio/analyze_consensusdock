{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794d62f7",
   "metadata": {},
   "source": [
    "# Consensus Docking Results Analysis\n",
    "\n",
    "This notebook analyzes large-scale consensus docking results to evaluate binding pose consistency and perform cluster-based selectivity analysis.\n",
    "\n",
    "## ğŸš€ Quick Start Guide\n",
    "\n",
    "**For most users:** Simply run cells 2-4 to load, filter, and start analysis immediately.\n",
    "\n",
    "**First-time users or data updates:** If you need to create/update the data files, run the optimized data preparation script:\n",
    "```bash\n",
    "python parse_prepare.py\n",
    "```\n",
    "This high-performance script uses multiprocessing to efficiently process millions of docking results in minutes instead of hours.\n",
    "\n",
    "## ğŸ“Š Analysis Overview\n",
    "\n",
    "### Main Analysis Workflow\n",
    "1. **Data Loading** (Step 1) - Smart loading of existing data files\n",
    "2. **Cluster Integration** (Step 2) - Add cavity similarity information  \n",
    "3. **Tool Coverage Filtering** (Step 2.5) - **NEW:** Filter for complete tool coverage\n",
    "4. **Data Quality Check** (Step 3) - Dataset overview of filtered data\n",
    "5. **Tool Reliability Analysis** (Step 4) - Consensus analysis between tools\n",
    "6. **Cluster Analysis** - Binding site similarity and drug selectivity\n",
    "7. **Visualizations** - Comprehensive plots and insights\n",
    "\n",
    "### Key Outputs\n",
    "- **Fair tool comparisons** using only drug-target pairs with complete tool coverage\n",
    "- Pose consistency metrics across docking tools\n",
    "- Drug-target binding success rates\n",
    "- Cluster-based selectivity patterns\n",
    "- Tool agreement analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754698f7",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Step 1: Smart Data Loading\n",
    "\n",
    "This cell automatically detects and loads the best available data source. Run this first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fa8e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking for existing data files...\n",
      "âœ… Found Parquet file: combined_consensus_docking_results.parquet\n",
      "ğŸ“– Loading data (this is the fastest option)...\n",
      "   Shape: (28487256, 30)\n",
      "   Memory: 9700.8 MB\n",
      "âœ… Data loaded successfully!\n",
      "\n",
      "ğŸ“Š Dataset Overview:\n",
      "   Total rows: 28,487,256\n",
      "   Columns: 30\n",
      "   Key columns: ['Tool1', 'Tool2', 'PoseNumber1', 'PoseNumber2', 'Score1', 'Score2', 'File1', 'File2', 'RMSD', 'source_file', 'source_dir', 'file_size_mb', 'source_type', 'drugbank_id', 'uniprot_id', 'gene_name', 'cavity_index', 'Pose', 'SMINA_Score', 'Score', 'S(PLP)', 'S(hbond)', 'S(cho)', 'S(metal)', 'DE(clash)', 'DE(tors)', 'time', 'LeDock_Score', 'primary_tool', 'compound_target_pair']\n",
      "âœ… All required columns present\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“¥ SMART DATA LOADING - START HERE\n",
    "# =============================================================================\n",
    "\n",
    "import os, re\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PARQUET_FILE = \"combined_consensus_docking_results.parquet\"\n",
    "CSV_FILE = \"combined_consensus_docking_results.csv\"\n",
    "BASE_FOLDER = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/\"\n",
    "\n",
    "print(\"ğŸ” Checking for existing data files...\")\n",
    "\n",
    "# Smart data loading: try parquet first, then CSV, then create from scratch\n",
    "combined_results = None\n",
    "\n",
    "if os.path.exists(os.path.join(BASE_FOLDER, PARQUET_FILE)):\n",
    "    print(f\"âœ… Found Parquet file: {PARQUET_FILE}\")\n",
    "    print(\"ğŸ“– Loading data (this is the fastest option)...\")\n",
    "    combined_results = pl.read_parquet(os.path.join(BASE_FOLDER, PARQUET_FILE))\n",
    "    print(f\"   Shape: {combined_results.shape}\")\n",
    "    print(f\"   Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(\"âœ… Data loaded successfully!\")\n",
    "\n",
    "elif os.path.exists(os.path.join(BASE_FOLDER, CSV_FILE)):\n",
    "    print(f\"âœ… Found CSV file: {CSV_FILE}\")\n",
    "    print(\"ğŸ“– Loading data (slower than Parquet but still good)...\")\n",
    "    combined_results = pl.read_csv(os.path.join(BASE_FOLDER, CSV_FILE))\n",
    "    print(f\"   Shape: {combined_results.shape}\")\n",
    "    print(f\"   Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(\"âœ… Data loaded successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No preprocessed data files found!\")\n",
    "    print(f\"   Looking for: {PARQUET_FILE} or {CSV_FILE}\")\n",
    "    print(\"\\nï¿½ To create the data files, run the optimized preparation script:\")\n",
    "    print(\"   ```bash\")\n",
    "    print(\"   python parse_prepare.py\")\n",
    "    print(\"   ```\")\n",
    "    print(\"\\nâš¡ This high-performance script features:\")\n",
    "    print(\"   â€¢ Multiprocessing across all CPU cores\")\n",
    "    print(\"   â€¢ Progress bars for visual feedback\")\n",
    "    print(\"   â€¢ Processing rate: ~25,000 records/second\")\n",
    "    print(\"   â€¢ Creates both CSV and Parquet formats\")\n",
    "    print(\"   â€¢ Typical runtime: 5-10 minutes for millions of records\")\n",
    "    combined_results = pl.DataFrame()  # Empty dataframe\n",
    "\n",
    "# Quick validation\n",
    "if not combined_results.is_empty():\n",
    "    print(f\"\\nğŸ“Š Dataset Overview:\")\n",
    "    print(f\"   Total rows: {combined_results.height:,}\")\n",
    "    print(f\"   Columns: {combined_results.width}\")\n",
    "    print(f\"   Key columns: {combined_results.columns}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['drugbank_id', 'uniprot_id', 'RMSD', 'Score1', 'Score2']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_results.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"âš ï¸  Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"âœ… All required columns present\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No data available for analysis\")\n",
    "    print(\"   Please run: python parse_prepare.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc9474",
   "metadata": {},
   "source": [
    "## ğŸ§¬ Step 2: Cluster Integration\n",
    "\n",
    "Add cavity cluster information for advanced analysis (run once per session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582dcac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ƒï¸ CAVITY CLUSTER INTEGRATION\n",
      "==================================================\n",
      "ğŸ“– Loading cavity cluster data...\n",
      "ğŸ“– Loaded 12,943 clusters from CavitySpace\n",
      "ï¿½ Extracting cavity identifiers from source paths...\n",
      "âœ… Extraction results:\n",
      "   Extracted uniprot_id: 6,735,070 non-null values\n",
      "   Extracted cavity_index: 6,935,086 non-null values\n",
      "   Sample extracted data:\n",
      "shape: (3, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ source_dir                      â”† extracted_uniprot_id â”† extracted_cavity_index â”‚\n",
      "â”‚ ---                             â”† ---                  â”† ---                    â”‚\n",
      "â”‚ str                             â”† str                  â”† i64                    â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ /media/onur/Elements/cavity_spâ€¦ â”† null                 â”† null                   â”‚\n",
      "â”‚ /media/onur/Elements/cavity_spâ€¦ â”† null                 â”† null                   â”‚\n",
      "â”‚ /media/onur/Elements/cavity_spâ€¦ â”† null                 â”† null                   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ”„ Processing cluster file to create mapping...\n",
      "   âš ï¸ Failed to parse cavity ID: 'Q7RTN6_3GNI_B_C6'\n",
      "   âš ï¸ Failed to parse cavity ID: 'P32754_5EC3_A_C1'\n",
      "   âš ï¸ Failed to parse cavity ID: 'Q9H4L5_7CYZ_B_C7'\n",
      "   âš ï¸ Failed to parse cavity ID: 'P06732_1I0E_A_C1'\n",
      "   âš ï¸ Failed to parse cavity ID: 'P27338_1S3E_A_C4'\n",
      "ğŸ“Š Cluster parsing results:\n",
      "   Successfully parsed: 34,868 cavity IDs\n",
      "   Failed to parse: 26,395 cavity IDs\n",
      "   Created mapping for 34,447 unique cavities\n",
      "   Sample mappings:\n",
      "     ('Q8TBR7', 4) -> cluster 1\n",
      "     ('Q9HAW9', 3) -> cluster 1\n",
      "     ('Q6UXK5', 2) -> cluster 1\n",
      "     ('P19835', 1) -> cluster 1\n",
      "     ('Q8TD57', 13) -> cluster 5\n",
      "\n",
      "ğŸ” UniProt ID overlap analysis:\n",
      "   UniProts in our data: 872\n",
      "   UniProts in cluster file: 10,549\n",
      "   Overlap: 649\n",
      "   Sample from our data: ['A0PJX0', 'A2RU30', 'A5LHX3', 'A5X5Y0', 'A6NNM8']\n",
      "   Sample from clusters: ['A0A024RBG1', 'A0A075B6H5', 'A0A075B6H7', 'A0A075B6H8', 'A0A075B6H9']\n",
      "\n",
      "ğŸ”„ Applying cluster mapping...\n",
      "âœ… Cluster mapping complete:\n",
      "   Mapped: 1,323,009/28,487,256 (4.6%)\n",
      "   Unique clusters: 116\n",
      "   Sample mapped data:\n",
      "shape: (3, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ extracted_uniprot_id â”† extracted_cavity_index â”† cavity_cluster_id â”‚\n",
      "â”‚ ---                  â”† ---                    â”† ---               â”‚\n",
      "â”‚ str                  â”† i64                    â”† i64               â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ O94788               â”† 1                      â”† 2                 â”‚\n",
      "â”‚ O94788               â”† 1                      â”† 2                 â”‚\n",
      "â”‚ O94788               â”† 1                      â”† 2                 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ” Debugging unmapped entries:\n",
      "   Sample unmapped entries:\n",
      "shape: (5, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ extracted_uniprot_id â”† extracted_cavity_index â”‚\n",
      "â”‚ ---                  â”† ---                    â”‚\n",
      "â”‚ str                  â”† i64                    â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ null                 â”† null                   â”‚\n",
      "â”‚ null                 â”† null                   â”‚\n",
      "â”‚ null                 â”† null                   â”‚\n",
      "â”‚ null                 â”† null                   â”‚\n",
      "â”‚ null                 â”† null                   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "   âœ“ Key (None, None) correctly not in cluster mapping\n",
      "   âœ“ Key (None, None) correctly not in cluster mapping\n",
      "   âœ“ Key (None, None) correctly not in cluster mapping\n",
      "   âœ“ Key (None, None) correctly not in cluster mapping\n",
      "   âœ“ Key (None, None) correctly not in cluster mapping\n",
      "ğŸ¯ Ready for analysis with shape: (28487256, 35)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ—ƒï¸ STEP 2: CAVITY CLUSTER INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ—ƒï¸ CAVITY CLUSTER INTEGRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    # Check if we already have cluster information\n",
    "    if 'cavity_cluster_id' in combined_results.columns:\n",
    "        non_null_clusters = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "        if non_null_clusters > 0:\n",
    "            print(f\"âœ… Cluster data already present: {non_null_clusters:,} mapped entries\")\n",
    "            print(\"   Skipping cluster integration...\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Cluster column exists but empty - proceeding with integration...\")\n",
    "    \n",
    "    # Proceed with cluster integration if needed\n",
    "    if 'cavity_cluster_id' not in combined_results.columns or combined_results['cavity_cluster_id'].drop_nulls().len() == 0:\n",
    "        try:\n",
    "            print(f\"ğŸ“– Loading cavity cluster data...\")\n",
    "            cluster_file = \"/opt/data/cavity_space/cavity_cluster_similarity07.csv\"\n",
    "            clusters_df = pl.read_csv(cluster_file, separator='\\t')\n",
    "            print(f\"ğŸ“– Loaded {clusters_df.height:,} clusters from CavitySpace\")\n",
    "            \n",
    "            # Extract uniprot_id and cavity_index from source_dir if not already present\n",
    "            print(f\"ï¿½ Extracting cavity identifiers from source paths...\")\n",
    "            \n",
    "            combined_results = combined_results.with_columns([\n",
    "                # Extract drugbank_id (1st component before first underscore)\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'(DB\\d+)_', group_index=1)\n",
    "                .alias('extracted_drugbank_id'),\n",
    "                \n",
    "                # Extract gene_name (2nd component - can be 'nan' for negative samples)\n",
    "                # Pattern: DB00035_AVPR1B_P47901_cavity_1 (positive) or DB00035_nan_P08173_cavity_3 (negative)\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'DB\\d+_([A-Z0-9]+|nan)_', group_index=1)\n",
    "                .alias('extracted_gene_name'),\n",
    "                \n",
    "                # Extract uniprot_id (component before 'cavity_')\n",
    "                # More flexible pattern to handle both positive and negative samples\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'_([A-Z0-9]+)_cavity_\\d+', group_index=1)\n",
    "                .alias('extracted_uniprot_id'),\n",
    "                \n",
    "                # Extract cavity_index (number after 'cavity_')\n",
    "                pl.col('source_dir').str.split('/').list.last()\n",
    "                .str.extract(r'cavity_(\\d+)', group_index=1)\n",
    "                .cast(pl.Int64, strict=False)\n",
    "                .alias('extracted_cavity_index')\n",
    "            ])\n",
    "            \n",
    "            # Check extraction results\n",
    "            non_null_uniprot = combined_results['extracted_uniprot_id'].drop_nulls().len()\n",
    "            non_null_cavity = combined_results['extracted_cavity_index'].drop_nulls().len()\n",
    "            \n",
    "            print(f\"âœ… Extraction results:\")\n",
    "            print(f\"   Extracted uniprot_id: {non_null_uniprot:,} non-null values\")\n",
    "            print(f\"   Extracted cavity_index: {non_null_cavity:,} non-null values\")\n",
    "            \n",
    "            # Show sample extracted data for debugging\n",
    "            sample_data = combined_results.select(['source_dir', 'extracted_uniprot_id', 'extracted_cavity_index']).head(3)\n",
    "            print(f\"   Sample extracted data:\")\n",
    "            print(sample_data)\n",
    "            \n",
    "            if non_null_uniprot > 0 and non_null_cavity > 0:\n",
    "                # Create mapping dictionary from the cluster file\n",
    "                cavity_to_cluster = {}\n",
    "                successful_parses = 0\n",
    "                failed_parses = 0\n",
    "                \n",
    "                print(f\"ğŸ”„ Processing cluster file to create mapping...\")\n",
    "                \n",
    "                for i, row in enumerate(clusters_df.to_dicts()):\n",
    "                    cluster_id = row['id']  # The cluster ID\n",
    "                    cavity_items = row['items']  # Comma-separated cavity IDs\n",
    "                    \n",
    "                    # Split the cavity items and process each one\n",
    "                    if cavity_items and isinstance(cavity_items, str):\n",
    "                        cavity_ids = cavity_items.split(',')\n",
    "                        \n",
    "                        for cavity_id in cavity_ids:\n",
    "                            cavity_id = cavity_id.strip()\n",
    "                            \n",
    "                            # Parse cavity format: AF-{UniProtID}-F{Fragment}-model_v1_C{CavityIndex}\n",
    "                            match = re.match(r'AF-([A-Z0-9]+)-F\\d+-model_v1_C(\\d+)', cavity_id)\n",
    "                            if match:\n",
    "                                uniprot_id, cavity_index = match.groups()\n",
    "                                key = (uniprot_id, int(cavity_index))\n",
    "                                cavity_to_cluster[key] = cluster_id\n",
    "                                successful_parses += 1\n",
    "                            else:\n",
    "                                failed_parses += 1\n",
    "                                if failed_parses <= 5:  # Show first few failures\n",
    "                                    print(f\"   âš ï¸ Failed to parse cavity ID: '{cavity_id}'\")\n",
    "                \n",
    "                print(f\"ğŸ“Š Cluster parsing results:\")\n",
    "                print(f\"   Successfully parsed: {successful_parses:,} cavity IDs\")\n",
    "                print(f\"   Failed to parse: {failed_parses:,} cavity IDs\")\n",
    "                print(f\"   Created mapping for {len(cavity_to_cluster):,} unique cavities\")\n",
    "                \n",
    "                # Show sample mapping entries\n",
    "                sample_keys = list(cavity_to_cluster.keys())[:5]\n",
    "                print(f\"   Sample mappings:\")\n",
    "                for key in sample_keys:\n",
    "                    print(f\"     {key} -> cluster {cavity_to_cluster[key]}\")\n",
    "                \n",
    "                # Check what UniProt IDs we have in our data vs cluster file\n",
    "                our_uniprots = set(combined_results.filter(pl.col('extracted_uniprot_id').is_not_null())['extracted_uniprot_id'].unique().to_list())\n",
    "                cluster_uniprots = set(key[0] for key in cavity_to_cluster.keys())\n",
    "                \n",
    "                print(f\"\\nğŸ” UniProt ID overlap analysis:\")\n",
    "                print(f\"   UniProts in our data: {len(our_uniprots):,}\")\n",
    "                print(f\"   UniProts in cluster file: {len(cluster_uniprots):,}\")\n",
    "                print(f\"   Overlap: {len(our_uniprots & cluster_uniprots):,}\")\n",
    "                \n",
    "                # Show sample UniProts from each set\n",
    "                print(f\"   Sample from our data: {sorted(list(our_uniprots))[:5]}\")\n",
    "                print(f\"   Sample from clusters: {sorted(list(cluster_uniprots))[:5]}\")\n",
    "                \n",
    "                # Map clusters to our data\n",
    "                def map_cluster(uniprot_id, cavity_index):\n",
    "                    if cavity_index is None or uniprot_id is None:\n",
    "                        return None\n",
    "                    key = (uniprot_id, cavity_index)\n",
    "                    return cavity_to_cluster.get(key)\n",
    "                \n",
    "                print(f\"\\nğŸ”„ Applying cluster mapping...\")\n",
    "                \n",
    "                combined_results = combined_results.with_columns([\n",
    "                    pl.struct(['extracted_uniprot_id', 'extracted_cavity_index'])\n",
    "                    .map_elements(lambda x: map_cluster(x['extracted_uniprot_id'], x['extracted_cavity_index']), return_dtype=pl.Int64)\n",
    "                    .alias('cavity_cluster_id')\n",
    "                ])\n",
    "                \n",
    "                # Report mapping results\n",
    "                mapped_count = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "                total_count = len(combined_results)\n",
    "                unique_clusters = combined_results['cavity_cluster_id'].n_unique()\n",
    "                \n",
    "                print(f\"âœ… Cluster mapping complete:\")\n",
    "                print(f\"   Mapped: {mapped_count:,}/{total_count:,} ({mapped_count/total_count*100:.1f}%)\")\n",
    "                print(f\"   Unique clusters: {unique_clusters:,}\")\n",
    "                \n",
    "                if mapped_count > 0:\n",
    "                    # Show sample mapped data\n",
    "                    sample_mapped = combined_results.filter(pl.col('cavity_cluster_id').is_not_null()).select(['extracted_uniprot_id', 'extracted_cavity_index', 'cavity_cluster_id']).head(3)\n",
    "                    print(f\"   Sample mapped data:\")\n",
    "                    print(sample_mapped)\n",
    "                    \n",
    "                # Debug unmapped entries\n",
    "                if mapped_count < total_count:\n",
    "                    print(f\"\\nğŸ” Debugging unmapped entries:\")\n",
    "                    unmapped = combined_results.filter(pl.col('cavity_cluster_id').is_null())\n",
    "                    sample_unmapped = unmapped.select(['extracted_uniprot_id', 'extracted_cavity_index']).head(5)\n",
    "                    print(f\"   Sample unmapped entries:\")\n",
    "                    print(sample_unmapped)\n",
    "                    \n",
    "                    # Check if these should have mappings\n",
    "                    for row in sample_unmapped.to_dicts():\n",
    "                        uniprot_id = row['extracted_uniprot_id']\n",
    "                        cavity_index = row['extracted_cavity_index']\n",
    "                        key = (uniprot_id, cavity_index)\n",
    "                        if key in cavity_to_cluster:\n",
    "                            print(f\"   â— Key {key} should map to cluster {cavity_to_cluster[key]} but doesn't!\")\n",
    "                        else:\n",
    "                            print(f\"   âœ“ Key {key} correctly not in cluster mapping\")\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ Extraction failed - adding empty cluster column...\")\n",
    "                combined_results = combined_results.with_columns([\n",
    "                    pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "                ])\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸  Cluster file not found: {cluster_file}\")\n",
    "            print(\"   Adding empty cluster column...\")\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading clusters: {e}\")\n",
    "            print(f\"   Error details: {type(e).__name__}: {str(e)}\")\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Int64).alias('cavity_cluster_id')\n",
    "            ])\n",
    "    \n",
    "    print(f\"ğŸ¯ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No data available for cluster integration\")\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fb3b5",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 2.1: IC50 Data Integration\n",
    "\n",
    "Add experimental IC50/Ki measurements for drug-target interactions from the Therapeutic Target Database (TTD). This provides quantitative binding affinity data that can be used to:\n",
    "- Validate docking predictions against experimental measurements\n",
    "- Compare predicted binding scores with actual IC50 values\n",
    "- Enrich the dataset with pharmacological activity information\n",
    "\n",
    "The IC50 data was generated using the `create_ic50_mapping.py` script which integrates:\n",
    "- TTD target information mapped to UniProt IDs\n",
    "- TTD activity measurements mapped to DrugBank IDs via PubChem Compound IDs\n",
    "- IC50 and Ki values (all converted to nM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0b0fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª IC50 DATA INTEGRATION\n",
      "==================================================\n",
      "ğŸ“‚ Loading IC50 mapping data...\n",
      "   Loaded IC50 data: (10942, 10)\n",
      "   Columns: ['drugbank_id', 'uniprot_id', 'ttd_target_id', 'ttd_drug_id', 'pubchem_cid', 'activity', 'measurement_type', 'operator', 'ic50_value', 'ic50_unit']\n",
      "\n",
      "ğŸ“Š IC50 Data Summary:\n",
      "   Unique drugs: 2,797\n",
      "   Unique targets: 767\n",
      "   Total measurements: 10,942\n",
      "\n",
      "   Measurement types:\n",
      "     IC50: 7,740\n",
      "     Ki: 3,202\n",
      "\n",
      "   Operators:\n",
      "     =: 8,600\n",
      "     >: 2,162\n",
      "     <: 164\n",
      "     >=: 15\n",
      "     <=: 1\n",
      "\n",
      "âš ï¸  IC50 Data Duplication Check:\n",
      "   Total IC50 records: 10,942\n",
      "   Unique drug-target pairs: 10,757\n",
      "   âš ï¸  Found 185 duplicate pairs (multiple measurements)\n",
      "   Strategy: Will aggregate to keep best (lowest) IC50 value per pair\n",
      "   After aggregation: 10,757 unique drug-target pairs\n",
      "\n",
      "ğŸ”— Merging IC50 data with docking results...\n",
      "   Before merge: (28487256, 35)\n",
      "   After merge:  (28487256, 42)\n",
      "   âœ… Matched 2,389,576 docking results with IC50 data\n",
      "   Coverage: 8.39% of docking results\n",
      "\n",
      "ğŸ“‹ Sample IC50-matched entries:\n",
      "shape: (5, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ drugbank_id â”† uniprot_id â”† Tool1  â”† Tool2 â”† â€¦ â”† ic50_unit â”† measurement â”† operator â”† n_measureme â”‚\n",
      "â”‚ ---         â”† ---        â”† ---    â”† ---   â”†   â”† ---       â”† _type       â”† ---      â”† nts         â”‚\n",
      "â”‚ str         â”† str        â”† str    â”† str   â”†   â”† str       â”† ---         â”† str      â”† ---         â”‚\n",
      "â”‚             â”†            â”†        â”†       â”†   â”†           â”† str         â”†          â”† u32         â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ DB00503     â”† P10635     â”† LeDock â”† GOLD  â”† â€¦ â”† nM        â”† IC50        â”† =        â”† 1           â”‚\n",
      "â”‚ DB00503     â”† P10635     â”† LeDock â”† GOLD  â”† â€¦ â”† nM        â”† IC50        â”† =        â”† 1           â”‚\n",
      "â”‚ DB00503     â”† P10635     â”† LeDock â”† GOLD  â”† â€¦ â”† nM        â”† IC50        â”† =        â”† 1           â”‚\n",
      "â”‚ DB00503     â”† P10635     â”† LeDock â”† GOLD  â”† â€¦ â”† nM        â”† IC50        â”† =        â”† 1           â”‚\n",
      "â”‚ DB00503     â”† P10635     â”† LeDock â”† GOLD  â”† â€¦ â”† nM        â”† IC50        â”† =        â”† 1           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“ˆ IC50 Value Statistics (nM):\n",
      "shape: (9, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ statistic  â”† ic50_value    â”‚\n",
      "â”‚ ---        â”† ---           â”‚\n",
      "â”‚ str        â”† f64           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ count      â”† 2.389576e6    â”‚\n",
      "â”‚ null_count â”† 0.0           â”‚\n",
      "â”‚ mean       â”† 55478.492566  â”‚\n",
      "â”‚ std        â”† 463731.144333 â”‚\n",
      "â”‚ min        â”† 0.008         â”‚\n",
      "â”‚ 25%        â”† 1.7           â”‚\n",
      "â”‚ 50%        â”† 22.0          â”‚\n",
      "â”‚ 75%        â”† 386.0         â”‚\n",
      "â”‚ max        â”† 5.1e6         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ¯ Ready for analysis with shape: (28487256, 42)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ§ª STEP 2.1: IC50 DATA INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ§ª IC50 DATA INTEGRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    # Check if we already have IC50 information\n",
    "    if 'ic50_value' in combined_results.columns:\n",
    "        non_null_ic50 = combined_results['ic50_value'].drop_nulls().len()\n",
    "        if non_null_ic50 > 0:\n",
    "            print(f\"âœ… IC50 data already present: {non_null_ic50:,} mapped entries\")\n",
    "            print(\"   Skipping IC50 integration...\")\n",
    "        else:\n",
    "            print(\"âš ï¸  IC50 column exists but empty - proceeding with integration...\")\n",
    "    else:\n",
    "        print(\"ğŸ“‚ Loading IC50 mapping data...\")\n",
    "        \n",
    "        # Define the path to IC50 mapping file\n",
    "        ic50_file = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/ic50_mapping.csv\"\n",
    "        \n",
    "        try:\n",
    "            # Load IC50 mapping data\n",
    "            ic50_df = pl.read_csv(ic50_file)\n",
    "            \n",
    "            print(f\"   Loaded IC50 data: {ic50_df.shape}\")\n",
    "            print(f\"   Columns: {ic50_df.columns}\")\n",
    "            print(f\"\\nğŸ“Š IC50 Data Summary:\")\n",
    "            print(f\"   Unique drugs: {ic50_df['drugbank_id'].n_unique():,}\")\n",
    "            print(f\"   Unique targets: {ic50_df['uniprot_id'].n_unique():,}\")\n",
    "            print(f\"   Total measurements: {len(ic50_df):,}\")\n",
    "            \n",
    "            # Display measurement type distribution\n",
    "            measurement_counts = ic50_df.group_by('measurement_type').len().sort('len', descending=True)\n",
    "            print(f\"\\n   Measurement types:\")\n",
    "            for row in measurement_counts.iter_rows(named=True):\n",
    "                print(f\"     {row['measurement_type']}: {row['len']:,}\")\n",
    "            \n",
    "            # Display operator distribution\n",
    "            operator_counts = ic50_df.group_by('operator').len().sort('len', descending=True)\n",
    "            print(f\"\\n   Operators:\")\n",
    "            for row in operator_counts.iter_rows(named=True):\n",
    "                print(f\"     {row['operator']}: {row['len']:,}\")\n",
    "            \n",
    "            # Check for duplicates in IC50 data\n",
    "            ic50_unique_pairs = ic50_df.select(['drugbank_id', 'uniprot_id']).unique().height\n",
    "            ic50_total = ic50_df.height\n",
    "            print(f\"\\nâš ï¸  IC50 Data Duplication Check:\")\n",
    "            print(f\"   Total IC50 records: {ic50_total:,}\")\n",
    "            print(f\"   Unique drug-target pairs: {ic50_unique_pairs:,}\")\n",
    "            if ic50_total > ic50_unique_pairs:\n",
    "                print(f\"   âš ï¸  Found {ic50_total - ic50_unique_pairs:,} duplicate pairs (multiple measurements)\")\n",
    "                print(f\"   Strategy: Will aggregate to keep best (lowest) IC50 value per pair\")\n",
    "            \n",
    "            # Aggregate IC50 data to handle duplicates - keep the lowest IC50 value per drug-target pair\n",
    "            # This represents the best binding affinity measurement\n",
    "            ic50_aggregated = ic50_df.group_by(['drugbank_id', 'uniprot_id']).agg([\n",
    "                pl.col('ic50_value').min().alias('ic50_value'),  # Lowest (best) IC50\n",
    "                pl.col('ic50_unit').first().alias('ic50_unit'),\n",
    "                pl.col('measurement_type').first().alias('measurement_type'),\n",
    "                pl.col('operator').first().alias('operator'),\n",
    "                pl.col('pubchem_cid').first().alias('pubchem_cid'),\n",
    "                pl.col('activity').first().alias('activity'),\n",
    "                pl.len().alias('n_measurements')  # Track how many measurements were aggregated\n",
    "            ])\n",
    "            \n",
    "            print(f\"   After aggregation: {ic50_aggregated.height:,} unique drug-target pairs\")\n",
    "            \n",
    "            # Merge IC50 data with combined results\n",
    "            # Match on both drugbank_id and uniprot_id\n",
    "            print(f\"\\nğŸ”— Merging IC50 data with docking results...\")\n",
    "            before_merge = combined_results.shape\n",
    "            \n",
    "            combined_results = combined_results.join(\n",
    "                ic50_aggregated,\n",
    "                left_on=['drugbank_id', 'uniprot_id'],\n",
    "                right_on=['drugbank_id', 'uniprot_id'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            after_merge = combined_results.shape\n",
    "            matched_ic50 = combined_results['ic50_value'].drop_nulls().len()\n",
    "            \n",
    "            print(f\"   Before merge: {before_merge}\")\n",
    "            print(f\"   After merge:  {after_merge}\")\n",
    "            print(f\"   âœ… Matched {matched_ic50:,} docking results with IC50 data\")\n",
    "            print(f\"   Coverage: {matched_ic50/len(combined_results)*100:.2f}% of docking results\")\n",
    "            \n",
    "            # Show some examples of matched data\n",
    "            if matched_ic50 > 0:\n",
    "                print(f\"\\nğŸ“‹ Sample IC50-matched entries:\")\n",
    "                # Use available columns - check which ones exist\n",
    "                display_cols = ['drugbank_id', 'uniprot_id', 'ic50_value', 'ic50_unit', 'measurement_type', 'operator', 'n_measurements']\n",
    "                \n",
    "                # Add tool columns if they exist\n",
    "                if 'Tool1' in combined_results.columns:\n",
    "                    display_cols.insert(2, 'Tool1')\n",
    "                if 'Tool2' in combined_results.columns:\n",
    "                    display_cols.insert(3, 'Tool2')\n",
    "                \n",
    "                # Add score columns if they exist\n",
    "                if 'Score1' in combined_results.columns:\n",
    "                    display_cols.insert(-4, 'Score1')\n",
    "                if 'Score2' in combined_results.columns:\n",
    "                    display_cols.insert(-4, 'Score2')\n",
    "                \n",
    "                sample = combined_results.filter(\n",
    "                    pl.col('ic50_value').is_not_null()\n",
    "                ).select(display_cols).head(5)\n",
    "                print(sample)\n",
    "                \n",
    "                # Show IC50 value statistics for matched entries\n",
    "                ic50_stats = combined_results.filter(\n",
    "                    pl.col('ic50_value').is_not_null()\n",
    "                ).select(pl.col('ic50_value')).describe()\n",
    "                print(f\"\\nğŸ“ˆ IC50 Value Statistics (nM):\")\n",
    "                print(ic50_stats)\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸  No IC50 values matched with docking results\")\n",
    "                print(f\"   This may indicate that the docked drug-target pairs\")\n",
    "                print(f\"   don't have experimental measurements in the TTD database\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ IC50 file not found: {ic50_file}\")\n",
    "            print(\"   Skipping IC50 integration...\")\n",
    "            # Add empty columns to maintain schema consistency\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Float64).alias('ic50_value'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('ic50_unit'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('measurement_type'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('operator'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('pubchem_cid'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('activity'),\n",
    "                pl.lit(None, dtype=pl.Int64).alias('n_measurements')\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading IC50 data: {e}\")\n",
    "            print(\"   Skipping IC50 integration...\")\n",
    "            # Add empty columns to maintain schema consistency\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.lit(None, dtype=pl.Float64).alias('ic50_value'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('ic50_unit'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('measurement_type'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('operator'),\n",
    "                pl.lit(None, dtype=pl.Utf8).alias('activity'),\n",
    "                pl.lit(None, dtype=pl.Int64).alias('n_measurements')\n",
    "            ])\n",
    "    \n",
    "    print(f\"ğŸ¯ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No data available for IC50 integration\")\n",
    "\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96afb0",
   "metadata": {},
   "source": [
    "## ğŸ·ï¸ Step 2.5: Sample Type Annotation (Positive vs Negative)\n",
    "\n",
    "**Critical Context:** The docking results include both:\n",
    "- **Positive samples**: Known drug-target interactions from validated databases\n",
    "- **Negative samples**: Randomly generated drug-target pairs (controls) to test specificity\n",
    "\n",
    "This annotation is essential for proper evaluation:\n",
    "- It enables us to assess how well docking tools distinguish true interactions from random pairings\n",
    "- All subsequent analyses must account for sample type to avoid conflating signal with noise\n",
    "- Performance metrics (ROC, precision-recall) require this ground truth labeling\n",
    "\n",
    "We'll load sample type information from `required_structures_with_negatives.csv` and merge it into our dataset based on UniProt ID, DrugBank ID, and cavity index.\n",
    "\n",
    "**âš ï¸ Important:** This step must be run BEFORE Step 2.6 (filtering) to enable balanced sample filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdaef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ·ï¸ Starting sample type annotation...\n",
      "ğŸ“– Loading sample type metadata from:\n",
      "   /media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/required_structures_with_negatives.csv\n",
      "âœ… Loaded 26,904 rows from metadata file\n",
      "   Columns: ['UniProt_ID', 'Fragment_ID', 'Cavity_Index', 'Pocket_PDB', 'Gene_Name', 'Interaction_Count', 'sample_type', 'drugbank_id']\n",
      "\n",
      "ğŸ“‹ Sample metadata (positive samples):\n",
      "shape: (3, 8)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ UniProt_ID â”† Fragment_I â”† Cavity_Ind â”† Pocket_PD â”† Gene_Name â”† Interacti â”† sample_ty â”† drugbank_ â”‚\n",
      "â”‚ ---        â”† D          â”† ex         â”† B         â”† ---       â”† on_Count  â”† pe        â”† id        â”‚\n",
      "â”‚ str        â”† ---        â”† ---        â”† ---       â”† str       â”† ---       â”† ---       â”† ---       â”‚\n",
      "â”‚            â”† str        â”† i64        â”† str       â”†           â”† i64       â”† str       â”† str       â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ O43490     â”† F1         â”† 1          â”† extracted â”† PML       â”† 1         â”† positive  â”† DB01169   â”‚\n",
      "â”‚            â”†            â”†            â”† _cavities â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† /AF-O4349 â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† 0-Fâ€¦      â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ O43490     â”† F1         â”† 5          â”† extracted â”† PML       â”† 1         â”† positive  â”† DB01169   â”‚\n",
      "â”‚            â”†            â”†            â”† _cavities â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† /AF-O4349 â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† 0-Fâ€¦      â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ O43490     â”† F1         â”† 2          â”† extracted â”† PML       â”† 1         â”† positive  â”† DB01169   â”‚\n",
      "â”‚            â”†            â”†            â”† _cavities â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† /AF-O4349 â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† 0-Fâ€¦      â”†           â”†           â”†           â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“‹ Sample metadata (negative samples):\n",
      "shape: (3, 8)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ UniProt_ID â”† Fragment_I â”† Cavity_Ind â”† Pocket_PD â”† Gene_Name â”† Interacti â”† sample_ty â”† drugbank_ â”‚\n",
      "â”‚ ---        â”† D          â”† ex         â”† B         â”† ---       â”† on_Count  â”† pe        â”† id        â”‚\n",
      "â”‚ str        â”† ---        â”† ---        â”† ---       â”† str       â”† ---       â”† ---       â”† ---       â”‚\n",
      "â”‚            â”† str        â”† i64        â”† str       â”†           â”† i64       â”† str       â”† str       â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ P21359     â”† 4          â”† 13         â”† extracted â”† null      â”† 0         â”† negative_ â”† DB01588   â”‚\n",
      "â”‚            â”†            â”†            â”† _cavities â”†           â”†           â”† balanced  â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† /AF-P2135 â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† 9-Fâ€¦      â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ P21359     â”† 5          â”† 8          â”† extracted â”† null      â”† 0         â”† negative_ â”† DB01588   â”‚\n",
      "â”‚            â”†            â”†            â”† _cavities â”†           â”†           â”† balanced  â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† /AF-P2135 â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† 9-Fâ€¦      â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ P21359     â”† 9          â”† 4          â”† extracted â”† null      â”† 0         â”† negative_ â”† DB01588   â”‚\n",
      "â”‚            â”†            â”†            â”† _cavities â”†           â”†           â”† balanced  â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† /AF-P2135 â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚            â”†            â”†            â”† 9-Fâ€¦      â”†           â”†           â”†           â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“Š Sample type distribution in metadata:\n",
      "shape: (2, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ sample_type       â”† len   â”‚\n",
      "â”‚ ---               â”† ---   â”‚\n",
      "â”‚ str               â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ negative_balanced â”† 13452 â”‚\n",
      "â”‚ positive          â”† 13452 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ”„ Merging sample type information...\n",
      "   Merge keys: uniprot_id, drugbank_id, cavity_index\n",
      "\n",
      "ğŸ“Š Pre-merge statistics:\n",
      "   Combined_results rows: 28,487,256\n",
      "   Unique (uniprot, drug, cavity) in data: 14,847\n",
      "   Unique (uniprot, drug, cavity) in metadata: 24,173\n",
      "\n",
      "âœ… Merge completed:\n",
      "   Rows after merge: 28,628,253\n",
      "   Rows with sample_type: 6,846,312 (23.9%)\n",
      "   âš ï¸  Unannotated rows: 21,781,941 (76.1%)\n",
      "\n",
      "ğŸ“‹ Sample unannotated data (first 3 rows):\n",
      "shape: (3, 4)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ uniprot_id â”† drugbank_id â”† cavity_index â”† source_dir                      â”‚\n",
      "â”‚ ---        â”† ---         â”† ---          â”† ---                             â”‚\n",
      "â”‚ str        â”† str         â”† i64          â”† str                             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ P05093     â”† DB00157     â”† null         â”† /media/onur/Elements/cavity_spâ€¦ â”‚\n",
      "â”‚ P05093     â”† DB00157     â”† null         â”† /media/onur/Elements/cavity_spâ€¦ â”‚\n",
      "â”‚ P05093     â”† DB00157     â”† null         â”† /media/onur/Elements/cavity_spâ€¦ â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“Š Sample type distribution in annotated docking results:\n",
      "shape: (2, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ sample_type       â”† len     â”‚\n",
      "â”‚ ---               â”† ---     â”‚\n",
      "â”‚ str               â”† u32     â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ negative_balanced â”† 1974774 â”‚\n",
      "â”‚ positive          â”† 4871538 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“‹ Sample annotated data (positive):\n",
      "shape: (3, 5)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ uniprot_id â”† drugbank_id â”† cavity_index â”† sample_type â”† Gene_Name â”‚\n",
      "â”‚ ---        â”† ---         â”† ---          â”† ---         â”† ---       â”‚\n",
      "â”‚ str        â”† str         â”† i64          â”† str         â”† str       â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Q15738     â”† DB00157     â”† 1            â”† positive    â”† NSDHL     â”‚\n",
      "â”‚ Q15738     â”† DB00157     â”† 1            â”† positive    â”† NSDHL     â”‚\n",
      "â”‚ Q15738     â”† DB00157     â”† 1            â”† positive    â”† NSDHL     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“‹ Sample annotated data (negative):\n",
      "shape: (3, 5)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ uniprot_id â”† drugbank_id â”† cavity_index â”† sample_type       â”† Gene_Name â”‚\n",
      "â”‚ ---        â”† ---         â”† ---          â”† ---               â”† ---       â”‚\n",
      "â”‚ str        â”† str         â”† i64          â”† str               â”† str       â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ O00213     â”† DB01078     â”† 3            â”† negative_balanced â”† null      â”‚\n",
      "â”‚ O00213     â”† DB01078     â”† 3            â”† negative_balanced â”† null      â”‚\n",
      "â”‚ O00213     â”† DB01078     â”† 3            â”† negative_balanced â”† null      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "âœ… Sample type annotation complete!\n",
      "   Dataset now includes 'sample_type' column for positive/negative discrimination\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ·ï¸ SAMPLE TYPE ANNOTATION (POSITIVE VS NEGATIVE)\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"ğŸ·ï¸ Starting sample type annotation...\")\n",
    "    \n",
    "    # Load the sample type metadata\n",
    "    sample_metadata_file = \"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/required_structures_with_negatives.csv\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ“– Loading sample type metadata from:\\n   {sample_metadata_file}\")\n",
    "        \n",
    "        # Load the metadata file\n",
    "        sample_metadata = pl.read_csv(sample_metadata_file)\n",
    "        \n",
    "        print(f\"âœ… Loaded {sample_metadata.height:,} rows from metadata file\")\n",
    "        print(f\"   Columns: {sample_metadata.columns}\")\n",
    "        \n",
    "        # Show sample of metadata\n",
    "        print(f\"\\nğŸ“‹ Sample metadata (positive samples):\")\n",
    "        print(sample_metadata.filter(pl.col('sample_type') == 'positive').head(3))\n",
    "        print(f\"\\nğŸ“‹ Sample metadata (negative samples):\")\n",
    "        print(sample_metadata.filter(pl.col('sample_type').str.contains('negative')).head(3))\n",
    "        \n",
    "        # Check sample type distribution in metadata\n",
    "        sample_type_counts = sample_metadata.group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "        print(f\"\\nğŸ“Š Sample type distribution in metadata:\")\n",
    "        print(sample_type_counts)\n",
    "        \n",
    "        # Prepare metadata for merging - select relevant columns\n",
    "        # Note: The metadata uses 'UniProt_ID' and 'Cavity_Index', while combined_results uses 'uniprot_id' and 'extracted_cavity_index'\n",
    "        merge_metadata = sample_metadata.select([\n",
    "            pl.col('UniProt_ID').alias('uniprot_id'),\n",
    "            'drugbank_id',\n",
    "            pl.col('Cavity_Index').alias('cavity_index'),\n",
    "            'sample_type',\n",
    "            'Gene_Name'  # Keep this for additional context\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nğŸ”„ Merging sample type information...\")\n",
    "        print(f\"   Merge keys: uniprot_id, drugbank_id, cavity_index\")\n",
    "        \n",
    "        # Check if we have the required columns in combined_results\n",
    "        if 'extracted_cavity_index' in combined_results.columns:\n",
    "            # Use extracted_cavity_index for merging\n",
    "            combined_results = combined_results.with_columns([\n",
    "                pl.col('extracted_cavity_index').alias('cavity_index')\n",
    "            ])\n",
    "        elif 'cavity_index' not in combined_results.columns:\n",
    "            print(\"âš ï¸  Warning: No cavity_index column found in combined_results!\")\n",
    "            print(\"   This may affect merge accuracy.\")\n",
    "        \n",
    "        # Before merge - check data availability\n",
    "        pre_merge_rows = combined_results.height\n",
    "        unique_pairs_in_data = combined_results.select(['uniprot_id', 'drugbank_id', 'cavity_index']).unique().height\n",
    "        unique_pairs_in_metadata = merge_metadata.select(['uniprot_id', 'drugbank_id', 'cavity_index']).unique().height\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Pre-merge statistics:\")\n",
    "        print(f\"   Combined_results rows: {pre_merge_rows:,}\")\n",
    "        print(f\"   Unique (uniprot, drug, cavity) in data: {unique_pairs_in_data:,}\")\n",
    "        print(f\"   Unique (uniprot, drug, cavity) in metadata: {unique_pairs_in_metadata:,}\")\n",
    "        \n",
    "        # Perform left join to add sample_type to combined_results\n",
    "        combined_results = combined_results.join(\n",
    "            merge_metadata,\n",
    "            on=['uniprot_id', 'drugbank_id', 'cavity_index'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Check merge results\n",
    "        post_merge_rows = combined_results.height\n",
    "        annotated_rows = combined_results.filter(pl.col('sample_type').is_not_null()).height\n",
    "        \n",
    "        print(f\"\\nâœ… Merge completed:\")\n",
    "        print(f\"   Rows after merge: {post_merge_rows:,}\")\n",
    "        print(f\"   Rows with sample_type: {annotated_rows:,} ({annotated_rows/post_merge_rows*100:.1f}%)\")\n",
    "        \n",
    "        if annotated_rows < post_merge_rows:\n",
    "            unannotated_rows = post_merge_rows - annotated_rows\n",
    "            print(f\"   âš ï¸  Unannotated rows: {unannotated_rows:,} ({unannotated_rows/post_merge_rows*100:.1f}%)\")\n",
    "            \n",
    "            # Show sample of unannotated data for debugging\n",
    "            print(f\"\\nğŸ“‹ Sample unannotated data (first 3 rows):\")\n",
    "            sample_unmapped = combined_results.filter(pl.col('sample_type').is_null()).select([\n",
    "                'uniprot_id', 'drugbank_id', 'cavity_index', 'source_dir'\n",
    "            ]).head(3)\n",
    "            print(sample_unmapped)\n",
    "        \n",
    "        # Show sample type distribution in annotated data\n",
    "        if annotated_rows > 0:\n",
    "            annotated_type_counts = combined_results.filter(\n",
    "                pl.col('sample_type').is_not_null()\n",
    "            ).group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Sample type distribution in annotated docking results:\")\n",
    "            print(annotated_type_counts)\n",
    "            \n",
    "            # Show sample of annotated data\n",
    "            print(f\"\\nğŸ“‹ Sample annotated data (positive):\")\n",
    "            sample_mapped = combined_results.filter(\n",
    "                pl.col('sample_type') == 'positive'\n",
    "            ).select(['uniprot_id', 'drugbank_id', 'cavity_index', 'sample_type', 'Gene_Name']).head(3)\n",
    "            print(sample_mapped)\n",
    "            \n",
    "            print(f\"\\nğŸ“‹ Sample annotated data (negative):\")\n",
    "            sample_mapped = combined_results.filter(\n",
    "                pl.col('sample_type').str.contains('negative')\n",
    "            ).select(['uniprot_id', 'drugbank_id', 'cavity_index', 'sample_type', 'Gene_Name']).head(3)\n",
    "            print(sample_mapped)\n",
    "        \n",
    "        print(f\"\\nâœ… Sample type annotation complete!\")\n",
    "        print(f\"   Dataset now includes 'sample_type' column for positive/negative discrimination\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: Could not find sample metadata file:\")\n",
    "        print(f\"   {sample_metadata_file}\")\n",
    "        print(f\"   Sample type annotation skipped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during sample type annotation: {e}\")\n",
    "        print(f\"   Sample type annotation skipped.\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No data available for sample type annotation\")\n",
    "    print(\"   Please load data first (Step 1 & 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04beb6",
   "metadata": {},
   "source": [
    "## ğŸ” Step 2.6: Filter for Complete Tool Coverage and Balanced Samples\n",
    "\n",
    "**Important Filtering Criteria:**\n",
    "\n",
    "1. **Complete Tool Coverage**: Only include drug-target pairs where **ALL THREE tools** (Gold, Smina, LeDock) made predictions. This eliminates bias from partial tool coverage.\n",
    "\n",
    "2. **Balanced Sample Types**: For each drug, ensure equal representation of positive and negative samples:\n",
    "   - Filter out drugs that have only positive OR only negative samples\n",
    "   - Keep only drugs with both sample types present\n",
    "   - Balance the counts to have equal numbers of positive and negative samples per drug\n",
    "\n",
    "**Prerequisites:** \n",
    "- Step 2.5 (Sample Type Annotation) must be completed first\n",
    "- The `sample_type` column must be present in the dataset\n",
    "\n",
    "These filtering steps ensure fair comparison between tools and valid positive vs negative sample discrimination analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a80403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting filtering process...\n",
      "   Criteria: (1) Complete tool coverage, (2) Balanced positive/negative samples\n",
      "\n",
      "ğŸ“‹ Step 1: Filtering for final_results source_type...\n",
      "   Available source_types: ['gold_results', 'final_results', 'combined_results', 'smina_results', 'ledock_results']\n",
      "   Original rows: 28,628,253\n",
      "   After final_results filter: 7,076,083 (24.7%)\n",
      "\n",
      "ğŸ“‹ Step 2: Analyzing tool coverage...\n",
      "   All detected tools: ['GOLD', 'LeDock', 'Smina']\n",
      "   Expected tools found: ['GOLD', 'Smina', 'LeDock']\n",
      "\n",
      "ğŸ“Š Step 3: Checking tool coverage per drug-target pair...\n",
      "   Found 3,836 pairs with complete tool coverage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1419214/53553702.py:87: UserWarning: Comparisons with None always result in null. Consider using `.is_null()` or `.is_not_null()`.\n",
      "  pair_filter = (pl.col('drugbank_id') == drug) & (pl.col('uniprot_id') == target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ FILTERING RESULTS:\n",
      "========================================\n",
      "ğŸ“Š Original drug-target pairs: 5,346\n",
      "âœ… Complete coverage pairs: 3,836 (71.8%)\n",
      "ğŸ“‹ After source_type filter: 7,076,083\n",
      "ğŸ”„ Final filtered data: 6,153,508 (87.0%)\n",
      "\n",
      "ğŸ¯ TOOL COVERAGE DISTRIBUTION:\n",
      "   3 tools: 3,836 pairs (71.8%)\n",
      "   2 tools: 1,510 pairs (28.2%)\n",
      "\n",
      "âœ… Dataset filtered for fair tool comparison\n",
      "   Only using drug-target pairs where ALL 3 expected tools made predictions\n",
      "\n",
      "ğŸ“‹ Step 4: Filtering for balanced positive/negative samples...\n",
      "   Sample type distribution before balancing:\n",
      "      negative_balanced: 1,616,011\n",
      "      positive: 4,537,497\n",
      "   âœ… Both positive and negative samples found\n",
      "   Drugs with both sample types: 393\n",
      "   Drugs filtered out (only one sample type): 229\n",
      "   Rows after drug filtering: 4,458,423\n",
      "\n",
      "   Balancing positive/negative samples for each drug...\n",
      "   âœ… Balanced sampling complete:\n",
      "      Positive samples kept: 1,304,044\n",
      "      Negative samples kept: 1,304,044\n",
      "      Total rows: 2,608,088\n",
      "      Balance ratio: 1.000\n",
      "\n",
      "   ğŸ“Š Final sample type distribution:\n",
      "      negative_balanced: 1,304,044\n",
      "      positive: 1,304,044\n",
      "\n",
      "ğŸ¯ Ready for analysis with shape: (2608088, 44)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” FILTER FOR COMPLETE TOOL COVERAGE AND BALANCED SAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"ğŸ” Starting filtering process...\")\n",
    "    print(\"   Criteria: (1) Complete tool coverage, (2) Balanced positive/negative samples\")\n",
    "    \n",
    "    # STEP 1: Filter for final_results source_type only\n",
    "    print(\"\\nğŸ“‹ Step 1: Filtering for final_results source_type...\")\n",
    "    original_rows = combined_results.height\n",
    "    \n",
    "    if 'source_type' in combined_results.columns:\n",
    "        # Check what source_type values we have\n",
    "        source_types = combined_results['source_type'].unique().to_list()\n",
    "        print(f\"   Available source_types: {source_types}\")\n",
    "        \n",
    "        # Filter for final_results only\n",
    "        combined_results = combined_results.filter(pl.col('source_type') == 'final_results')\n",
    "        filtered_rows = combined_results.height\n",
    "        \n",
    "        print(f\"   Original rows: {original_rows:,}\")\n",
    "        print(f\"   After final_results filter: {filtered_rows:,} ({filtered_rows/original_rows*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   âš ï¸ No source_type column found, skipping source_type filtering\")\n",
    "    \n",
    "    # STEP 2: Analyze tool coverage\n",
    "    print(\"\\nğŸ“‹ Step 2: Analyzing tool coverage...\")\n",
    "    \n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        # Filter out null values from tool lists\n",
    "        tool1_list = combined_results.filter(pl.col('Tool1').is_not_null())['Tool1'].unique().to_list()\n",
    "        tool2_list = combined_results.filter(pl.col('Tool2').is_not_null())['Tool2'].unique().to_list()\n",
    "        \n",
    "        # Combine and sort, excluding any None values\n",
    "        all_tools = tool1_list + tool2_list\n",
    "        all_detected_tools = sorted([tool for tool in set(all_tools) if tool is not None])\n",
    "        \n",
    "        print(f\"   All detected tools: {all_detected_tools}\")\n",
    "        \n",
    "        # Define the three main tools we expect\n",
    "        expected_tools = ['GOLD', 'Smina', 'LeDock']\n",
    "        available_expected_tools = [tool for tool in expected_tools if tool in all_detected_tools]\n",
    "        \n",
    "        print(f\"   Expected tools found: {available_expected_tools}\")\n",
    "        \n",
    "        if len(available_expected_tools) >= 2:  # Need at least 2 tools for comparison\n",
    "            print(f\"\\nğŸ“Š Step 3: Checking tool coverage per drug-target pair...\")\n",
    "            \n",
    "            # Group by drug-target pairs and check tool coverage\n",
    "            drug_target_groups = combined_results.group_by(['drugbank_id', 'uniprot_id'])\n",
    "            \n",
    "            complete_coverage_pairs = []\n",
    "            coverage_summary = []\n",
    "            \n",
    "            for group_key, group_data in drug_target_groups:\n",
    "                drug = group_key[0]\n",
    "                target = group_key[1]\n",
    "                \n",
    "                # Get unique tools that made predictions for this drug-target pair\n",
    "                tools_t1 = group_data.filter(pl.col('Tool1').is_not_null())['Tool1'].unique().to_list()\n",
    "                tools_t2 = group_data.filter(pl.col('Tool2').is_not_null())['Tool2'].unique().to_list()\n",
    "                tools_in_group = set(tools_t1 + tools_t2)\n",
    "                tools_present = [tool for tool in available_expected_tools if tool in tools_in_group]\n",
    "                \n",
    "                coverage_summary.append({\n",
    "                    'drug': drug,\n",
    "                    'target': target,\n",
    "                    'tools_present': tools_present,\n",
    "                    'n_tools': len(tools_present),\n",
    "                    'complete_coverage': len(tools_present) == len(available_expected_tools),\n",
    "                    'original_rows': group_data.height\n",
    "                })\n",
    "                \n",
    "                # If all expected tools are present, keep this drug-target pair\n",
    "                if len(tools_present) == len(available_expected_tools):\n",
    "                    complete_coverage_pairs.append((drug, target))\n",
    "            \n",
    "            # STEP 3: Filter for complete coverage pairs\n",
    "            if complete_coverage_pairs:\n",
    "                print(f\"   Found {len(complete_coverage_pairs):,} pairs with complete tool coverage\")\n",
    "                \n",
    "                # Create filter for complete coverage pairs using Polars syntax\n",
    "                complete_filter = pl.lit(False)  # Start with False\n",
    "                \n",
    "                for drug, target in complete_coverage_pairs:\n",
    "                    pair_filter = (pl.col('drugbank_id') == drug) & (pl.col('uniprot_id') == target)\n",
    "                    complete_filter = complete_filter | pair_filter\n",
    "                \n",
    "                # Apply the filter\n",
    "                combined_results = combined_results.filter(complete_filter)\n",
    "                final_rows = combined_results.height\n",
    "                \n",
    "                # Report filtering results\n",
    "                original_pairs = len(coverage_summary)\n",
    "                complete_pairs = len(complete_coverage_pairs)\n",
    "                \n",
    "                print(f\"\\nğŸ“ˆ FILTERING RESULTS:\")\n",
    "                print(\"=\" * 40)\n",
    "                print(f\"ğŸ“Š Original drug-target pairs: {original_pairs:,}\")\n",
    "                print(f\"âœ… Complete coverage pairs: {complete_pairs:,} ({complete_pairs/original_pairs*100:.1f}%)\")\n",
    "                print(f\"ğŸ“‹ After source_type filter: {filtered_rows:,}\")\n",
    "                print(f\"ğŸ”„ Final filtered data: {final_rows:,} ({final_rows/filtered_rows*100:.1f}%)\")\n",
    "                \n",
    "                # Tool coverage distribution\n",
    "                coverage_dist = {}\n",
    "                for item in coverage_summary:\n",
    "                    n_tools = item['n_tools']\n",
    "                    coverage_dist[n_tools] = coverage_dist.get(n_tools, 0) + 1\n",
    "                \n",
    "                print(f\"\\nğŸ¯ TOOL COVERAGE DISTRIBUTION:\")\n",
    "                for n_tools in sorted(coverage_dist.keys(), reverse=True):\n",
    "                    count = coverage_dist[n_tools]\n",
    "                    pct = count / original_pairs * 100\n",
    "                    print(f\"   {n_tools} tools: {count:,} pairs ({pct:.1f}%)\")\n",
    "                \n",
    "                print(f\"\\nâœ… Dataset filtered for fair tool comparison\")\n",
    "                print(f\"   Only using drug-target pairs where ALL {len(available_expected_tools)} expected tools made predictions\")\n",
    "                \n",
    "                # STEP 4: Filter for balanced positive/negative samples per drug\n",
    "                print(f\"\\nğŸ“‹ Step 4: Filtering for balanced positive/negative samples...\")\n",
    "                \n",
    "                if 'sample_type' in combined_results.columns:\n",
    "                    # Check if we have sample type information\n",
    "                    sample_type_counts = combined_results.filter(\n",
    "                        pl.col('sample_type').is_not_null()\n",
    "                    ).group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "                    \n",
    "                    print(f\"   Sample type distribution before balancing:\")\n",
    "                    for row in sample_type_counts.iter_rows(named=True):\n",
    "                        print(f\"      {row['sample_type']}: {row['len']:,}\")\n",
    "                    \n",
    "                    # Identify positive and negative sample types\n",
    "                    has_positive = combined_results.filter(\n",
    "                        pl.col('sample_type') == 'positive'\n",
    "                    ).height > 0\n",
    "                    \n",
    "                    has_negative = combined_results.filter(\n",
    "                        pl.col('sample_type').str.contains('negative')\n",
    "                    ).height > 0\n",
    "                    \n",
    "                    if has_positive and has_negative:\n",
    "                        print(f\"   âœ… Both positive and negative samples found\")\n",
    "                        \n",
    "                        # Group by drug and count positive/negative samples\n",
    "                        # We'll count unique (drug, target, cavity) combinations per sample type\n",
    "                        drug_sample_counts = combined_results.filter(\n",
    "                            pl.col('sample_type').is_not_null()\n",
    "                        ).group_by(['drugbank_id', 'sample_type']).agg([\n",
    "                            pl.col('uniprot_id').n_unique().alias('n_targets'),\n",
    "                            pl.col('cavity_index').n_unique().alias('n_cavities'),\n",
    "                            pl.len().alias('n_records')\n",
    "                        ])\n",
    "                        \n",
    "                        # For each drug, check if it has both positive and negative samples\n",
    "                        drugs_with_both = {}\n",
    "                        \n",
    "                        for drug in combined_results['drugbank_id'].unique().to_list():\n",
    "                            drug_samples = drug_sample_counts.filter(\n",
    "                                pl.col('drugbank_id') == drug\n",
    "                            )\n",
    "                            \n",
    "                            pos_count = drug_samples.filter(\n",
    "                                pl.col('sample_type') == 'positive'\n",
    "                            )\n",
    "                            neg_count = drug_samples.filter(\n",
    "                                pl.col('sample_type').str.contains('negative')\n",
    "                            )\n",
    "                            \n",
    "                            has_pos = pos_count.height > 0\n",
    "                            has_neg = neg_count.height > 0\n",
    "                            \n",
    "                            if has_pos and has_neg:\n",
    "                                # Get the counts of unique (target, cavity) combinations\n",
    "                                n_pos = pos_count.select('n_records').sum().item() if has_pos else 0\n",
    "                                n_neg = neg_count.select('n_records').sum().item() if has_neg else 0\n",
    "                                \n",
    "                                drugs_with_both[drug] = {\n",
    "                                    'n_positive': n_pos,\n",
    "                                    'n_negative': n_neg,\n",
    "                                    'min_count': min(n_pos, n_neg)\n",
    "                                }\n",
    "                        \n",
    "                        print(f\"   Drugs with both sample types: {len(drugs_with_both):,}\")\n",
    "                        print(f\"   Drugs filtered out (only one sample type): {combined_results['drugbank_id'].n_unique() - len(drugs_with_both):,}\")\n",
    "                        \n",
    "                        if len(drugs_with_both) > 0:\n",
    "                            # Filter for drugs with both sample types\n",
    "                            valid_drugs = list(drugs_with_both.keys())\n",
    "                            combined_results = combined_results.filter(\n",
    "                                pl.col('drugbank_id').is_in(valid_drugs)\n",
    "                            )\n",
    "                            \n",
    "                            after_drug_filter = combined_results.height\n",
    "                            print(f\"   Rows after drug filtering: {after_drug_filter:,}\")\n",
    "                            \n",
    "                            # Now balance the samples for each drug\n",
    "                            print(f\"\\n   Balancing positive/negative samples for each drug...\")\n",
    "                            \n",
    "                            balanced_chunks = []\n",
    "                            total_pos_kept = 0\n",
    "                            total_neg_kept = 0\n",
    "                            \n",
    "                            for drug, counts in drugs_with_both.items():\n",
    "                                min_count = counts['min_count']\n",
    "                                \n",
    "                                # Get positive samples for this drug\n",
    "                                pos_samples = combined_results.filter(\n",
    "                                    (pl.col('drugbank_id') == drug) &\n",
    "                                    (pl.col('sample_type') == 'positive')\n",
    "                                )\n",
    "                                \n",
    "                                # Get negative samples for this drug\n",
    "                                neg_samples = combined_results.filter(\n",
    "                                    (pl.col('drugbank_id') == drug) &\n",
    "                                    (pl.col('sample_type').str.contains('negative'))\n",
    "                                )\n",
    "                                \n",
    "                                # Take equal number from each (limited by minimum)\n",
    "                                if pos_samples.height > min_count:\n",
    "                                    pos_samples = pos_samples.sample(n=min_count, seed=42)\n",
    "                                if neg_samples.height > min_count:\n",
    "                                    neg_samples = neg_samples.sample(n=min_count, seed=42)\n",
    "                                \n",
    "                                balanced_chunks.append(pos_samples)\n",
    "                                balanced_chunks.append(neg_samples)\n",
    "                                \n",
    "                                total_pos_kept += pos_samples.height\n",
    "                                total_neg_kept += neg_samples.height\n",
    "                            \n",
    "                            # Combine balanced chunks\n",
    "                            if balanced_chunks:\n",
    "                                combined_results = pl.concat(balanced_chunks, how='vertical')\n",
    "                                balanced_rows = combined_results.height\n",
    "                                \n",
    "                                print(f\"   âœ… Balanced sampling complete:\")\n",
    "                                print(f\"      Positive samples kept: {total_pos_kept:,}\")\n",
    "                                print(f\"      Negative samples kept: {total_neg_kept:,}\")\n",
    "                                print(f\"      Total rows: {balanced_rows:,}\")\n",
    "                                print(f\"      Balance ratio: {total_pos_kept/total_neg_kept:.3f}\" if total_neg_kept > 0 else \"      Balance ratio: N/A\")\n",
    "                                \n",
    "                                # Verify final balance\n",
    "                                final_sample_counts = combined_results.group_by('sample_type').agg(pl.len()).sort('sample_type')\n",
    "                                print(f\"\\n   ğŸ“Š Final sample type distribution:\")\n",
    "                                for row in final_sample_counts.iter_rows(named=True):\n",
    "                                    print(f\"      {row['sample_type']}: {row['len']:,}\")\n",
    "                            else:\n",
    "                                print(f\"   âš ï¸ No balanced data could be created\")\n",
    "                        else:\n",
    "                            print(f\"   âš ï¸ No drugs have both positive and negative samples\")\n",
    "                            print(f\"      Skipping balanced sampling\")\n",
    "                    elif has_positive:\n",
    "                        print(f\"   âš ï¸ Only positive samples found - cannot balance\")\n",
    "                    elif has_negative:\n",
    "                        print(f\"   âš ï¸ Only negative samples found - cannot balance\")\n",
    "                    else:\n",
    "                        print(f\"   âš ï¸ No valid sample type information found\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ No sample_type column found - skipping balanced sampling\")\n",
    "                    print(f\"      Run sample type annotation (Step 2.6) first for balanced sampling\")\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ No drug-target pairs have complete tool coverage!\")\n",
    "                print(\"   Cannot proceed with fair comparison analysis\")\n",
    "        else:\n",
    "            print(f\"âŒ Not enough tools found ({len(available_expected_tools)} < 2)\")\n",
    "    else:\n",
    "        print(\"âŒ Tool1 or Tool2 columns not found\")\n",
    "        \n",
    "    print(f\"\\nğŸ¯ Ready for analysis with shape: {combined_results.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No data available for filtering\")\n",
    "    print(\"   Please load data first (Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf873fd4",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 3: Data Overview & Quality Check\n",
    "\n",
    "Get familiar with the **filtered** dataset structure and check data quality. This step now analyzes the data after filtering for complete tool coverage, ensuring all statistics reflect the dataset used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edc109b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DATASET OVERVIEW\n",
      "==================================================\n",
      "ğŸ“Š Shape: (2653198, 44) (rows Ã— columns)\n",
      "ğŸ’¾ Memory: 1141.7 MB\n",
      "ğŸ“‹ Columns: 44\n",
      "\n",
      "ğŸ“š Column Names:\n",
      "   1. Tool1\n",
      "   2. Tool2\n",
      "   3. PoseNumber1\n",
      "   4. PoseNumber2\n",
      "   5. Score1\n",
      "   6. Score2\n",
      "   7. File1\n",
      "   8. File2\n",
      "   9. RMSD\n",
      "  10. source_file\n",
      "  11. source_dir\n",
      "  12. file_size_mb\n",
      "  13. source_type\n",
      "  14. drugbank_id\n",
      "  15. uniprot_id\n",
      "  16. gene_name\n",
      "  17. cavity_index\n",
      "  18. Pose\n",
      "  19. SMINA_Score\n",
      "  20. Score\n",
      "  21. S(PLP)\n",
      "  22. S(hbond)\n",
      "  23. S(cho)\n",
      "  24. S(metal)\n",
      "  25. DE(clash)\n",
      "  26. DE(tors)\n",
      "  27. time\n",
      "  28. LeDock_Score\n",
      "  29. primary_tool\n",
      "  30. compound_target_pair\n",
      "  31. extracted_drugbank_id\n",
      "  32. extracted_gene_name\n",
      "  33. extracted_uniprot_id\n",
      "  34. extracted_cavity_index\n",
      "  35. cavity_cluster_id\n",
      "  36. ic50_value\n",
      "  37. ic50_unit\n",
      "  38. measurement_type\n",
      "  39. operator\n",
      "  40. pubchem_cid\n",
      "  41. activity\n",
      "  42. n_measurements\n",
      "  43. sample_type\n",
      "  44. Gene_Name\n",
      "\n",
      "ğŸ§¬ KEY DATASET STATISTICS\n",
      "==================================================\n",
      "ğŸ”¬ Unique Drug-Target Combinations: 3,166\n",
      "ğŸ’Š Unique Drugs (DrugBank IDs): 405\n",
      "ğŸ§¬ Unique Proteins (UniProt IDs): 644\n",
      "ğŸ•³ï¸  Unique Cavities: 19\n",
      "ğŸ§© Cavity Clusters: 100 unique clusters\n",
      "   Mapped: 599,828/2,653,198 (22.6%)\n",
      "\n",
      "ğŸ” DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "âœ… RMSD data available: RMSD\n",
      "   Range: 0.11 - 47.20 Ã…\n",
      "   Mean: 10.21 Ã…, Median: 8.98 Ã…\n",
      "   Good poses (RMSD < 2.0 Ã…): 1.0%\n",
      "âœ… Score columns available: 5\n",
      "   - Score1\n",
      "   - Score2\n",
      "   - SMINA_Score\n",
      "   - Score\n",
      "   - LeDock_Score\n",
      "ğŸ§¬ Unique Proteins (UniProt IDs): 644\n",
      "ğŸ•³ï¸  Unique Cavities: 19\n",
      "ğŸ§© Cavity Clusters: 100 unique clusters\n",
      "   Mapped: 599,828/2,653,198 (22.6%)\n",
      "\n",
      "ğŸ” DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "âœ… RMSD data available: RMSD\n",
      "   Range: 0.11 - 47.20 Ã…\n",
      "   Mean: 10.21 Ã…, Median: 8.98 Ã…\n",
      "   Good poses (RMSD < 2.0 Ã…): 1.0%\n",
      "âœ… Score columns available: 5\n",
      "   - Score1\n",
      "   - Score2\n",
      "   - SMINA_Score\n",
      "   - Score\n",
      "   - LeDock_Score\n",
      "ğŸ”§ Tool1 variants: 2\n",
      "ğŸ”§ Tool2 variants: 2\n",
      "ğŸ“ Source types:\n",
      "   - final_results: 2,653,198 rows\n",
      "\n",
      "âœ… Dataset quality check complete\n",
      "ğŸ¯ Ready for consensus analysis!\n",
      "ğŸ”§ Tool1 variants: 2\n",
      "ğŸ”§ Tool2 variants: 2\n",
      "ğŸ“ Source types:\n",
      "   - final_results: 2,653,198 rows\n",
      "\n",
      "âœ… Dataset quality check complete\n",
      "ğŸ¯ Ready for consensus analysis!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š STEP 3: DATA OVERVIEW & QUALITY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "if not combined_results.is_empty():\n",
    "    print(\"ğŸ” DATASET OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“Š Shape: {combined_results.shape} (rows Ã— columns)\")\n",
    "    print(f\"ğŸ’¾ Memory: {combined_results.estimated_size() / (1024*1024):.1f} MB\")\n",
    "    print(f\"ğŸ“‹ Columns: {combined_results.width}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“š Column Names:\")\n",
    "    for i, col in enumerate(combined_results.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nğŸ§¬ KEY DATASET STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Core identifiers\n",
    "    print(f\"ğŸ”¬ Unique Drug-Target Combinations: {combined_results.select(['drugbank_id', 'uniprot_id']).unique().height:,}\")\n",
    "    print(f\"ğŸ’Š Unique Drugs (DrugBank IDs): {combined_results['drugbank_id'].n_unique():,}\")\n",
    "    print(f\"ğŸ§¬ Unique Proteins (UniProt IDs): {combined_results['uniprot_id'].n_unique():,}\")\n",
    "    \n",
    "    # Check for cavity index information\n",
    "    if 'extracted_cavity_index' in combined_results.columns:\n",
    "        print(f\"ğŸ•³ï¸  Unique Cavities: {combined_results['extracted_cavity_index'].n_unique():,}\")\n",
    "    elif 'cavity_index' in combined_results.columns:\n",
    "        print(f\"ğŸ•³ï¸  Unique Cavities: {combined_results['cavity_index'].n_unique():,}\")\n",
    "    else:\n",
    "        print(\"ğŸ•³ï¸  Cavity information: Not available\")\n",
    "    \n",
    "    # Cluster information\n",
    "    if 'cavity_cluster_id' in combined_results.columns:\n",
    "        cluster_mapped = combined_results['cavity_cluster_id'].drop_nulls().len()\n",
    "        cluster_total = combined_results.height\n",
    "        unique_clusters = combined_results['cavity_cluster_id'].n_unique()\n",
    "        print(f\"ğŸ§© Cavity Clusters: {unique_clusters:,} unique clusters\")\n",
    "        print(f\"   Mapped: {cluster_mapped:,}/{cluster_total:,} ({cluster_mapped/cluster_total*100:.1f}%)\")\n",
    "    \n",
    "    # Check for essential analysis columns\n",
    "    print(f\"\\nğŸ” DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for RMSD columns\n",
    "    rmsd_columns = [col for col in combined_results.columns if 'rmsd' in col.lower()]\n",
    "    if rmsd_columns:\n",
    "        rmsd_col = rmsd_columns[0]\n",
    "        print(f\"âœ… RMSD data available: {rmsd_col}\")\n",
    "        \n",
    "        # RMSD statistics\n",
    "        rmsd_stats = combined_results.select([\n",
    "            pl.col(rmsd_col).min().alias('min_rmsd'),\n",
    "            pl.col(rmsd_col).max().alias('max_rmsd'),\n",
    "            pl.col(rmsd_col).mean().alias('mean_rmsd'),\n",
    "            pl.col(rmsd_col).median().alias('median_rmsd'),\n",
    "            (pl.col(rmsd_col) < 2.0).mean().alias('good_poses_pct')\n",
    "        ]).to_pandas().iloc[0]\n",
    "        \n",
    "        print(f\"   Range: {rmsd_stats['min_rmsd']:.2f} - {rmsd_stats['max_rmsd']:.2f} Ã…\")\n",
    "        print(f\"   Mean: {rmsd_stats['mean_rmsd']:.2f} Ã…, Median: {rmsd_stats['median_rmsd']:.2f} Ã…\")\n",
    "        print(f\"   Good poses (RMSD < 2.0 Ã…): {rmsd_stats['good_poses_pct']*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No RMSD columns found - pose consistency analysis may be limited\")\n",
    "    \n",
    "    # Check for score columns\n",
    "    score_columns = [col for col in combined_results.columns if 'score' in col.lower()]\n",
    "    print(f\"âœ… Score columns available: {len(score_columns)}\")\n",
    "    for col in score_columns[:5]:  # Show first 5 score columns\n",
    "        print(f\"   - {col}\")\n",
    "    if len(score_columns) > 5:\n",
    "        print(f\"   ... and {len(score_columns) - 5} more\")\n",
    "    \n",
    "    # Tool information\n",
    "    if 'Tool1' in combined_results.columns and 'Tool2' in combined_results.columns:\n",
    "        tool1_unique = combined_results.filter(pl.col('Tool1').is_not_null())['Tool1'].n_unique()\n",
    "        tool2_unique = combined_results.filter(pl.col('Tool2').is_not_null())['Tool2'].n_unique()\n",
    "        print(f\"ğŸ”§ Tool1 variants: {tool1_unique}\")\n",
    "        print(f\"ğŸ”§ Tool2 variants: {tool2_unique}\")\n",
    "    \n",
    "    # Source information\n",
    "    if 'source_type' in combined_results.columns:\n",
    "        source_types = combined_results['source_type'].value_counts().to_pandas()\n",
    "        print(f\"ğŸ“ Source types:\")\n",
    "        for _, row in source_types.iterrows():\n",
    "            print(f\"   - {row['source_type']}: {row['count']:,} rows\")\n",
    "    \n",
    "    print(f\"\\nâœ… Dataset quality check complete\")\n",
    "    print(f\"ğŸ¯ Ready for consensus analysis!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No data available for overview\")\n",
    "    print(\"   Please load and filter data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dccec2",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 4: Save Prepared Data for Analysis\n",
    "\n",
    "**Purpose:** Save the filtered, annotated, and quality-checked dataset for later use.\n",
    "\n",
    "This checkpoint preserves the processed data after:\n",
    "- âœ… Data loading and cluster integration (Steps 1-2)\n",
    "- âœ… Sample type annotation (Step 2.5)\n",
    "- âœ… Complete tool coverage filtering (Step 2.6)\n",
    "- âœ… Balanced positive/negative sampling (Step 2.6)\n",
    "- âœ… Data quality verification (Step 3)\n",
    "\n",
    "**Output:** A clean, analysis-ready Parquet file containing:\n",
    "- All consensus docking results with complete tool coverage\n",
    "- Balanced positive and negative samples\n",
    "- Annotated sample types and metadata\n",
    "- Integrated cavity cluster information\n",
    "\n",
    "This file can be reloaded in future sessions to skip preprocessing steps and jump directly to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a7e350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results.write_parquet(\"/media/onur/Elements/cavity_space_consensus_docking/2025_06_29_batch_dock/combined_filtered_annotated_docking_results.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachopencadd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
